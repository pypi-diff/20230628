# Comparing `tmp/mllibs-0.1.2-py2.py3-none-any.whl.zip` & `tmp/mllibs-0.1.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,35 @@
-Zip file size: 90519 bytes, number of entries: 30
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-23 08:27 mllibs/__init__.py
--rw-rw-rw-  2.0 fat     4129 b- defN 23-Jun-23 08:27 mllibs/interface.py
--rw-rw-rw-  2.0 fat     6567 b- defN 23-Jun-23 08:27 mllibs/mdsplit.py
--rw-rw-rw-  2.0 fat     9615 b- defN 23-Jun-23 08:27 mllibs/meda_scplot.py
--rw-rw-rw-  2.0 fat    22334 b- defN 23-Jun-23 08:27 mllibs/meda_splot.py
--rw-rw-rw-  2.0 fat    31758 b- defN 23-Jun-23 08:27 mllibs/membedding.py
--rw-rw-rw-  2.0 fat    15320 b- defN 23-Jun-23 08:27 mllibs/mencoder.py
--rw-rw-rw-  2.0 fat     5312 b- defN 23-Jun-23 08:27 mllibs/mloader.py
--rw-rw-rw-  2.0 fat     9293 b- defN 23-Jun-23 08:27 mllibs/mnlp_encoder.py
--rw-rw-rw-  2.0 fat    10978 b- defN 23-Jun-23 08:27 mllibs/moutliers.py
--rw-rw-rw-  2.0 fat     5409 b- defN 23-Jun-23 08:27 mllibs/mpd_df.py
--rw-rw-rw-  2.0 fat     7007 b- defN 23-Jun-23 08:27 mllibs/mseda.py
--rw-rw-rw-  2.0 fat    28065 b- defN 23-Jun-23 08:27 mllibs/msllinear.py
--rw-rw-rw-  2.0 fat    16859 b- defN 23-Jun-23 08:27 mllibs/mtextnorm.py
--rw-rw-rw-  2.0 fat    19588 b- defN 23-Jun-23 08:27 mllibs/musldimred.py
--rw-rw-rw-  2.0 fat    30696 b- defN 23-Jun-23 08:27 mllibs/nlpi.py
--rw-rw-rw-  2.0 fat    16856 b- defN 23-Jun-23 08:27 mllibs/nlpm.py
+Zip file size: 154667 bytes, number of entries: 33
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-23 14:43 mllibs/__init__.py
+-rw-rw-rw-  2.0 fat    15120 b- defN 23-Jun-27 23:00 mllibs/common_eval.py
+-rw-rw-rw-  2.0 fat     4513 b- defN 23-Jun-24 02:02 mllibs/interface.py
+-rw-rw-rw-  2.0 fat    10770 b- defN 23-Jun-27 14:31 mllibs/mdsplit.py
+-rw-rw-rw-  2.0 fat     9912 b- defN 23-Jun-24 06:11 mllibs/meda_scplot.py
+-rw-rw-rw-  2.0 fat    22347 b- defN 23-Jun-25 14:45 mllibs/meda_splot.py
+-rw-rw-rw-  2.0 fat    31706 b- defN 23-Jun-25 21:30 mllibs/membedding.py
+-rw-rw-rw-  2.0 fat    15353 b- defN 23-Jun-24 06:16 mllibs/mencoder.py
+-rw-rw-rw-  2.0 fat     5312 b- defN 23-Jun-25 11:40 mllibs/mloader.py
+-rw-rw-rw-  2.0 fat    12911 b- defN 23-Jun-25 14:33 mllibs/moutliers.py
+-rw-rw-rw-  2.0 fat     6457 b- defN 23-Jun-24 04:53 mllibs/mpd_df.py
+-rw-rw-rw-  2.0 fat     6940 b- defN 23-Jun-25 10:54 mllibs/mseda.py
+-rw-rw-rw-  2.0 fat    28224 b- defN 23-Jun-27 23:28 mllibs/mslensemble.py
+-rw-rw-rw-  2.0 fat    19238 b- defN 23-Jun-27 21:31 mllibs/msllinear.py
+-rw-rw-rw-  2.0 fat    16763 b- defN 23-Jun-23 14:44 mllibs/mtextnorm.py
+-rw-rw-rw-  2.0 fat    19605 b- defN 23-Jun-24 17:56 mllibs/musldimred.py
+-rw-rw-rw-  2.0 fat    31905 b- defN 23-Jun-27 14:19 mllibs/nlpi.py
+-rw-rw-rw-  2.0 fat    17085 b- defN 23-Jun-27 15:35 mllibs/nlpm.py
 -rw-rw-rw-  2.0 fat    75880 b- defN 23-Jun-23 08:27 mllibs/corpus/wordlist.10000.txt
+-rw-rw-rw-  2.0 fat   126077 b- defN 23-Jun-24 17:03 mllibs/models/cv_ner_tagger.pickle
+-rw-rw-rw-  2.0 fat     2745 b- defN 23-Jun-24 17:03 mllibs/models/dtc_ner_tagger.pickle
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-23 08:27 mlmodels/__init__.py
 -rw-rw-rw-  2.0 fat     2994 b- defN 23-Jun-23 08:27 mlmodels/bl_regressor.py
 -rw-rw-rw-  2.0 fat     4402 b- defN 23-Jun-23 08:27 mlmodels/gmm.py
 -rw-rw-rw-  2.0 fat     5761 b- defN 23-Jun-23 08:27 mlmodels/gp_bclassifier.py
 -rw-rw-rw-  2.0 fat     4627 b- defN 23-Jun-23 08:27 mlmodels/gp_regressor.py
 -rw-rw-rw-  2.0 fat     6350 b- defN 23-Jun-23 08:27 mlmodels/gpr_bclassifier.py
 -rw-rw-rw-  2.0 fat     5170 b- defN 23-Jun-23 08:27 mlmodels/kriging_regressor.py
--rw-rw-rw-  2.0 fat     1091 b- defN 23-Jun-23 08:56 mllibs-0.1.2.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     7278 b- defN 23-Jun-23 08:56 mllibs-0.1.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat      110 b- defN 23-Jun-23 08:56 mllibs-0.1.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       16 b- defN 23-Jun-23 08:56 mllibs-0.1.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2316 b- defN 23-Jun-23 08:56 mllibs-0.1.2.dist-info/RECORD
-30 files, 355781 bytes uncompressed, 86899 bytes compressed:  75.6%
+-rw-rw-rw-  2.0 fat     1091 b- defN 23-Jun-27 23:31 mllibs-0.1.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     8344 b- defN 23-Jun-27 23:31 mllibs-0.1.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-27 23:31 mllibs-0.1.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       16 b- defN 23-Jun-27 23:31 mllibs-0.1.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2580 b- defN 23-Jun-27 23:31 mllibs-0.1.3.dist-info/RECORD
+33 files, 520290 bytes uncompressed, 150641 bytes compressed:  71.0%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: mllibs/__init__.py
 Comment: 
 
+Filename: mllibs/common_eval.py
+Comment: 
+
 Filename: mllibs/interface.py
 Comment: 
 
 Filename: mllibs/mdsplit.py
 Comment: 
 
 Filename: mllibs/meda_scplot.py
@@ -18,26 +21,26 @@
 
 Filename: mllibs/mencoder.py
 Comment: 
 
 Filename: mllibs/mloader.py
 Comment: 
 
-Filename: mllibs/mnlp_encoder.py
-Comment: 
-
 Filename: mllibs/moutliers.py
 Comment: 
 
 Filename: mllibs/mpd_df.py
 Comment: 
 
 Filename: mllibs/mseda.py
 Comment: 
 
+Filename: mllibs/mslensemble.py
+Comment: 
+
 Filename: mllibs/msllinear.py
 Comment: 
 
 Filename: mllibs/mtextnorm.py
 Comment: 
 
 Filename: mllibs/musldimred.py
@@ -48,14 +51,20 @@
 
 Filename: mllibs/nlpm.py
 Comment: 
 
 Filename: mllibs/corpus/wordlist.10000.txt
 Comment: 
 
+Filename: mllibs/models/cv_ner_tagger.pickle
+Comment: 
+
+Filename: mllibs/models/dtc_ner_tagger.pickle
+Comment: 
+
 Filename: mlmodels/__init__.py
 Comment: 
 
 Filename: mlmodels/bl_regressor.py
 Comment: 
 
 Filename: mlmodels/gmm.py
@@ -69,23 +78,23 @@
 
 Filename: mlmodels/gpr_bclassifier.py
 Comment: 
 
 Filename: mlmodels/kriging_regressor.py
 Comment: 
 
-Filename: mllibs-0.1.2.dist-info/LICENSE
+Filename: mllibs-0.1.3.dist-info/LICENSE
 Comment: 
 
-Filename: mllibs-0.1.2.dist-info/METADATA
+Filename: mllibs-0.1.3.dist-info/METADATA
 Comment: 
 
-Filename: mllibs-0.1.2.dist-info/WHEEL
+Filename: mllibs-0.1.3.dist-info/WHEEL
 Comment: 
 
-Filename: mllibs-0.1.2.dist-info/top_level.txt
+Filename: mllibs-0.1.3.dist-info/top_level.txt
 Comment: 
 
-Filename: mllibs-0.1.2.dist-info/RECORD
+Filename: mllibs-0.1.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mllibs/interface.py

```diff
@@ -11,14 +11,15 @@
 from mllibs.mencoder import encoder, configure_nlpencoder
 from mllibs.mdsplit import make_fold,configure_makefold
 from mllibs.moutliers import data_outliers,configure_outliers
 from mllibs.membedding import embedding,configure_nlpembed
 from mllibs.mtextnorm import cleantext, configure_nlptxtclean
 from mllibs.msllinear import sllinear, configure_sllinear
 from mllibs.musldimred import usldimred, configure_usldimred
+from mllibs.mslensemble import slensemble, configure_slensemble
 
 # single command interpreter, multiple command interpreter & interface (chat)
 
 '''
 
 Single command interpreter interface
 
@@ -60,21 +61,24 @@
 
 '''
 
 # interface class is a user interaction class
 
 class interface(snlpi,mnlpi,nlpi):
 
-    def __init__(self):
+    def __init__(self,silent=False):
         
         # compile modules
         self.collection = self.prestart()
-        snlpi.__init__(self,self.collection) 
+        snlpi.__init__(self,self.collection)
+        if(silent is False):
+            nlpi.silent = False
+        else:
+            nlpi.silent = True 
                
-
     def __getitem__(self,command:str):
         self.exec(command,args=None)
         
 
     def prestart(self):
 
         collection = nlpm()
@@ -84,52 +88,53 @@
                          eda_colplot(configure_colplot),  # column based visuals
                          dataframe_oper(configure_pda),   # pandas dataframe operations
                          encoder(configure_nlpencoder),    # encode text to values
                          make_fold(configure_makefold),     # create subset folds
                          data_outliers(configure_outliers), # create data outliers
                          embedding(configure_nlpembed),    # generate text embeddings
                          cleantext(configure_nlptxtclean), # clean text 
-                         sllinear(configure_sllinear),      # linear regression models                        
-                         usldimred(configure_usldimred)
+                         sllinear(configure_sllinear),      # linear machine learning models                        
+                         usldimred(configure_usldimred),     # unsupervised learning dimension reduction
+                         slensemble(configure_slensemble)   # ensemble machine learning models
                         ])
 
 
         collection.train()
                             
         return collection
         
     
-    def iter_loop(self):
-        
-        # user command 
-        if(command == None):
-            print('What would you like to do?')
-            self.command = input()
-        else:
-            self.command = command
-            
-        ''' Check for multicommand '''
-        # currently simple implementation based on rules
-        
-        tokens = nlpi.nltk_tokeniser(self.command)
+    # def iter_loop(self):
         
-        for token in tokens:
-            if(token in text_store.dividers):
-                ctype = 'multiple'
-            else:
-                ctype = 'single'
-        
-        # activate relevant interpreter
-        if(ctype == 'multiple'):
-            mnpli.__init__(self,self.collection)
-            self.exec(str(self.command))
-        elif(ctype == 'single'):
-            snlpi.__init__(self,self.collection)
-            self.exec(str(self.command))
-            self.return_data()
-            
-            
-    def return_data(self):
-        print('storing data in global variable: stored')
-        globals()['stored'] = self.glr()
+    #     # user command 
+    #     if(command == None):
+    #         print('What would you like to do?')
+    #         self.command = input()
+    #     else:
+    #         self.command = command
+            
+    #     ''' Check for multicommand '''
+    #     # currently simple implementation based on rules
+        
+    #     tokens = nlpi.nltk_tokeniser(self.command)
+        
+    #     for token in tokens:
+    #         if(token in text_store.dividers):
+    #             ctype = 'multiple'
+    #         else:
+    #             ctype = 'single'
+        
+    #     # activate relevant interpreter
+    #     if(ctype == 'multiple'):
+    #         mnpli.__init__(self,self.collection)
+    #         self.exec(str(self.command))
+    #     elif(ctype == 'single'):
+    #         snlpi.__init__(self,self.collection)
+    #         self.exec(str(self.command))
+    #         self.return_data()
+            
+            
+    # def return_data(self):
+    #     print('storing data in global variable: stored')
+    #     globals()['stored'] = self.glr()
```

## mllibs/mdsplit.py

```diff
@@ -1,13 +1,14 @@
 import pandas as pd
 from mllibs.nlpi import nlpi
 from collections import OrderedDict
 from sklearn.model_selection import KFold
 from sklearn.model_selection import StratifiedKFold
 from sklearn.model_selection import train_test_split
+import random
 
 
 '''
 
 Split Data into Subsets 
 
 
@@ -16,149 +17,199 @@
 
 class make_fold(nlpi):
     
     # called in nlpm
     def __init__(self,nlp_config):
         self.name = 'make_folds'             
         self.nlp_config = nlp_config 
+
+    @staticmethod
+    def sfp(args,preset,key:str):
+        
+        if(args[key] is not None):
+            return eval(args[key])
+        else:
+            return preset[key] 
+        
+    # set general parameter
+        
+    @staticmethod
+    def sgp(args,key:str):
+        
+        if(args[key] is not None):
+            return eval(args[key])
+        else:
+            return None
         
     # called in nlpi
     def sel(self,args:dict):
         
+        # define instance parameters
         self.select = args['pred_task']
         self.args = args
+        self.data_name = args['data_name']  # name of the data
         
         if(self.select == 'kfold_label'):
             self.kfold_label(self.args)
         elif(self.select == 'skfold_label'):
             self.skfold_label(self.args)
         elif(self.select == 'tts_label'):
             self.tts_label(self.args)
         
-    # Kfold splitting
+    ''' 
+    
+    ACTIVATION FUNCTIONS 
+    
+    '''
+    # kfold_label 
+    # skfold_label
+    # tts_label
         
     def kfold_label(self,args:dict):
+
+        pre = {'splits':3,'shuffle':True,'rs':random.randint(1,500)}
        
-        kf = KFold(n_splits=eval(args['splits']), 
-                   shuffle=eval(args['shuffle']), 
-                   random_state=eval(args['rs']))
+        kf = KFold(n_splits=self.sfp(args,pre,'n_splits'), 
+                   shuffle=self.sfp(args,pre,'shuffle'), 
+                   random_state=self.sfp(args,pre,'rs'))
                     
         for i, (_, v_ind) in enumerate(kf.split(args['data'])):
             args['data'].loc[args['data'].index[v_ind], 'kfold'] = f"fold{i+1}"
         
         # store relevant data about operation
+
         nlpi.memory_output.append({'data':args['data'],
-                                   'shuffle':args['shuffle'],
-                                   'n_splits':args['splits'],
+                                   'shuffle':self.sfp(args,pre,'shuffle'),
+                                   'n_splits':self.sfp(args,pre,'splits'),
                                    'split':kf,
-                                   'rs':args['rs']}) 
-                    
+                                   'rs':self.sfp(args,pre,'rs')}) 
+        
+        # store split data into input data source
+
+        nlpi.data[self.data_name[0]]['splits'][f'kfold_{nlpi.iter}'] = kf
+   
     # Stratified kfold splitting             
     
     def skfold_label(self,args:dict):
+
+        pre = {'splits':3,'shuffle':True,'rs':random.randint(1,500)}
         
         if(type(args['y']) is str):
 
-            kf = StratifiedKFold(n_splits=eval(args['splits']), 
-                                 shuffle=eval(args['shuffle']), 
-                                 random_state=eval(args['rs']))
+            kf = StratifiedKFold(n_splits=self.sfp(args,pre,'n_splits'), 
+                                 shuffle=self.sfp(args,pre,'shuffle'), 
+                                 random_state=self.sfp(args,pre,'rs'))
                         
             for i, (_, v_ind) in enumerate(kf.split(args['data'],args['data'][[args['y']]])):
                 args['data'].loc[args['data'].index[v_ind], 'skfold'] = f"fold{i+1}"
                 
             # store relevant data about operation
             nlpi.memory_output.append({'data':args['data'],
-                                       'shuffle':args['shuffle'],
-                                       'n_splits':args['splits'],
+                                       'shuffle':self.sfp(args,pre,'shuffle'),
+                                       'n_splits':self.sfp(args,pre,'splits'),
                                        'stratify':args['y'],
                                        'split':kf,
-                                       'rs':args['rs']}) 
+                                       'rs':self.sfp(args,pre,'rs')}) 
+            
+            # store relevant data about operation
+            nlpi.data[self.data_name[0]]['splits'][f'skfold_{nlpi.iter}'] = kf
+            
         else:
             print('specify y data token for stratification!')    
             nlpi.memory_output(None)                           
             
         
     # Train test split labeling (one df only)
         
     def tts_label(self,args:dict):
+
+        # preset setting 
+        pre = {'test_size':0.3,'shuffle':True,'rs':random.randint(1,500)}
         
         train, test = train_test_split(args['data'],
-                                       test_size=eval(args['test_size']),
-                                       shuffle=eval(args['shuffle']),
+                                       test_size=self.sfp(args,pre,'test_size'),
+                                       shuffle=self.sfp(args,pre,'shuffle'),
                                        stratify=args['y'],
-                                       random_state=eval(args['rs'])
+                                       random_state=self.sfp(args,pre,'rs')
                                        )
         
         train['tts'] = 'train'
         test['tts'] = 'test'
         ldf = pd.concat([train,test],axis=0)
         ldf = ldf.sort_index()
         
         # store relevant data about operation
         nlpi.memory_output.append({'data':ldf,
-                                   'stratified':args['y'],
-                                   'shuffle':args['shuffle'],
+                                   'shuffle':self.sfp(args,pre,'shuffle'),
                                    'stratify':args['y'],
-                                   'test_size':args['test_size'],
-                                   'rs':args['rs']}
-                                   )
+                                   'test_size':self.sfp(args,pre,'test_size'),
+                                   'rs':self.sfp(args,pre,'rs')}
+                                )
+
+        # store relevant data about operation in data source
+        nlpi.data[self.data_name[0]]['splits'][f'tts_{nlpi.iter}'] = ldf['tts']
    
 '''
 
-
 Corpus
 
-
 '''   
 
 corpus_makefold = OrderedDict({"kfold_label":['create kfold',
-                                      'make kfold'
-                                      'create subset folds',
-                                      'make subset fold',
-                                      'label kfold'],
+                                              'create kfolds',
+                                              'make kfold',
+                                              'create kfold labels',
+                                              'create subset folds',
+                                              'make subset fold',
+                                              'label kfold'],
                                       
                                 "skfold_label": ['stratified kfold',
-                                            'create stratified kfold',
-                                            'make stratified kfold',
-                                            'generate stratified kfold',
-                                            'label statified kfold'],
+                                                 'stratified kfolds',
+                                                 'create stratified kfold',
+                                                 'make stratified kfold',
+                                                 'generate stratified kfold',
+                                                 'label statified kfold'],
                                             
                                 'tts_label': ['train test split label',
-                                             'create tts label',
-                                             'make tts label',
-                                             'make train test split label',
-                                             'train-test-split label',
-                                             'create train-test-split label',
-                                             'label tts',
-                                             'tts labels',
-                                             'create tts labels']
+                                              'train test split labels',
+                                              'train test splitting labels',
+                                              'create tts label',
+                                              'make tts label',
+                                              'make train test split label',
+                                              'train-test-split label',
+                                              'create train-test-split label',
+                                              'label tts',
+                                              'tts labels',
+                                              'create tts labels']
                                       
                                       })
 
 
 info_makefold = {'kfold_label': {'module':'make_folds',
-                            'action':'action',
-                            'topic':'topic',
-                            'subtopic':'sub topic',
-                            'input_format':'pd.DataFrame',
-                            'description':'generate kfolds labels for dataframe',
-                            'arg_compat':'splits shuffle rs'},
+                                'action':'create subset',
+                                'topic':'subset generation',
+                                'subtopic':'kfold cross validation',
+                                'input_format':'pd.DataFrame',
+                                'description':"K-fold cross-validation is a technique used in machine learning to evaluate the performance of a model. It involves dividing the dataset into k equal-sized subsets, or folds. The model is then trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold being used as the test set once. The results are averaged across the k iterations to provide an estimate of the model's performance. K-fold cross-validation helps to reduce the risk of overfitting and provides a more accurate estimate of the model's generalization performance. It is commonly used in machine learning to tune hyperparameters, select models, and compare different algorithms.",
+                                'arg_compat':'splits shuffle rs'},
                             
                 'skfold_label': {'module':'make_folds',
-                            'action':'action',
-                            'topic':'topic',
-                            'subtopic':'sub topic',
-                            'input_format':'pd.DataFrame',
-                            'description':'generate stratified kfolds labels for dataframe'},
+                                'action':'create subset',
+                                'topic':'subset generation',
+                                'subtopic':'stratified kfold cross validation',
+                                'input_format':'pd.DataFrame',
+                                'description':"Stratified k-fold cross-validation is a variation of k-fold cross-validation that ensures that each fold is representative of the overall distribution of the target variable. This is particularly useful when dealing with imbalanced datasets, where one class may be significantly underrepresented. In stratified k-fold cross-validation, the dataset is divided into k folds, but the division is done in such a way that each fold contains approximately the same proportion of samples from each class as the original dataset. This ensures that each fold is representative of the overall distribution of the target variable, and reduces the risk of bias in the evaluation of the model's performance. Stratified k-fold cross-validation is commonly used in classification tasks where the goal is to predict the class label of a sample based on its features.",
+                                'arg_compat':'splits shuffle rs'
+                                },
 
                 'tts_label': {'module':'make_folds',
-                            'action':'action',
-                            'topic':'topic',
-                            'subtopic':'sub topic',
-                            'input_format':'pd.DataFrame',
-                            'description':'generate train-test-split labels for dataframe'}
-                            
+                              'action':'create subset',
+                              'topic':'subset generation',
+                              'subtopic':'train test split',
+                              'input_format':'pd.DataFrame',
+                              'description':"Train test splitting is a technique used in machine learning to evaluate the performance of a model. It involves dividing the available dataset into two subsets: the training set and the testing set. The training set is used to train the model, while the testing set is used to evaluate its performance. The idea behind train test splitting is to assess how well the model generalizes to new, unseen data. By evaluating the model on a separate testing set, we can get an estimate of its performance on new data that it has not seen before. The size of the training and testing sets can vary depending on the size of the dataset, but a common practice is to use 70-80 of the data for training and the remaining 20-30 for testing.",
+                              'arg_compat':'test_size shuffle rs'}
                                  
                             }
                          
 # configuration dictionary (passed in nlpm)
 configure_makefold = {'corpus':corpus_makefold,'info':info_makefold}
```

## mllibs/meda_scplot.py

```diff
@@ -47,18 +47,30 @@
             
     @staticmethod
     def split_types(df):
         numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  
         numeric = df.select_dtypes(include=numerics)
         categorical = df.select_dtypes(exclude=numerics)
         return numeric,categorical
+
+    '''
+    
+    Activation Functions
+    
+    '''
+    # eda_colplot_kde
+    # eda_colplot_box
+    # eda_colplot_scatter
+
+    # column KDE plots for numeric columns
         
     def eda_colplot_kde(self,args:dict):
         
-        num,cat = self.split_types(args['data'])
+        # get numeric column names only
+        num,_ = self.split_types(args['data'])
             
         if(args['x'] is not None):
             xloc = args['data'][args['x']]
         else:
             xloc = None
             
         if(args['hue'] is not None):
@@ -102,14 +114,16 @@
             ax[i].set_xlabel(None)
     
         for i in range(i+1, len(ax)):
             ax[i].axis('off')
                       
         plt.tight_layout()
         
+    # column boxplots for numeric columns
+
     def eda_colplot_box(self,args:dict):
 
         # split data into numeric & non numeric
         num,cat = self.split_types(args['data'])
           
         columns = list(num.columns)  
         n_cols = 3
@@ -166,19 +180,21 @@
             ax[i].set_xlabel(None)
             
             
         for i in range(i+1, len(ax)):
             ax[i].axis('off')
         
         plt.tight_layout()
+
+    # column scatter plot for numeric columns only
         
     def eda_colplot_scatter(self,args:dict):
 
         # split data into numeric & non numeric
-        num,cat = self.split_types(args['data'])
+        num,_ = self.split_types(args['data'])
           
         columns = list(num.columns)  
         n_cols = 3
         n_rows = math.ceil(len(columns)/n_cols)
         
         if(args['x'] is not None):
             xloc = args['data'][args['x']]
```

## mllibs/meda_splot.py

```diff
@@ -346,17 +346,17 @@
                                'alpha':nlpi.pp['alpha'],
                                's':nlpi.pp['s']})   
         
         sns.despine(left=True, bottom=True)
         plt.show()
         nlpi.resetpp()
         
-        
-    @staticmethod
-    def seaborn_lineplot(args:dict):
+    # Lineplot
+
+    def seaborn_lineplot(self,args:dict):
     
         if(args['hue'] is not None):
             hueloc = args['data'][args['hue']]
             if(type(nlpi.pp['stheme']) is str):
                 palette = nlpi.pp['stheme']
             else:
                 palette = palette_rgb[:len(hueloc.value_counts())]
@@ -368,47 +368,54 @@
         sns.set_style("whitegrid", {
             "ytick.major.size": 0.1,
             "ytick.minor.size": 0.05,
             'grid.linestyle': '--'
          })
 
         sns.lineplot(x=args['x'], 
-                    y=args['y'],
+                     y=args['y'],
                      hue=args['hue'],
-                    alpha= nlpi.pp['alpha'],
-                    linewidth=nlpi.pp['mew'],
-                        data=args['data'],
-                        palette=palette)
+                     alpha= nlpi.pp['alpha'],
+                     linewidth=nlpi.pp['mew'],
+                     data=args['data'],
+                     palette=palette)
         
         sns.despine(left=True, bottom=True)
         plt.show()
         nlpi.resetpp()
+
+    # Heatmap
                 
     def seaborn_heatmap(self,args:dict):
         
         if(args['hue'] is not None):
             hueloc = args['data'][args['hue']]
             if(type(nlpi.pp['stheme']) is str):
                 palette = nlpi.pp['stheme']
             else:
                 palette = palette_rgb[:len(hueloc.value_counts())]
                 
         else:
             hueloc = None
             palette = palette_rgb
         
-        num,cat = self.split_types(args['data'])
+        num,_ = self.split_types(args['data'])
         sns.heatmap(num,cmap=palette,
                     square=False,lw=2,
                     annot=True,cbar=True)    
                     
         plt.show()
         nlpi.resetpp()
     
-        
+'''
+
+Corpus
+
+'''
+
 corpus_edaplt = OrderedDict({  
                             
                             
                             'sscatterplot':['create seaborn scatter plot',
                                            'make seaborn scatter plot',
                                            'make seaborn scatterplot',
                                            'create seaborn scatterplot',
```

## mllibs/membedding.py

```diff
@@ -1,10 +1,9 @@
 from keras.preprocessing import text
 from keras.preprocessing.text import Tokenizer
-from keras.preprocessing.sequence import skipgrams
 from collections import Counter
 import torch
 from torch.autograd import Variable
 import torch.optim as optim
 import torch.nn.functional as F
 import numpy as np
 import pandas as pd
@@ -23,15 +22,14 @@
 
 Embedding Generation Only
 
 '''
 
 # in this module we generate embedding vectors, store in dataframe
 
-
 class embedding(nlpi):
     
     def __init__(self,nlp_config):
         self.name = 'nlp_embedding'
         self.nlp_config = nlp_config 
         self.select = None
         self.data = None
```

## mllibs/mencoder.py

```diff
@@ -1,13 +1,11 @@
-from sklearn.preprocessing import OneHotEncoder, LabelEncoder
+from sklearn.preprocessing import LabelEncoder
 from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
-from collections import Counter
 from mllibs.nlpi import nlpi
 import pandas as pd
-from collections import OrderedDict
 import warnings; warnings.filterwarnings('ignore')
 from sklearn.base import clone
 from copy import deepcopy
 import torch
 from nltk.tokenize import word_tokenize
 from torch.nn.utils.rnn import pad_sequence
 
@@ -72,28 +70,36 @@
             self.cv(self.data,self.args)  
         elif(self.select == 'tfidf_vectoriser'):
             self.tfidf(self.data,self.args)
         elif(self.select == 'torch_text_encode'):
             self.text_torch_encoding(self.data,self.args)
             
             
-    ''' One Hot Encode DataFrame '''
+    ''' 
+    
+    One Hot Encode DataFrame 
+    
+    '''
             
     def ohe(self,data:pd.DataFrame,args):
            
         # if just data is specified
         if(self.subset is None):     
             df_matrix = pd.get_dummies(data)
             nlpi.memory_output.append({'data':df_matrix})
         else:      
             df_matrix = pd.get_dummies(data,columns=self.subset)
             nlpi.memory_output.append({'data':df_matrix})
                    
         
-    ''' Label Encode DataFrame column '''
+    ''' 
+    
+    Label Encode DataFrame column 
+    
+    '''
 
     def le(self,data:pd.DataFrame,args):
         
         encoder = LabelEncoder()
         data = deepcopy(data)
         
         if(self.subset is None):
@@ -125,15 +131,19 @@
                                           'vectoriser':lencoder})      
             else:     
                 add_label = pd.concat([data,lst_df[0]],axis=1)
                 nlpi.memory_output.append({'data':add_label,
                                       'vectoriser':lencoder})      
                 
    
-    ''' CountVectoriser '''
+    ''' 
+    
+    CountVectoriser 
+    
+    '''
 
     def cv(self,data:pd.DataFrame,args):
                     
         # preset value dictionary
         pre = {'ngram_range':(1,1),'min_df':1,'max_df':1.0}
         data = deepcopy(data)
         
@@ -177,15 +187,19 @@
             else:
             
                 add_label = pd.concat([data,lst_df[0]],axis=1)
                 nlpi.memory_output.append({'data':add_label,
                                           'vectoriser':lvectoriser})
             
     
-    ''' TF-IDF '''
+    ''' 
+    
+    TF-IDF
+    
+    '''
     
     def tfidf(self,data:pd.DataFrame,args):
             
         pre = {'ngram_range':(1,1),'min_df':1,'max_df':1.0,
                'smooth_idf':True,'use_idf':True}
         
         # create new object
@@ -233,15 +247,19 @@
                 
             else:
             
                 add_label = pd.concat([data,lst_df[0]],axis=1)
                 nlpi.memory_output.append({'data':add_label,
                                        'vectoriser':vectoriser})
         
-    ''' Encode a corpus of documents to a numeric tensor '''
+    ''' 
+    
+    Encode a corpus of documents to a numeric tensor 
+    
+    '''
                 
     def text_torch_encoding(self,data:list,args):
         
         ''' Tokenise Documents '''
         
         lst_tokens = []
         for doc in data:
@@ -267,14 +285,19 @@
     
         # pad tensor if required    
         if(args['maxlen'] is not None):
             padded_vals = padded_vals[:,:eval(args['maxlen'])]
     
         nlpi.memory_output.append({'data':padded_vals,'dict':word2id})
         
+'''
+
+Corpus
+
+'''
     
 # corpus for module
 dict_nlpencode = {'encoding_ohe':['one hot encode',
                                   'one-hot-encode',
                                   'ohe',
                                   'one-hot encode',
                                   'encode with one-hot-encoding',
```

## mllibs/moutliers.py

```diff
@@ -11,61 +11,92 @@
 def hex_to_rgb(h):
     h = h.lstrip('#')
     return tuple(int(h[i:i+2], 16)/255 for i in (0, 2, 4))
 
 palette = ['#b4d2b1', '#568f8b', '#1d4a60', '#cd7e59', '#ddb247', '#d15252']
 palette_rgb = [hex_to_rgb(x) for x in palette]
 
+'''
+
+FIND OUTLIERS IN DATA
+
+'''
+
 class data_outliers(nlpi):
     
     # called in nlpm
     def __init__(self,nlp_config):
         self.name = 'outliers'          
         self.nlp_config = nlp_config  
         
     # called in nlpi
+
     def sel(self,args:dict):
                   
         select = args['pred_task']
-            
+        self.data_name = args['data_name']  # name of the data
+
         if(select == 'outlier_iqr'):
             self.outlier_iqr(args)
         elif(select == 'outlier_zscore'):
             self.outlier_zscore(args)
         elif(select == 'outlier_norm'):
             self.outlier_normal(args)
         elif(select == 'outlier_dbscan'):
             self.outlier_dbscan(args)
 
     @staticmethod
+    def sfp(args,preset,key:str):
+        
+        if(args[key] is not None):
+            return eval(args[key])
+        else:
+            return preset[key] 
+        
+    # set general parameter
+        
+    @staticmethod
+    def sgp(args,key:str):
+        
+        if(args[key] is not None):
+            return eval(args[key])
+        else:
+            return None
+
+    @staticmethod
     def split_types(df):
         numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  
         numeric = df.select_dtypes(include=numerics)
         categorical = df.select_dtypes(exclude=numerics)
         return numeric,categorical
+    
+    ''' 
+    
+    ACTIVATION FUNCTIONS 
+    
+    '''
+    # outlier_iqr
+    # outlier_zscore
+    # outlier_normal
+    # outlier_dbscan
             
     # find outliers using IQR values
         
-    @staticmethod
-    def outlier_iqr(args:dict):
+    def outlier_iqr(self,args:dict):
         
-        df = args['data']
+        pre = {'scale':1.5}
 
-        # get the indicies of data outside 1.5 x IQR 
-        
-        if(args['scale'] is None):
-            scale = 1.5  # set default
-        else:
-            scale = eval(args['scale']) # update value
+        df = args['data']
+        scale = self.sfp(args,pre,'scale')
 
+        # helper function
         def get_iqroutlier(df):
 
             dict_outlier_index = {}
-            dict_outlier = {}
-            for k, v in df.items():
+            for _, v in df.items():
                 q1 = v.quantile(0.25);
                 q3 = v.quantile(0.75);
                 irq = q3 - q1
 
                 # select data
                 v_col = v[(v <= q1 - scale * irq) | (v >= q3 + scale * irq)]
                 dict_outlier_index[v_col.name] = list(v_col.index)
@@ -74,146 +105,155 @@
         
         # return dictionary containing indicies of outliers
         dict_outlier_index = get_iqroutlier(df)
         
         def label_outliers(data):
             ldata = data.copy()
             ldata.loc[:,'outlier_iqr'] = 'internal'
-            for k,v in data.items():
+            for _,v in data.items():
                 if(len(dict_outlier_index[v.name]) != 0):
                     ldata.loc[dict_outlier_index[v.name],'outlier_iqr'] = v.name
             return ldata
         
         ldata = label_outliers(df)
-        nlpi.memory_output.append(ldata)
 
+         # store relevant data about operation in data source
+        nlpi.memory_output.append({'data':ldata})
+
+         # store relevant data about operation in data source
+        nlpi.data[self.data_name[0]]['outlier'][f'outlier_iqr_{nlpi.iter}'] = ldata['outlier_iqr']
         
     # find outliers using z_scores
 
     def outlier_zscore(self,args:dict):
+
+        pre = {'threshold':3}
         
         df = args['data']
-        num,cat = self.split_types(args['data'])
-        
-        if(args['threshold'] is None):
-            threshold = 3
-        else:
-            threshold = eval(args['threshold'])
+        num,_ = self.split_types(args['data'])
         
         def outliers_z_score(ys, threshold):
             mean_y = np.mean(ys)
             std_y = np.std(ys)
             z_scores = [(y - mean_y) / std_y for y in ys]
             return np.where(np.abs(z_scores) > threshold)[0]
     
         dict_outlier_index = {}
-        for k, v in num.items():
-            dict_outlier_index[v.name] = list(outliers_z_score(v,threshold))
+        for _, v in num.items():
+            dict_outlier_index[v.name] = list(outliers_z_score(v,self.sfp(args,pre,'threshold')))
             
         def label_outliers(data):
             ldata = data.copy()
             ldata.loc[:,'outlier_zscore'] = 'internal'
-            for k,v in data.items():
+            for _,v in data.items():
                 if(len(dict_outlier_index[v.name]) != 0):
                     ldata.loc[dict_outlier_index[v.name],'outlier_zscore'] = v.name
             return ldata
         
         ldata = label_outliers(df)
-        nlpi.memory_output.append(ldata)
+
+         # store relevant data about operation in data source
+        nlpi.memory_output.append({'data':ldata})
+
+        # store relevant data about operation in data source
+        nlpi.data[self.data_name[0]]['outlier'][f'outlier_zscore_{nlpi.iter}'] = ldata['outlier_zscore']
      
     # find outliers using normal distribution
     
-    @staticmethod
-    def outlier_normal(args:dict):
+    def outlier_normal(self,args:dict):
+
+        pre = {'threshold':0.014}
  
         def estimate_gaussian(dataset):
             mu = np.mean(dataset, axis=0)
             sigma = np.cov(dataset.T)
             return mu, sigma
 
         def get_gaussian(mu, sigma):
             distribution = norm(mu, sigma)
             return distribution
 
         def get_probs(distribution, dataset):
             return distribution.pdf(dataset)
         
-        # loop through all columns
-        
-        if(args['threshold'] is None):
-            threshold = 0.014
-        else:
-            threshold = eval(args['threshold'])
-        
+        # loop through all columns 
         df = args['data']
    
         dict_outlier_index = {}
-        for k, v in df.items():
+        for _, v in df.items():
             
             # standardisation of columns
             w = v.to_frame()
             w_sc = StandardScaler().fit_transform(w)        
             v = pd.Series(w_sc[:,0],name=v.name)
             mu, sigma = estimate_gaussian(v.dropna())
             distribution = get_gaussian(mu, sigma)
 
             # calculate probability of the point appearing
             probabilities = get_probs(distribution,v.dropna())        
-            dict_outlier_index[v.name] = np.where(probabilities < threshold)[0]
+            dict_outlier_index[v.name] = np.where(probabilities < self.sfp(args,pre,'threshold'))[0]
             
         def label_outliers(data):
             ldata = data.copy()
             ldata.loc[:,'outlier_normal'] = 'internal'
-            for k,v in data.items():
+            for _,v in data.items():
                 if(len(dict_outlier_index[v.name]) != 0):
                     ldata.loc[dict_outlier_index[v.name],'outlier_normal'] = v.name
             return ldata
         
         ldata = label_outliers(df)
-        nlpi.memory_output.append(ldata)
+
+        # store relevant data about operation in data source
+        nlpi.memory_output.append({'data':ldata})
+
+        # store relevant data about operation in data source
+        nlpi.data[self.data_name[0]]['outlier'][f'outlier_normal_{nlpi.iter}'] = ldata['outlier_normal']
         
     # find outliers using dbscan
         
     def outlier_dbscan(self,args:dict):
+
+        pre = {'eps':2.4,'min_samples':3}
             
         df = args['data']
-        num,cat = self.split_types(args['data'])
+        num,_ = self.split_types(args['data'])
    
         w_sc = StandardScaler().fit_transform(num)        
         v = pd.DataFrame(w_sc,
                          columns = num.columns, 
                          index = num.index)
         
-        if(args['eps'] is None):
-            eps = 2.4
-        else:
-            eps = eval(args['eps'])
-            
-        if(args['min_samples'] is None):
-            min_samples = 3
-        else:
-            min_samples = eval(args['min_samples'])
-        
-        db = DBSCAN(eps=eps,
-                    min_samples=min_samples).fit(v)
+        db = DBSCAN(eps=self.sfp(args,pre,'eps'),
+                    min_samples=self.sfp(args,pre,'min_samples')).fit(v)
         
         # for all features the same
         dict_outlier_index = OrderedDict({new_list: np.where(db.labels_ == -1)[0] for new_list in list(num.columns)})
         first = dict_outlier_index[[*dict_outlier_index.keys()][0]]
 
         def label_outliers(data):
             ldata = data.copy()
             ldata.loc[:,'outlier_dbscan'] = 'internal'
             ldata.loc[first,'outlier_dbscan'] = 'outlier'
             return ldata
             
         ldata = label_outliers(df)
-        nlpi.memory_output.append(ldata)
-        
-            
+
+        # store relevant data about operation in data source
+        nlpi.memory_output.append({'data':ldata,
+                                   'model':db})
+
+        # store relevant data about operation in data source
+        nlpi.data[self.data_name[0]]['outlier'][f'outlier_dbscan_{nlpi.iter}'] = ldata['outlier_dbscan']
+
+                    
+'''
+
+Corpus
+
+'''
     
 corpus_outliers = OrderedDict({"outlier_iqr":['find outliers in data using IQR',
                                            'find outliers using IQR',
                                            'get IQR outliers',
                                            'find IQR outliers',
                                            'find IQR outlier',
                                            'get outliers using IRQ',
@@ -250,37 +290,41 @@
                             
                             
 info_outliers = {'outlier_iqr': {'module':'outliers',
                                 'action':'action',
                                 'topic':'topic',
                                 'subtopic':'sub topic',
                                 'input_format':'pd.DataFrame',
-                                'description':'find outliers using inter quartile range (IQR)'},
+                                'description':'find outliers using inter quartile range (IQR)',
+                                'arg_compat':'scale'},
               
              'outlier_zscore': {'module':'outliers',
                                 'action':'action',
                                 'topic':'topic',
                                 'subtopic':'sub topic',
                                 'input_format':'pd.DataFrame',
-                                'description':'find outliers using zscore'},
+                                'description':'find outliers using zscore',
+                                'arg_compat':'threshold'},
                  
                 
              'outlier_norm': {'module':'outliers',
                                 'action':'action',
                                 'topic':'topic',
                                 'subtopic':'sub topic',
                                 'input_format':'pd.DataFrame',
-                                'description':'find outliers using normal distribution'},
+                                'description':'find outliers using normal distribution',
+                                'arg_compat':'threshold'},
                  
                  
              'outlier_dbscan': {'module':'outliers',
-                                'action':'action',
-                                'topic':'topic',
-                                'subtopic':'sub topic',
+                                'action':'detect outliers',
+                                'topic':'outlier detection',
+                                'subtopic':'dbscan',
                                 'input_format':'pd.DataFrame',
-                                'description':'find outliers using unsupervised learning algorithm DBSCAN'},
+                                'description':"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can be used to detect outliers. DBSCAN is a clustering algorithm that groups together points that are close to each other based on a density criterion. Points that do not belong to any cluster are considered outliers or noise. The algorithm identifies these points by looking for areas of low density, where the distance between neighboring points is greater than a specified threshold. These points are then labeled as outliers. Therefore, DBSCAN can be used as an effective method for outlier detection in datasets where the majority of the data points form clusters.",
+                                'arg_compat':'eps min_samples',},
 
                  
               }
                          
 # configuration dictionary (passed in nlpm)
 configure_outliers = {'corpus':corpus_outliers,'info':info_outliers}
```

## mllibs/mpd_df.py

```diff
@@ -14,70 +14,109 @@
 # sample module class structure
 class dataframe_oper(nlpi):
     
     # called in nlpm
     def __init__(self,nlp_config):
         self.name = 'pd_df'             
         self.nlp_config = nlp_config 
+
+    @staticmethod
+    def sfp(args,preset,key:str):
+        
+        if(args[key] is not None):
+            return eval(args[key])
+        else:
+            return preset[key] 
+        
+    # set general parameter
+        
+    @staticmethod
+    def sgp(args,key:str):
+        
+        if(args[key] is not None):
+            return eval(args[key])
+        else:
+            return None
         
     # called in nlpi
     def sel(self,args:dict):
         
         self.select = args['pred_task']
         self.args = args
         
         if(self.select == 'groupby'):
             self.dfgroupby(self.args)
         elif(self.select == 'concat'):
             self.dfconcat(self.args)
         elif(self.select == 'subset_concat'):
             self.subset_label(self.args)
             
+    ''' 
+    
+    ACTIVATION FUNCTIONS 
+
+    '''
+    # dfgroupby
+    # dfconcat
+    # subset_label
+
     # Groupby DataFrame (or Pivot Table)
     
     def dfgroupby(self,args:dict):
 
-        def groupby(df:pd.DataFrame,i:str,c:str=None,v:str=None,agg='mean'):
+        pre = {'agg':'mean'}
+
+        # groupby helper function
+        def groupby(df:pd.DataFrame, # input dataframe
+                    i:str,           # index
+                    c:str=None,      # column
+                    v:str=None,      # value
+                    agg='mean'       # aggregation function
+                    ):
     
+            # pivot table / standard groupby
             if(i is not None or v is not None):
                 return pd.pivot_table(data=df,
-                						  index = i,
-                						  columns=c,
-                						  values=v,
-                						  aggfunc=agg)
+                                      index = i,
+                                      columns=c,
+                                      values=v,
+                                      aggfunc=agg)
             else:
                 return df.groupby(by=i).agg(agg)
         
+        # general groupby function (either pivot_table or groupby)
         grouped_data = groupby(args['data'],
                                args['row'],
                                c=args['col'],
                                v=args['val'],
-                               agg=args['agg'])
-                                
-        nlpi.memory_output.append(grouped_data)
+                               agg=self.sfp(args,pre,'agg'))
+           
+        nlpi.memory_output.append({'data':grouped_data})
                 
     # Merge DataFrames
 
     def dfconcat(self,args:dict):
+
+        pre = {'axis':0}
         
         def concat(lst_df,join='outer',ax=0):
             return pd.concat(lst_df,
-				               join=join,
-            					 axis=ax,
-            					 )
+				             join=join,
+            				 axis=ax,
+            				)
             
         # merge both data frames
         merged_df = concat(args['data'],
                            join=args['join'],
-                           
-                           ax=eval(args['axis']))
+                           ax=self.sfp(args,pre,'axis'))
         
         # store result
-        nlpi.memory_output.append(merged_df)
+        nlpi.memory_output.append({'data':merged_df})
         
+    # Add subset label for two dataframes
         
     def subset_label(self,args:dict):
     
         if(type(args['data']) is list):
         
             df1 = args['data'][0]
             df2 = args['data'][1]
@@ -98,80 +137,79 @@
                 df1['set'] = 'first'
                 df2['set'] = 'second'
                 
                 return pd.concat([df1,df2],axis=0)
     
             merged_df = subset_merge(df1,df2)
             merged_df.reset_index(inplace=True)
-            nlpi.memory_output.append(merged_df)
+            nlpi.memory_output.append({'data':merged_df})
             
 '''
 
 Corpus
 
 
 '''
-        
-        
+          
 corpus_pda = OrderedDict({})
 corpus_pda['groupby'] =  ['pandas groupby',
                           'groupby operation',
                           'group by pandas',
                           'dataframe groupby',
                           'pivot table groupby',
                           'pivot table',
                           'pivot data',
                           'pivot operation',
                           'do pivot operation']
-                             
-                             
+                                           
 corpus_pda['concat'] = ['concat dataframe',
                         'concatenate dataframe',
                         'concat df',
                         'merge dataframe',
                         'merge df',
                         'combine df',
                         'combine dataframes']
                         
 corpus_pda['subset_concat'] = ['merge subsets',
 								  'concat subsets',
 								  'concatenate subsets',
 								  'compare subset dataframes',
 								  'compare subset df',
-           					  'label subset dataframes',
+                                  'label subset dataframes',
             					  'create subset dataframe labels',
         
 ]
  
 '''
 
 Module Information Dictionary
 
-
 '''
 
  
 info_pda = {}
              
 info_pda['groupby'] = {'module':'pd_df',
                       'action':'action',
                       'topic':'topic',
                       'subtopic':'sub topic',
                       'input_format':'pd.DataFrame',
-                      'description':'pandas groupby operation, data wrangling with index, column and values'}
+                      'description':'pandas groupby operation, data wrangling with index, column and values',
+                      'arg_compat':'agg'}
 
 info_pda['concat'] = {'module':'pd_df',
                       'action':'action',
                       'topic':'topic',
                       'subtopic':'sub topic',
                       'input_format':'pd.DataFrame',
-                      'description':'merge together two dataframes'}
+                      'description':'merge together two dataframes',
+                      'arg_compat':'axis'}
                       
 info_pda['subset_concat'] = {'module':'pd_df',
-                     			 'action':'action',
+                     		 'action':'action',
                       		 'topic':'topic',
                       		 'subtopic':'sub topic',
-		                      'input_format':'pd.DataFrame',
-              			        'description':'label two subset dataframes'}
+		                     'input_format':'pd.DataFrame',
+              			     'description':'label two subset dataframes'}
 
 
 configure_pda = {'corpus':corpus_pda,'info':info_pda}
```

## mllibs/mseda.py

```diff
@@ -13,15 +13,14 @@
 
 class simple_eda(nlpi):
     
     def __init__(self,nlp_config):
         self.name = 'eda'             # unique module name identifier
         self.nlp_config = nlp_config  # text based info related to module
         self.select = None            # store index which module was activated
-        self.data = None              # store data which is to be used in activated 
         # store all module arguments which were passed to module
         self.args = None              
     
     # describe contents of class
                     
     def sel(self,args:dict):
         
@@ -67,14 +66,20 @@
         corr_mat = corr_mat.dropna(how='all',axis=1)
         display(corr_mat)
         
                                     
     @staticmethod
     def show_info(args:dict):
         print(args['data'].info())
+
+'''
+
+Corpus
+
+'''
         
 # Create Dataset of possible commands
 corpus_eda = OrderedDict({
     
                 'show_info' : ['show data information',
                                'show dataset information',
                                'show dataframe information',
```

## mllibs/msllinear.py

```diff
@@ -1,313 +1,82 @@
 from sklearn.linear_model import LinearRegression, LogisticRegression,Ridge, RidgeClassifier, Lasso, ElasticNet, BayesianRidge
-from sklearn.metrics import classification_report 
-from sklearn.metrics import mean_squared_error
-from mllibs.nlpi import nlpi
-import pandas as pd
-import numpy as np
+from mllibs.common_eval import eval_base
 
 
 '''
 
 LINEAR MODEL MODULE
 
 '''
 
-class sllinear(nlpi):
-    
+# requires parent evaluation class 
+# we only need to replace class name & set model attribute/method
+
+class sllinear(eval_base):
+
     def __init__(self,nlp_config):
         self.name = 'sllinear'
-        self.nlp_config = nlp_config 
-        self.select = None
-        self.args = None
-        
-    @staticmethod
-    def sfp(args,preset,key:str):
-        
-        if(args[key] is not None):
-            return eval(args[key])
-        else:
-            return preset[key] 
-        
-    # set general parameter
-        
-    @staticmethod
-    def sgp(args,key:str):
-        
-        if(args[key] is not None):
-            return eval(args[key])
-        else:
-            return None
-           
-    # make selection  
-
-    def sel(self,args:dict):
-    
-        self.select = args['pred_task']
-        self.args = args    
-        
-        # check feature and target variable have been defined
-        set_feat = False; set_target = False
-        if(len(self.args['features']) > 1):
-            features = self.args['data'][self.args['features']]
-            set_feat = True
-        if(len(self.args['target']) == 1):
-            target = self.args['data'][self.args['target'][0]]
-            set_target = True
-        
-        if(set_feat is not True and set_target is not True):
-            print('features and target not set correctly')
-            return 
-        
-        ''' select appropriate predicted method '''
-        
-        # linear regression
-        
-        if(self.select == 'fit_lr'):
-            self.sklinear_lr(features,target,self.args)
-        elif(self.select == 'lr_fpred'):
-            self.sklinear_lr_fpred(features,target,self.args)
-            
-        # logistic regression
-            
-        elif(self.select == 'fit_lgr'):
-            self.sklinear_lgr(features,target,self.args)
-        elif(self.select == 'lgr_fpred'):
-            self.sklinear_lgr_fpred(features,target,self.args)
+        eval_base.__init__(self,nlp_config,self.name)
 
-        # ridge regression
-            
-        elif(self.select == 'fit_ridge'):
-            self.sklinear_ridge(features,target,self.args)
-        elif(self.select ==  'ridge_fpred'):
-            self.sklinear_ridge_fpred(features,target,self.args)
-            
-        # ridge classification
-            
-        elif(self.select == 'fit_cridge'):
-            self.sklinear_cridge(features,target,self.args)
-        elif(self.select ==  'cridge_fpred'):
-            self.sklinear_cridge_fpred(features,target,self.args)   
-            
-        # lasso regression
-        
-        elif(self.select == 'fit_lasso'):
-            self.sklinear_lasso(features,target,self.args)
-        elif(self.select ==  'lasso_fpred'):
-            self.sklinear_lasso_fpred(features,target,self.args)
-            
-        # elasticnet regression
-            
-        elif(self.select == "fit_elastic"):
-            self.sklinear_elasticnet(features,target,self.args)
-        elif(self.select ==  'elastic_fpred'):
-            self.sklinear_elastic_fpred(features,target,self.args)                 
-            
-        # bridge regression
-            
-        elif(self.select == "fit_bridge"):
-            self.sklinear_bridge(features,target,self.args)
-        elif(self.select ==  'bridge_fpred'):
-            self.sklinear_bridge_fpred(features,target,self.args)
-            
-            
-    # fit linear regression model
-            
-    def sklinear_lr(self,features,target,args:dict):
 
-        model = LinearRegression()
-        model.fit(features,target)
-        nlpi.memory_output.append({'data':features,
-                                   'target':target,
-                                   'model':model}) 
-        
-    # fit logistic regression model
-        
-    def sklinear_lgr(self,features,target,args:dict):
-            
-        pre = {'const':1.0}
-        model = LogisticRegression(c=self.sfp(args,pre,'const'))
-        model.fit(features,target)
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model}) 
-        
-    # fit ridge regression model (w/ regularisation)
-        
-    def sklinear_ridge(self,features,target,args:dict):
+    def set_model(self,args):
 
-        pre = {'const':1.0}
-        model = Ridge(alpha=self.sfp(args,pre,'const'))
-        model.fit(features,target)
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model}) 
-        
-    # fit lasso regression model (w/ regularisation)    
-        
-    def sklinear_lasso(self,features,target,args:dict):
+        # select model (7 variations - 5 regressors / 2 classifiers)
 
-        pre = {'const':1.0}
-        model = Lasso(alpha=self.sfp(args,pre,'const'))
-        model.fit(features,target)
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model}) 
-        
-    def sklinear_elasticnet(self,features,target,args:dict):
+        if(self.select == 'fit_lr' or self.select == 'lr_fpred'):   
 
-        pre = {'const':1.0,'l1_ratio':0.5}
-        model = ElasticNet(alpha=self.sfp(args,pre,'const'),
-                      l1_ratio=self.sfp(args,pre,'l1_ratio'))
-        model.fit(features,target)
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model}) 
+            self.model_type = 'reg'
+            self.model = LinearRegression()
 
-        
-    def sklinear_cridge(self,features,target,args:dict):
+        elif(self.select == 'fit_lgr' or self.select == 'lgr_fpred'):
 
-        pre = {'const':1.0}
-        model = RidgeClassifier(alpha=self.sfp(args,pre,'const'))
-        model.fit(features,target)
-        nlpi.memory_output.append({'feature':features,
-                                   'target':target,
-                                   'model':model}) 
-        
-        
-    def sklinear_bridge(self,features,target,args:dict):
+            self.model_type = 'class'
+            pre = {'const':1.0}
+            self.model = LogisticRegression(c=self.sfp(args,pre,'const'))
 
-        pre = {'alpha_1':1e-6,'alpha_2':1e-6,'lambda_1':1e-6,'lambda_2':1e-6}
-        model = BayesianRidge(alpha_1=self.sfp(args,pre,'alpha_1'),
-                              alpha_2=self.sfp(args,pre,'alpha_2'),
-                              lambda_1=self.sfp(args,pre,'lambda_1'),
-                              lambda_2=self.sfp(args,pre,'lambda_2')
-                             )
-        model.fit(features,target)
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model}) 
-       
-    # fit and predict linear regression model
-    
-    def sklinear_lr_fpred(self,features,target,args:dict):
-        
-        model = LinearRegression()
-        model.fit(features,target)
-        y_pred = model.predict(features)
-        mse = mean_squared_error(target,y_pred,squared=False)
-        
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model,
-                                   'y_pred':y_pred,
-                                   'rmse':mse}) 
-        
-    # fit and predict logistic regression model
-    
-    def sklinear_lgr_fpred(self,features,target,args:dict):
-        
-        model = LogisticRegression()
-        model.fit(features,target)
-        y_pred = model.predict(features)
-        report = classification_report(target,y_pred)
-        
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model,
-                                   'y_pred':y_pred,
-                                   'report':report}) 
-        
-    # fit and predict ridge regression
-        
-    def sklinear_ridge_fpred(self,features,target,args:dict):
-        
-        pre = {'const':1.0}
-        model = Ridge(alpha=self.sfp(args,pre,'const'))
-        model.fit(features,target)
-        y_pred = model.predict(features)
-        mse = mean_squared_error(target,y_pred,squared=False)
-        
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model,
-                                   'y_pred':y_pred,
-                                   'rmse':mse}) 
-        
-    # fit and predict lasso regression model
-    
-    def sklinear_lasso_fpred(self,features,target,args:dict):
-        
-        pre = {'const':1.0}
-        model = Lasso(alpha=self.sfp(args,pre,'const'))
-        model.fit(features,target)
-        y_pred = model.predict(features)
-        mse = mean_squared_error(target,y_pred,squared=False)
-        
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model,
-                                   'y_pred':y_pred,
-                                   'rmse':mse}) 
-        
-    # fit and predict elastic regression
-        
-    def sklinear_elastic_fpred(self,features,target,args:dict):
-        
-        pre = {'const':1.0,'l1_ratio':0.5}
-        model = ElasticNet(alpha=self.sfp(args,pre,'const'),
-                           l1_ratio=self.sfp(args,pre,'l1_ratio'))
-        model.fit(features,target)
-        y_pred = model.predict(features)
-        mse = mean_squared_error(target,y_pred,squared=False)
-        
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model,
-                                   'y_pred':y_pred,
-                                   'rmse':mse
-                                }) 
-        
-    # fit and predict ridge classification model
-    
-    def sklinear_cridge_fpred(self,features,target,args:dict):
-        
-        pre = {'const':1.0}
-        model = RidgeClassifier(alpha=self.sfp(args,pre,'const'))
-        model.fit(features,target)
-        y_pred = model.predict(features)
-        report = classification_report(target,y_pred)
-        
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model,
-                                   'y_pred':y_pred,
-                                   'report':report}) 
-        
-    # fit and predict bridge regression
-        
-    def sklinear_bridge_fpred(self,features,target,args:dict):
-        
-        pre = {'alpha_1':1e-6,'alpha_2':1e-6,'lambda_1':1e-6,'lambda_2':1e-6}
-        model = BayesianRidge(alpha_1=self.sfp(args,pre,'alpha_1'),
-                              alpha_2=self.sfp(args,pre,'alpha_2'),
-                              lambda_1=self.sfp(args,pre,'lambda_1'),
-                              lambda_2=self.sfp(args,pre,'lambda_2')
-                             )
-        
-        model.fit(features,target)
-        y_pred = model.predict(features)
-        mse = mean_squared_error(target,y_pred,squared=False)
-        
-        nlpi.memory_output.append({'features':features,
-                                   'target':target,
-                                   'model':model,
-                                   'y_pred':y_pred,
-                                   'rmse':mse
-                                }) 
+        elif(self.select == 'fit_ridge' or self.select == 'ridge_fpred'):
+
+            self.model_type = 'reg'
+            pre = {'const':1.0}
+            self.model = Ridge(alpha=self.sfp(args,pre,'const'))
+
+        elif(self.select == 'fit_cridge' or self.select == 'cridge_fpred'):
+
+            self.model_type = 'class'
+            pre = {'const':1.0}
+            self.model = RidgeClassifier(alpha=self.sfp(args,pre,'const'))
+
+        elif(self.select == 'fit_lasso'):
+
+            self.model_type = 'reg'
+            pre = {'const':1.0}
+            self.model = Lasso(alpha=self.sfp(args,pre,'const'))
+
+        elif(self.select == 'fit_elastic' or self.select == 'elastic_fpred'):
+
+            self.model_type = 'reg'
+            pre = {'const':1.0,'l1_ratio':0.5}
+            self.model = ElasticNet(alpha=self.sfp(args,pre,'const'),
+                                l1_ratio=self.sfp(args,pre,'l1_ratio'))
+            
+        elif(self.select == 'fit_bridge' or self.select == 'bridge_fpred'):
+
+            self.model_type = 'reg'
+            pre = {'alpha_1':1e-6,'alpha_2':1e-6,'lambda_1':1e-6,'lambda_2':1e-6}
+            self.model = BayesianRidge(alpha_1=self.sfp(args,pre,'alpha_1'),
+                                    alpha_2=self.sfp(args,pre,'alpha_2'),
+                                    lambda_1=self.sfp(args,pre,'lambda_1'),
+                                    lambda_2=self.sfp(args,pre,'lambda_2')
+                                    )
+
+''' 
+
+Corpus
+
+'''
         
           
 dict_sllinear = {'fit_lr':['create a linear regression model',
                            'create LinearRegression model',
                            'create linear regression model',
                            'create linear model',
                            'create LinearRegression',
```

## mllibs/mtextnorm.py

```diff
@@ -1,18 +1,15 @@
 # importing the dependencies needed for pre processing
 import re
 from nltk.corpus import stopwords
 from nltk.tokenize import word_tokenize
-from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer
+from nltk.stem import WordNetLemmatizer, SnowballStemmer
 import emoji
 import string
-from copy import deepcopy
-from collections import OrderedDict
 from mllibs.nlpi import nlpi
-import numpy as np
 import pandas as pd
 import spacy
 
 
 class cleantext(nlpi):
     
     def __init__(self,nlp_config):
```

## mllibs/musldimred.py

```diff
@@ -233,14 +233,19 @@
         model.fit(data)
         X = pd.DataFrame(model.transform(data),index=self.catn.index)
         X.columns = [f'dim_{i}' for i in range(0,self.sfp(args,pre,'dim')) ]
     
         nlpi.memory_output.append({'data':pd.concat([X,self.catn],axis=1),
                                    'model':model}) 
     
+'''
+
+Corpus
+
+'''
 
 corpus_usldimred = OrderedDict({"PCA":['PCA dimension reduction',
                                        'PCA reduce dimension',
                                        'PCA dimred',
                                        'PCA dimensionality reduction',
                                        'principal component analysis',
                                        'principal component analysis dimension reduction',
```

## mllibs/nlpi.py

```diff
@@ -3,15 +3,14 @@
 import numpy as np
 import pandas as pd
 import random
 import panel as pn
 from nltk.tokenize import word_tokenize, WhitespaceTokenizer 
 from inspect import isfunction
 from seaborn import load_dataset
-import pkgutil
 
 # default plot palette
 
 def hex_to_rgb(h):
     h = h.lstrip('#')
     return tuple(int(h[i:i+2], 16)/255 for i in (0, 2, 4))
 
@@ -20,57 +19,59 @@
 
 ########################################################################
 
 
 # interaction & tect interpreter class
  
 class nlpi(nlpm):
+
+    data = {}    # dictionary for storing data
+    iter = -1    # keep track of all user requests
+    memory_name = []                 # store order of executed tasks
+    memory_stack = []                # memory stack of task information
+    memory_output = []
     
     # instantiation requires module
-    
     def __init__(self,module=None,verbose=0):
         
         self.module = module                  # collection of modules
         self._make_task_info()                # create self.task_info
-        
-        self.data = {}                        # dictionary for storing data
         self.dsources = {}                    # store all data source keys
         self.token_data = []                  # store all token data
         self.verbose = verbose                # print output text flag
+        nlpi.silent = False                    
 
-        nlpi.memory_output = []          # keep order of stored operations
-        nlpi.memory_name = []                      # store order of executed tasks
-        nlpi.memory_stack = []                # memory stack of task information
-        nlpi.iter = -1                         # execution iteraction counter
-        
         # class plot parameters
         nlpi.pp = {'alpha':1,'mew':0,'mec':'k','fill':True,'stheme':palette_rgb,'s':30}
         
     # set plotting parameter
         
     def setpp(self,params:dict):
         if(type(params) is not dict):
-            print("plot parameter dictionary: {'alpha':1,'mew':1,'mec':'k',...}")
+            if(nlpi.silent is False):
+                print("plot parameter dictionary: {'alpha':1,'mew':1,'mec':'k',...}")
         else:
             nlpi.pp.update(params)
-            print('plot parameter updated!')
+            if(nlpi.silent is False):
+                print('plot parameter updated!')
    
     @classmethod
     def resetpp(cls):
         nlpi.pp = {'alpha':1,'mew':0,'mec':'k','fill':True,'stheme':palette_rgb,'s':30}
 
     # Check all available data sources, update dsources dictionary
                     
     def check_dsources(self):
         
-        lst_data = list(self.data.keys())            # data has been loaded
+        lst_data = list(nlpi.data.keys())            # data has been loaded
         self.dsources = {'inputs':lst_data}
-                
-        print('inputs:')
-        print(lst_data,'\n')
+               
+        if(nlpi.silent is False): 
+            print('inputs:')
+            print(lst_data,'\n')
         
     ''' 
     
     store data 
     
     '''
     
@@ -78,33 +79,39 @@
     
     @staticmethod
     def split_types(df):
         numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  
         numeric = df.select_dtypes(include=numerics)
         categorical = df.select_dtypes(exclude=numerics)
         return list(numeric.columns),list(categorical.columns)
+
+        
+    ''' 
+    
+    STORE INPUT DATA
     
+    '''
+
     # Load Dataset from Seaborn Repository
     
     def load_dataset(self,name:str,info:str=None):
         
         # load data from seaborn repository             
         data = load_dataset(name)
         self.store(data,name,info)
         
-    # Store input data
-        
     def store(self,data,name:str,info:str=None):
         
 		# dictionary to store data information
-        datainfo = {'data':None,'subset':None,
+        datainfo = {'data':None,'subset':None,'splits':None,
                     'features':None,'target':None,
                     'cat':None,'num':None,
                     'miss':None,'corpus':None,
-                    'size':None,'dim':None}
+                    'size':None,'dim':None,
+                    }
     
                     
         ''' 1. DESCRIPTION TOKENISATION '''
                     
         if(info is not None):
             
             dtokens = self.nltk_wtokeniser(info) # unigram
@@ -179,26 +186,32 @@
                 tokens = len([*filter(lambda x: x >= 5, token_len)]) > 0
                 if(tokens):
                     col_corpus.append(col)
                         
             datainfo['corpus'] = col_corpus  
             
             ''' Determine size of data '''
-            
+    
             datainfo['size'] = data.shape[0]
-            datainfo['dim'] = data.shape[1]                    
+            datainfo['dim'] = data.shape[1]
+
+            # Initialise other storage information
+            datainfo['splits'] = {}   # data subset splitting info
+            datainfo['outliers'] = {}  # determined outliers
+            datainfo['dimred'] = {}    # dimensionally reduced data 
                 
         ''' Store Data '''
         
-        print(f'\ndata information for {name}')
-        print('=========================================')
-        print(datainfo)
+        if(nlpi.silent is False):
+            print(f'\ndata information for {name}')
+            print('=========================================')
+            print(datainfo)
                  
         datainfo['data'] = data
-        self.data[name] = datainfo
+        nlpi.data[name] = datainfo
         
     # activation function list
     
     def fl(self,show='all'):
                             
         # function information
         df_funct = self.task_info
@@ -271,21 +284,22 @@
         # intialise data column in token info
         self.token_info['data'] = np.nan  # store data type if present
         self.token_info['dtype'] = np.nan  # store data type if present
         self.token_info['data'] = self.token_info['data'].astype('Int64')
                     
         # cycle through all available key names    
         dict_tokens = {}
-        for source_name in list(self.data.keys()):
+        for source_name in list(nlpi.data.keys()):
             if(source_name in self.tokens):     
                 if(source_name in dict_tokens):
-                    print('another data source found, overwriting')
-                    dict_tokens[source_name] = self.data[source_name]['data']
+                    if(nlpi.silent is False):
+                        print('another data source found, overwriting')
+                    dict_tokens[source_name] = nlpi.data[source_name]['data']
                 else:
-                    dict_tokens[source_name] = self.data[source_name]['data']
+                    dict_tokens[source_name] = nlpi.data[source_name]['data']
                     
         ''' if we have found matching tokens '''
                     
         if(len(dict_tokens) != 0):
             for token,value in dict_tokens.items():
                 
                 # store data (store index of stored data)
@@ -387,35 +401,38 @@
         
         '''
             
         def predict_module_task(text):
     
             # determine which module to activate
             ms_name = self.module.test_name('ms',text)
-            print(f'using module: {ms_name}')
+            if(nlpi.silent is False):
+                print(f'using module: {ms_name}')
         
             # Available tasks 
             lst_tasks = self.module.module_task_name[ms_name]
             t_pred_p = self.module.test(ms_name,text)  
             t_pred = np.argmax(t_pred_p)
     
             # [2] name o the module task to be called
-            t_name = lst_tasks[t_pred] 
-            print(f'Executing Module Task: {t_name}')
+            t_name = lst_tasks[t_pred]
+            if(nlpi.silent is False): 
+                print(f'Executing Module Task: {t_name}')
 
             # store predictions
             self.task_pred = t_pred
             self.task_name = t_name
             self.module_name = ms_name
         
         # check condition 
         if(len(self.token_split)>1):
             text = ' '.join(self.token_split[0])
-            print('ner split token found, using base text for task prediction')
-            print(f"{text}")
+            if(nlpi.silent is False):
+                print('ner split token found, using base text for task prediction')
+                print(f"{text}")
         else:
             text = self.command
         
         # predict both module and task
         predict_module_task(text)
             
     
@@ -444,31 +461,33 @@
             
         # dataframe containing information of data sources of tokens
         available_data = self.token_info[['data','dtype']].dropna() 
         len_data = len(available_data) # number of rows of data
         
         '''
         
-        //////////////////////////////////////////////////////////////////
-        
         DATA TOKEN IS FOUND 
         
         //////////////////////////////////////////////////////////////////
         
         '''
         
         # if we only have data token, it should be the required function input
         
         if(len_data == 1):
         
-            ldtype = available_data.loc[available_data.index,'dtype'].values[0]
-            ldata = self.get_td(available_data.index)
+            ldtype = available_data.loc[available_data.index,'dtype'].values[0] # get the data type
+            ldata = self.get_td(available_data.index)  # get the data 
             
+            # We have one data source & it meets input function criteria
+            # else try to change data 
+
             if(ldtype == in_format):
                 self.module_args['data'] = self.get_td(available_data.index)
+                self.module_args['data_name'] = available_data.index
                 
             else:
                 
                 # try to convert input data to dataframe
                 if(in_format == 'pd.DataFrame'):
                     self.module_args['data'] = self.convert_to_df(ldata)
                 elif(in_format == 'list'):
@@ -491,23 +510,24 @@
                 
                 self.module_args['data'] = []
                 for idx in list(data_type_match.index):
                     self.module_args['data'].append(self.get_td(idx))
                     
             else:
                 
-                print('[note] more than 2 data sources found')
+                if(nlpi.silent is False):
+                    print('[note] more than 2 data sources found')
 
                 
         else:
-            print('[note] no data has been set')
+            if(nlpi.silent is False):
+                print('[note] no data has been set')
           
         
         ''' 
-        //////////////////////////////////////////////////////////////////
         
         Check for column related tokens 
         
         //////////////////////////////////////////////////////////////////        
         '''
                 
         # indicies for tokens
@@ -525,26 +545,25 @@
             # for each token that was found in dataframe columns            
             for token in column_tokens:
                 
                 # find index where it is located in input command
                 matched_index_in_tokens = self.command.index(token)
                 
                 # all possible options we are interested
-                lst_options = ['x','y','hue','col','row','target','val']
+                lst_options = ['x','y','hue','col','row','target','val','splits']
                 
                 for option in lst_options:
                     
                     for ii,segment in enumerate(tokens_index):
                         if(matched_index_in_tokens in segment):
                             if(self.tokens[ii-1] == option):
                                 self.module_args[option] = token
 
                         
         '''
-        //////////////////////////////////////////////////////////////////
         
         Check general plot setting tokens
         
         //////////////////////////////////////////////////////////////////
         
         '''
         
@@ -552,15 +571,14 @@
             if(self.tokens[self.tokens.index(token)-1] == 'col_wrap'):
                 self.module_args['col_wrap'] = token
             if(self.tokens[self.tokens.index(token)-1] == 'kind'):
                 self.module_args['kind'] = token                                       
                     
         
         '''
-        //////////////////////////////////////////////////////////////////
         
         USE NER TO SORT MODULE ARGS
         
         //////////////////////////////////////////////////////////////////
         '''
         
         # only if input is a dataframe
@@ -679,19 +697,21 @@
                         # pandas operations can require two (eg. concat)
                         elif(len(lst_match) == 2):
     
                             self.module_args[token_name] = []
                             for idx in list(lst_match.index):
                                 self.module_args[token_name].append(self.get_td(idx))
     
-                            print('stored multiple data in subset, please confirm')
+                            if(nlpi.silent is False):
+                                print('stored multiple data in subset, please confirm')
     
                         else:
     
-                            print('please use lists for subset when main data is df')
+                            if(nlpi.silent is False):
+                                print('please use lists for subset when main data is df')
     
                     else:
     
                         print('implement me')
                         
                         
                 else:
@@ -714,19 +734,23 @@
         # lets find some general tokens 
         # uses token taging
         
         for ii,token in enumerate(self.tokens):
             
             lst_gtokens = ['agg','join','axis','bw','splits','shuffle','rs','const',
                           'threshold','scale','eps','min_samples','ngram_range',
-                          'min_df','max_df',
+                          'min_df','max_df','n_splits',
                           'use_idf','smooth_idf','dim','window','epoch','lr',
                           'maxlen','sample','whiten','whiten_solver',
                           'n_neighbours','radius','l1_ratio',
-                          'alpha_1','alpha_2','lambda_1','lambda_2']
+                          'alpha_1','alpha_2','lambda_1','lambda_2',
+                          'estimator','n_estimators','loss','criterion',
+                          'min_samples_leaf','min_samples_split',
+                          'max_depth','max_features','bootstrap','oob_score',
+                          'max_bins','validation_fraction','n_iter_no_change']
             
             for gtoken in lst_gtokens:
                 if(self.tokens[self.tokens.index(token)-1] == gtoken):
                     self.module_args[gtoken] = token
                     
     '''
     
@@ -752,15 +776,15 @@
         
     def _make_task_info(self):
     
         td = self.module.task_dict
         ts = self.module.mod_summary
     
         outs = {}
-        for k,v in td.items():
+        for _,v in td.items():
             for l,w in v.items():
                 r = random.choice(w)
                 outs[l] = r
     
         show = pd.Series(outs,index=outs.keys()).to_frame()
         show.columns = ['sample']
     
@@ -808,30 +832,34 @@
         '''
        
         # user input command
         self.command = command
         
         # Initialise arguments dictionary
        
-        self.module_args = {'pred_task': None, 'data': None,'subset': None,
+        self.module_args = {'pred_task': None, 'data': None,'subset': None,'splits':None,
                             'features': None, 'target' : None,
                             'x': None, 'y': None, 'hue': None,'col':None,'row':None,
-                            'col_wrap':None,'kind':'scatter', 'val':None, 'agg':'mean',
-                            'join':'inner','axis':'0','bw':None,
-                            'figsize':[None,None],'test_size':'0.3',
-                            'splits':'3','shuffle':'True','rs':'32',
+                            'col_wrap':None,'kind':'scatter', 'val':None, 'agg':None,
+                            'join':'inner','axis':None,'bw':None,
+                            'figsize':[None,None],'test_size':None,
+                            'n_splits':None,'shuffle':None,'rs':None,
                             'threshold':None,'eps':None,'min_samples':None,'scale':None,
                             'ngram_range':None,'min_df':None,'max_df':None,
                             'tokeniser':None,'use_idf':None,'smooth_idf':None,
                             'dim':None,'window':None,
                             'epoch':None,'lr':None,'maxlen':None,'const':None,
                             'neg_sample':None,'batch':None,
                             'kernel':None,'sample':None,'whiten':None,'whiten_solver':None,
                             'n_neighbours':None,'radius':None,'l1_ratio':None,
-                            'alpha_1':None,'alpha_2':None,'lambda_1':None,'lambda_2':None
+                            'alpha_1':None,'alpha_2':None,'lambda_1':None,'lambda_2':None,
+                            'estimator':None,'n_estimators':None,'loss':None,
+                            'criterion':None,'min_samples_leaf':None,'min_samples_split':None,
+                            'max_depth':None,'max_features':None,'bootstrap':None,'oob_score':None,
+                            'max_bins':None,'validation_fraction':None,'n_iter_no_change':None
                            }
         
         # update argument dictionary if it was set
         
         if(args is not None):
             self.module_args.update(args)
             
@@ -857,24 +885,20 @@
         '''
         
         # activate relevant class & pass arguments
         nlpi.iter += 1
         
         self.module_args['pred_task'] = self.task_name
         
-        # list of executed tasks
         nlpi.memory_name.append(self.task_name)  
-        
-        # task information series
-        task_info = self.module.mod_summary.loc[nlpi.memory_name[nlpi.iter]] 
-        
-        nlpi.memory_stack.append(task_info)
+        nlpi.memory_stack.append(self.module.mod_summary.loc[nlpi.memory_name[nlpi.iter]] )
         nlpi.memory_info = pd.concat(self.memory_stack,axis=1) # stack task information order
         
         # activate function
+
         self.module.functions[self.module_name].sel(self.module_args)
         
         # if not data has been added
         # initialise output data (overwritten in module.function
         
         if(len(nlpi.memory_output) == nlpi.iter+1):
             pass
```

## mllibs/nlpm.py

```diff
@@ -2,30 +2,29 @@
 from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
 from sklearn.linear_model import LogisticRegression
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.metrics.pairwise import cosine_similarity
 from sklearn.metrics.pairwise import linear_kernel,sigmoid_kernel
 from sklearn.base import clone
 from collections import OrderedDict
+import pickle
 import numpy as np
 import pandas as pd
-import zipfile
+# import zipfile
 import pkgutil
-import io
 
 import nltk
-nltk.download('wordnet')
+# nltk.download('wordnet')
 
-wordn = '/usr/share/nltk_data/corpora/wordnet.zip'
-wordnt = '/usr/share/nltk_data/corpora/'
+# wordn = '/usr/share/nltk_data/corpora/wordnet.zip'
+# wordnt = '/usr/share/nltk_data/corpora/'
 
-with zipfile.ZipFile(wordn,"r") as zip_ref:
-     zip_ref.extractall(wordnt)
+# with zipfile.ZipFile(wordn,"r") as zip_ref:
+#      zip_ref.extractall(wordnt)
 
-from nltk.stem import WordNetLemmatizer
 from nltk.tokenize import word_tokenize, WhitespaceTokenizer
 
 
 '''
 
 
 NLPM class
@@ -275,17 +274,14 @@
 
         ''' Train model on numeric corpus '''
         
         model_lr = LogisticRegression()
         model = clone(model_lr)
         model.fit(X,y)
         self.model[module_name] = model # store model
-
-        # show the training score 
-        y_pred = model.predict(X)
         score = model.score(X,y)
         print(module_name,model,'accuracy',round(score,3))
     
     '''
     
     Train Relevant Models
     
@@ -326,18 +322,14 @@
     '''
     ///////////////////////////////////////////////////////////////
     
     ADDITIONAL MODELS
     
     ///////////////////////////////////////////////////////////////
     
-    ''' 
-    
-    ''' 
-    
     1. CREATE SUBSET DETERMINATION MODEL 
     
     create multiclass classification model which will determine 
     which approach to utilise for the selection of subset features    
     
     '''
      
@@ -368,16 +360,16 @@
         
         lst_tag = [0,0,0,0,0,0,0, 
                    1,1,1,1,1,1,
                    2,2,2,2,2,2,
                    3,3,3,3,3,
                    4,4,4,4,4,
                    5,5,5,5]
-                  
-                  
+        
+                   
         data = pd.DataFrame({'corpus':lst_data,'label':lst_tag})
 
         vectoriser = CountVectorizer(stop_words=['using','use'])
         X = vectoriser.fit_transform(list(data['corpus'])).toarray()
         y = data['label'].values
 
         model = LogisticRegression().fit(X,y)
@@ -393,53 +385,60 @@
     identify key tokens; features, target, subset & data
 
     //////////////////////////////////////////////////////////////////                
     '''
         
     def ner_tokentag_model(self):
 
-        flatten = lambda l: [item for sublist in l for item in sublist]
+        # flatten = lambda l: [item for sublist in l for item in sublist]
         
-        def tokenise(text):
-            return WhitespaceTokenizer().tokenize(text)
+        # def tokenise(text):
+        #     return WhitespaceTokenizer().tokenize(text)
         
-        typea = ['features','feature list','feature columns','independent']
-        typeb = ['target','target column','target variable','dependent']
-        typec = ['subset','subset columns']
-        typed = ['data','data source','source']
-        type_all = typea + typeb + typec + typed
-        tokens = [tokenise(i) for i in type_all]
-        unique_tokens = flatten(tokens)
-        
-        #with open('corpus/wordlist.10000.txt') as f:
-            #lines = f.readlines()
+        # typea = ['features','feature list','feature columns','independent']
+        # typeb = ['target','target column','target variable','dependent']
+        # typec = ['subset','subset columns']
+        # typed = ['data','data source','source']
+        # type_all = typea + typeb + typec + typed
+        # tokens = [tokenise(i) for i in type_all]
+        # unique_tokens = flatten(tokens)
              
-        f = pkgutil.get_data('mllibs',"corpus/wordlist.10000.txt")
-        content = io.TextIOWrapper(io.BytesIO(f), encoding='utf-8')
-        lines = content.readlines()
-        
+        # f = pkgutil.get_data('mllibs',"corpus/wordlist.10000.txt")
+        # content = io.TextIOWrapper(io.BytesIO(f), encoding='utf-8')
+        # lines = content.readlines()
            
-        cleaned = []
-        for line in lines:
-            removen = line.rstrip()
-            if removen not in unique_tokens:
-                cleaned.append(removen)
-                
-        corpus = typea + typeb + typec + typed + cleaned
-        labels = [0,0,0,0,1,1,1,1,2,2,3,3,3] + [4 for i in range(len(cleaned))]
-        data = pd.DataFrame({'corpus':corpus,'label':labels})
-        
-        vectoriser = CountVectorizer(ngram_range=(1,2))
-        X = vectoriser.fit_transform(data['corpus']).toarray()
-        y = data['label'].values
-        
-        # we have a dissbalanced class model, so lets use class_weight
-        model = DecisionTreeClassifier(class_weight={0:0.25,1:0.25,2:0.25,3:0.25,4:0.0001})
-        model.fit(X,y)
-        model.predict(X)
+        # cleaned = []
+        # for line in lines:
+        #     removen = line.rstrip()
+        #     if removen not in unique_tokens:
+        #         cleaned.append(removen)
+                
+        # corpus = typea + typeb + typec + typed + cleaned
+        # labels = [0,0,0,0,1,1,1,1,2,2,3,3,3] + [4 for i in range(len(cleaned))]
+        # data = pd.DataFrame({'corpus':corpus,'label':labels})
+        
+        # vectoriser = CountVectorizer(ngram_range=(1,2))
+        # X = vectoriser.fit_transform(data['corpus'])
+        # y = data['label'].values
+        
+        # # we have a dissbalanced class model, so lets use class_weight
+        # model = DecisionTreeClassifier(class_weight={0:0.25,1:0.25,2:0.25,3:0.25,4:0.0001})
+        # model.fit(X,y)
+
+        # with open('dtc_ner_tagger.pickle', 'wb') as f:
+        #     pickle.dump(model, f)
+
+        # with open('cv_ner_tagger.pickle', 'wb') as f:
+        #     pickle.dump(vectoriser, f)
+
+        vectoriser_load = pkgutil.get_data('mllibs','models/cv_ner_tagger.pickle')
+        vectoriser = pickle.loads(vectoriser_load)
+
+        model_load = pkgutil.get_data('mllibs','models/dtc_ner_tagger.pickle')
+        model = pickle.loads(model_load)
         
         self.vectoriser['token_ner'] = vectoriser
         self.model['token_ner'] = model      
         self.label['token_ner'] = ['features','target','subset','data','other']             
         
     '''
```

## Comparing `mllibs-0.1.2.dist-info/LICENSE` & `mllibs-0.1.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mllibs-0.1.2.dist-info/METADATA` & `mllibs-0.1.3.dist-info/METADATA`

 * *Files 24% similar despite different names*

```diff
@@ -1,40 +1,54 @@
 Metadata-Version: 2.1
 Name: mllibs
-Version: 0.1.2
-Summary: Simplifying machine learning
+Version: 0.1.3
+Summary: Simplifying Machine Learning
 Home-page: https://github.com/shtrausslearning/mllibs
 Author: Andrey Shtrauss
 Author-email: shtraussart@gmail.com
 Project-URL: Bug Tracker, https://github.com/shtrausslearning/mllibs/issues
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
+Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 License-File: LICENSE
+Requires-Dist: requests
+Requires-Dist: importlib-metadata
+Requires-Dist: plotly
+Requires-Dist: setuptools
+Requires-Dist: wheel
+Requires-Dist: numpy
+Requires-Dist: pandas
+Requires-Dist: scipy
+Requires-Dist: nltk
+Requires-Dist: seaborn
+Requires-Dist: matplotlib
+Requires-Dist: panel
+Requires-Dist: scikit-learn
+Requires-Dist: keras
+Requires-Dist: torch
+Requires-Dist: gensim
+Requires-Dist: spacy
+Requires-Dist: emoji
 
 ![](https://i.imgur.com/9ASYY1n.jpg)
 
 ![](https://camo.githubusercontent.com/d38e6cc39779250a2835bf8ed3a72d10dbe3b05fa6527baa3f6f1e8e8bd056bf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652d507974686f6e2d696e666f726d6174696f6e616c3f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d776869746526636f6c6f723d326262633861) ![](https://badgen.net/badge/status/WIP/blue) 
 
-#### **1. ABOUT PROJECT**
+#### **ABOUT PROJECT**
 
-***
-
-- **mllibs** is a Machine Learning (ML) library which utilises natural language processing (NLP)
+- <code>mllibs</code> is a Machine Learning (ML) library which utilises natural language processing (NLP)
 - Development of such helper modules are motivated by the fact that everyones understanding of coding & subject matter (ML in this case) may be different 
 - Often we see people create **functions** and **classes** to simplify the process of achieving something (which is good practice)
 - Likewise, **NLP interpreters** follow this trend as well, except, in this case our only inputs for activating certain code is **natural language**
 - Using python, we can interpret **natural language** in the form of **string** type data, using **natural langauge interpreters**
+- <code>mllibs</code> aims to provide an automated way to do machine learning using natural language
 
-<br>
-
-#### **2. CODE AUTOMATION**
-
-***
+#### **CODE AUTOMATION**
 
 I'm sure most people are familiar with code automation:
 
 (1) Let's take a look at how we would simplify our code using `functions`
 
 ```python
 def fib_list(n):
@@ -74,79 +88,50 @@
 (3) Let's take a look how we could simplify this using **language**
 
 ```python
 input = 'calculate the fibonacci sequence for the value of 5'
 nlp_interpreter(input) # [0, 1, 1, 2, 3]
 ```
 
-<br>
-
-#### **3. LETS LOOK AT AN EXAMPLE USING MLLIBS**
-
-***
+#### **SAMPLE FROM MLLIBS**
 
 Let's check out one example of how it works; let's visualise some data by requesting:
 
 ![](https://i.imgur.com/20f3i1Y.jpg)
 
 Our cell output will be:
 
 ![](https://i.imgur.com/4TTAAgp.png)
 
-<br>
-
-#### **4. WHY THIS LIBRARY EXISTS**
-
-***
-
-A good question to ask ourselves is why would this be needed?
-
-Here are some anwsers:
-> - Not everyone level of programming is the same, someone might struggle, whilst others know it quite well
-> - The same goes for the topic 'Machine Learning', there are quite a few concepts to remember
-
-
-#### **4. Package aims to provide:**
-- A userfiendly way introduction to Machine Learning for someone new to the field that have little knowledge of programming
-
-<br>
+#### **PROJECT STATUS**
 
+`mllibs` is usable, but still very raw, I'm constantly trying way to improve and clean the code structure
 
-#### **5. PROJECT STATUS**
+| **Kaggle** | **Github** | **PyPI**
+| - | - | - |
+| **<code>[mllibs](https://www.kaggle.com/datasets/shtrausslearning/mllibs)</code>** **0.1.2** | **<code>[mllibs](https://github.com/shtrausslearning/mllibs)</code>** **0.1.2** | [![PyPI version](https://badge.fury.io/py/mllibs.svg)](https://badge.fury.io/py/mllibs) | 
 
-***
 
-- `mllibs` is usable, but still very raw, I'm constantly trying way to improve and clean the code structure
-- If you would like to try it out, you can simply fork the notebook **<code>[nlp module for mllibs](https://www.kaggle.com/code/shtrausslearning/nlp-nlp-module-for-mllibs)</code>**
-
-<br>
-
-#### **6. LIBRARY COMPONENTS**
-
-***
+#### **LIBRARY COMPONENTS**
 
 `mllibs` consists of two parts:
 
 (1) modules associated with the **interpreter**
 
 - `nlpm` - groups together everything required for the interpreter module `nlpi`
 - `nlpi` - main interpreter component module (requires `nlpm` instance)
 - `snlpi` - single request interpreter module (uses `nlpi`)
 - `mnlpi` - multiple request interpreter module (uses `nlpi`)
 - `interface` - interactive module (chat type)
 
 (2) custom added modules, for mllibs these library are associated with **machine learning**
 
-You can check all the activations functions using <code>session.fl()</code> as shown in the sample notebooks
+You can check all the activations functions using <code>session.fl()</code> as shown in the sample notebooks in folder <code>examples</code>
 
-<br>
-
-#### **7. MODULE COMPONENT STRUCTURE**
-
-***
+#### **MODULE COMPONENT STRUCTURE**
 
 Currently new modules can be added using a custom class `sample` and a configuration dictionary `configure_sample`
 
 ```python
 
 # sample module class structure
 class sample(nlpi):
@@ -185,21 +170,19 @@
                             'description':'write description'}}
                          
 # configuration dictionary (passed in nlpm)
 configure_sample = {'corpus':corpus_sample,'info':info_sample}
 
 ```
 
-<br>
-
-#### **8. CREATING A COLLECTION**
+#### **CREATING A COLLECTION**
 
 There are two ways to start an interpreter session, manually importing and grouping modules or using  <code>interface</code> class
 
-***
+##### **FIRST APPROACH**
 
 First we need to combine all our module components together, this will link all passed modules together
 
 ```python
 
 collection = nlpm()
 collection.load([loader(configure_loader),
@@ -229,15 +212,28 @@
 
 ```python
 
 session.exec('create a scatterplot using data with x dimension1 y dimension2')
 
 ```
 
-***
+##### **SECOND APPROACH**
 
 The faster way, includes all loaded modules and groups them together for us:
 
 ```python
 from mllibs.interface import interface
 session = interface()
 ```
+
+#### **SAMPLE NOTEBOOKS**
+
+Here are some notebooks that will help you familiarise yourself with the library:
+
+- **[Sample EDA notebook](https://www.kaggle.com/code/shtrausslearning/mllibs-sample-eda-notebook)** (exploratory data analysis options)
+- **[Convert text into numeric data ](https://www.kaggle.com/code/shtrausslearning/mllibs-text-to-numeric-representation)** (change text data into numerical representation)
+- **[Normalise text](https://www.kaggle.com/code/shtrausslearning/mllibs-text-normalisation)** (Text cleaning preprocessing)
+- **[Linear Machine Learning Models](https://www.kaggle.com/code/shtrausslearning/mllibs-linear-models)** (Training and Prediction using linear models)
+- **[Dimension Reduction](https://www.kaggle.com/code/shtrausslearning/mllibs-dimensionality-reduction)** (Reducing the size of the feature matrix)
+
+These notebooks can also be found in folder <code>examples</code>
+
```

