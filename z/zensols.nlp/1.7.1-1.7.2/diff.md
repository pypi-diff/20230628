# Comparing `tmp/zensols.nlp-1.7.1-py3-none-any.whl.zip` & `tmp/zensols.nlp-1.7.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,26 +1,27 @@
-Zip file size: 53249 bytes, number of entries: 24
--rw-rw-r--  2.0 unx      131 b- defN 23-Jun-20 10:34 zensols/nlp/__init__.py
--rw-rw-r--  2.0 unx     9681 b- defN 23-Jun-20 10:34 zensols/nlp/combine.py
--rw-rw-r--  2.0 unx     6724 b- defN 23-Jun-20 10:34 zensols/nlp/component.py
--rw-rw-r--  2.0 unx    43376 b- defN 23-Jun-20 10:34 zensols/nlp/container.py
--rw-rw-r--  2.0 unx     1395 b- defN 23-Jun-20 10:34 zensols/nlp/dataframe.py
--rw-rw-r--  2.0 unx     4226 b- defN 23-Jun-20 10:34 zensols/nlp/decorate.py
--rw-rw-r--  2.0 unx     8363 b- defN 23-Jun-20 10:34 zensols/nlp/domain.py
--rw-rw-r--  2.0 unx     6579 b- defN 23-Jun-20 10:34 zensols/nlp/nerscore.py
--rw-rw-r--  2.0 unx    17214 b- defN 23-Jun-20 10:34 zensols/nlp/norm.py
--rw-rw-r--  2.0 unx    27793 b- defN 23-Jun-20 10:34 zensols/nlp/parser.py
--rw-rw-r--  2.0 unx    21289 b- defN 23-Jun-20 10:34 zensols/nlp/score.py
--rw-rw-r--  2.0 unx     5856 b- defN 23-Jun-20 10:34 zensols/nlp/serial.py
--rw-rw-r--  2.0 unx      581 b- defN 23-Jun-20 10:34 zensols/nlp/stemmer.py
--rw-rw-r--  2.0 unx    18311 b- defN 23-Jun-20 10:34 zensols/nlp/tok.py
--rw-r--r--  2.0 unx      398 b- defN 23-Jun-20 10:34 zensols/nlp/resources/component.conf
--rw-r--r--  2.0 unx      897 b- defN 23-Jun-20 10:34 zensols/nlp/resources/decorator.conf
--rw-r--r--  2.0 unx      823 b- defN 23-Jun-20 10:34 zensols/nlp/resources/mapper.conf
--rw-rw-r--  2.0 unx     1328 b- defN 23-Jun-20 10:34 zensols/nlp/resources/obj.conf
--rw-r--r--  2.0 unx     1441 b- defN 23-Jun-20 10:34 zensols/nlp/resources/score.yml
--rw-r--r--  2.0 unx      420 b- defN 23-Jun-20 10:34 zensols/nlp/resources/serial.conf
--rw-rw-r--  2.0 unx     6286 b- defN 23-Jun-20 10:34 zensols.nlp-1.7.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jun-20 10:34 zensols.nlp-1.7.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       12 b- defN 23-Jun-20 10:34 zensols.nlp-1.7.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1957 b- defN 23-Jun-20 10:34 zensols.nlp-1.7.1.dist-info/RECORD
-24 files, 185173 bytes uncompressed, 50107 bytes compressed:  72.9%
+Zip file size: 54643 bytes, number of entries: 25
+-rw-rw-r--  2.0 unx      154 b- defN 23-Jun-28 00:54 zensols/nlp/__init__.py
+-rw-rw-r--  2.0 unx     9681 b- defN 23-Jun-28 00:54 zensols/nlp/combine.py
+-rw-rw-r--  2.0 unx     6724 b- defN 23-Jun-28 00:54 zensols/nlp/component.py
+-rw-rw-r--  2.0 unx    43376 b- defN 23-Jun-28 00:54 zensols/nlp/container.py
+-rw-rw-r--  2.0 unx     1395 b- defN 23-Jun-28 00:54 zensols/nlp/dataframe.py
+-rw-rw-r--  2.0 unx     4226 b- defN 23-Jun-28 00:54 zensols/nlp/decorate.py
+-rw-rw-r--  2.0 unx     8363 b- defN 23-Jun-28 00:54 zensols/nlp/domain.py
+-rw-rw-r--  2.0 unx     6579 b- defN 23-Jun-28 00:54 zensols/nlp/nerscore.py
+-rw-rw-r--  2.0 unx    17214 b- defN 23-Jun-28 00:54 zensols/nlp/norm.py
+-rw-rw-r--  2.0 unx     9198 b- defN 23-Jun-28 00:54 zensols/nlp/parser.py
+-rw-rw-r--  2.0 unx    21289 b- defN 23-Jun-28 00:54 zensols/nlp/score.py
+-rw-rw-r--  2.0 unx     5856 b- defN 23-Jun-28 00:54 zensols/nlp/serial.py
+-rw-rw-r--  2.0 unx    19633 b- defN 23-Jun-28 00:54 zensols/nlp/sparser.py
+-rw-rw-r--  2.0 unx      581 b- defN 23-Jun-28 00:54 zensols/nlp/stemmer.py
+-rw-rw-r--  2.0 unx    18311 b- defN 23-Jun-28 00:54 zensols/nlp/tok.py
+-rw-r--r--  2.0 unx      398 b- defN 23-Jun-28 00:54 zensols/nlp/resources/component.conf
+-rw-r--r--  2.0 unx      897 b- defN 23-Jun-28 00:54 zensols/nlp/resources/decorator.conf
+-rw-r--r--  2.0 unx      823 b- defN 23-Jun-28 00:54 zensols/nlp/resources/mapper.conf
+-rw-rw-r--  2.0 unx     1328 b- defN 23-Jun-28 00:54 zensols/nlp/resources/obj.conf
+-rw-r--r--  2.0 unx     1441 b- defN 23-Jun-28 00:54 zensols/nlp/resources/score.yml
+-rw-r--r--  2.0 unx      420 b- defN 23-Jun-28 00:54 zensols/nlp/resources/serial.conf
+-rw-rw-r--  2.0 unx     6694 b- defN 23-Jun-28 00:54 zensols.nlp-1.7.2.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-28 00:54 zensols.nlp-1.7.2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       12 b- defN 23-Jun-28 00:54 zensols.nlp-1.7.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2036 b- defN 23-Jun-28 00:54 zensols.nlp-1.7.2.dist-info/RECORD
+25 files, 186721 bytes uncompressed, 51381 bytes compressed:  72.5%
```

## zipnote {}

```diff
@@ -30,14 +30,17 @@
 
 Filename: zensols/nlp/score.py
 Comment: 
 
 Filename: zensols/nlp/serial.py
 Comment: 
 
+Filename: zensols/nlp/sparser.py
+Comment: 
+
 Filename: zensols/nlp/stemmer.py
 Comment: 
 
 Filename: zensols/nlp/tok.py
 Comment: 
 
 Filename: zensols/nlp/resources/component.conf
@@ -54,20 +57,20 @@
 
 Filename: zensols/nlp/resources/score.yml
 Comment: 
 
 Filename: zensols/nlp/resources/serial.conf
 Comment: 
 
-Filename: zensols.nlp-1.7.1.dist-info/METADATA
+Filename: zensols.nlp-1.7.2.dist-info/METADATA
 Comment: 
 
-Filename: zensols.nlp-1.7.1.dist-info/WHEEL
+Filename: zensols.nlp-1.7.2.dist-info/WHEEL
 Comment: 
 
-Filename: zensols.nlp-1.7.1.dist-info/top_level.txt
+Filename: zensols.nlp-1.7.2.dist-info/top_level.txt
 Comment: 
 
-Filename: zensols.nlp-1.7.1.dist-info/RECORD
+Filename: zensols.nlp-1.7.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## zensols/nlp/__init__.py

```diff
@@ -1,6 +1,7 @@
 from .domain import *
 from .norm import *
 from .tok import *
 from .container import *
 from .parser import *
+from .sparser import *
 from .combine import *
```

## zensols/nlp/parser.py

```diff
@@ -1,95 +1,27 @@
 """Parse documents and generate features in an organized taxonomy.
 
 """
 from __future__ import annotations
 __author__ = 'Paul Landes'
-from typing import (
-    Tuple, Dict, Any, Sequence, Set, List, Iterable, Type, ClassVar
-)
+from typing import Tuple, Dict, Any, Sequence, Set, ClassVar
 from dataclasses import dataclass, field
 from abc import abstractmethod, ABCMeta, ABC
 import logging
-import itertools as it
-import sys
-import re
-from io import TextIOBase, StringIO
-import spacy
-from spacy.symbols import ORTH
-from spacy.tokens import Doc, Span, Token
+from io import StringIO
 from spacy.language import Language
 from zensols.util import Hasher
 from zensols.config import ImportIniConfig, ImportConfigFactory
-from zensols.persist import (
-    persisted, PersistedWork, PersistableContainer, Stash
-)
-from zensols.config import Dictable, ConfigFactory
-from . import (
-    NLPError, ParseError, TokenNormalizer, FeatureToken, SpacyFeatureToken,
-    FeatureSentence, FeatureDocument,
-)
+from zensols.persist import PersistableContainer, Stash
+from zensols.config import Dictable
+from . import NLPError, FeatureToken, FeatureSentence, FeatureDocument
 
 logger = logging.getLogger(__name__)
 
 
-@dataclass
-class _DictableDoc(Dictable):
-    """Utility class to pretty print and serialize Spacy documents.
-
-    """
-    doc: Doc = field(repr=False)
-    """The document from which to create a :class:`.dict`."""
-
-    def _write_token(self, tok: Token, depth: int, writer: TextIOBase):
-        s = (f'{tok}: tag={tok.tag_}, pos={tok.pos_}, stop={tok.is_stop}, ' +
-             f'lemma={tok.lemma_}, dep={tok.dep_}')
-        self._write_line(s, depth, writer)
-
-    def write(self, depth: int = 0, writer: TextIOBase = sys.stdout,
-              token_limit: int = sys.maxsize):
-        """Pretty print the document.
-
-        :param token_limit: the max number of tokens to write, which defaults
-                            to all of them
-
-        """
-        text = self._trunc(str(self.doc.text))
-        self._write_line(f'text: {text}', depth, writer)
-        self._write_line('tokens:', depth, writer)
-        for sent in self.doc.sents:
-            self._write_line(self._trunc(str(sent)), depth + 1, writer)
-            for t in it.islice(sent, token_limit):
-                self._write_token(t, depth + 2, writer)
-        self._write_line('entities:', depth, writer)
-        for ent in self.doc.ents:
-            self._write_line(f'{ent}: {ent.label_}', depth + 1, writer)
-
-    def _from_dictable(self, *args, **kwargs) -> Dict[str, Any]:
-        sents = tuple(self.doc.sents)
-        em = {}
-        for e in self.doc.ents:
-            for tok in self.doc[e.start:e.end]:
-                em[tok.i] = e.label_
-
-        def tok_json(t):
-            return {'tag': t.tag_, 'pos': t.pos_,
-                    'is_stop': t.is_stop, 'lemma': t.lemma_, 'dep': t.dep_,
-                    'text': t.text, 'idx': t.idx,
-                    'ent': None if t.i not in em else em[t.i],
-                    'childs': tuple(map(lambda c: c.i, t.children))}
-
-        def sent_json(idx):
-            s = sents[idx]
-            return {t.i: tok_json(t) for t in self.doc[s.start:s.end]}
-
-        return {'text': self.doc.text,
-                'sents': {i: sent_json(i) for i in range(len(sents))},
-                'ents': [(str(e), e.label_,) for e in self.doc.ents]}
-
-
 class ComponentInitializer(ABC):
     """Called by :class:`.Component` to do post spaCy initialization.
 
     """
     @abstractmethod
     def init_nlp_model(self, model: Language, component: Component):
         """Do any post spaCy initialization on the the referred framework."""
@@ -242,396 +174,18 @@
     """
     @abstractmethod
     def decorate(self, doc: FeatureDocument):
         pass
 
 
 @dataclass
-class SpacyFeatureDocumentParser(FeatureDocumentParser):
-    """This langauge resource parses text in to Spacy documents.  Loaded spaCy
-    models have attribute ``doc_parser`` set enable creation of factory
-    instances from registered pipe components (i.e. specified by
-    :class:`.Component`).
-
-    Configuration example::
-
-        [doc_parser]
-        class_name = zensols.nlp.SpacyFeatureDocumentParser
-        lang = en
-        model_name = ${lang}_core_web_sm
-
-    """
-    _MODELS = {}
-    """Contains cached models, such as ``en_core_web_sm``."""
-
-    config_factory: ConfigFactory = field()
-    """A configuration parser optionally used by pipeline :class:`.Component`
-    instances.
-
-    """
-    name: str = field()
-    """The name of the parser, which is taken from the section name when created
-    with a :class:`~zensols.config.ConfigFactory`.
-
-    """
-    lang: str = field(default='en')
-    """The natural language the identify the model."""
-
-    model_name: str = field(default=None)
-    """The Spacy model name (defualts to ``en_core_web_sm``); this is ignored
-    if ``model`` is not ``None``.
-
-    """
-    token_feature_ids: Set[str] = field(
-        default_factory=lambda: FeatureDocumentParser.TOKEN_FEATURE_IDS)
-    """The features to keep from spaCy tokens.
-
-    :see: :obj:`TOKEN_FEATURE_IDS`
-
-    """
-    components: Sequence[Component] = field(default=())
-    """Additional Spacy components to add to the pipeline."""
-
-    token_decorators: Sequence[FeatureTokenDecorator] = field(default=())
-    """A list of decorators that can add, remove or modify features on a token.
-
-    """
-    sentence_decorators: Sequence[FeatureSentenceDecorator] = field(
-        default=())
-    """A list of decorators that can add, remove or modify features on a
-    sentence.
-
-    """
-    document_decorators: Sequence[FeatureDocumentDecorator] = field(
-        default=())
-    """A list of decorators that can add, remove or modify features on a
-    document.
-
-    """
-    disable_component_names: Sequence[str] = field(default=None)
-    """Components to disable in the spaCy model when creating documents in
-    :meth:`parse`.
-
-    """
-    token_normalizer: TokenNormalizer = field(default=None)
-    """The token normalizer for methods that use it, i.e. ``features``."""
-
-    special_case_tokens: List = field(default_factory=list)
-    """Tokens that will be parsed as one token, i.e. ``</s>``."""
-
-    doc_class: Type[FeatureDocument] = field(default=FeatureDocument)
-    """The type of document instances to create."""
-
-    sent_class: Type[FeatureSentence] = field(default=FeatureSentence)
-    """The type of sentence instances to create."""
-
-    token_class: Type[FeatureToken] = field(default=SpacyFeatureToken)
-    """The type of document instances to create."""
-
-    remove_empty_sentences: bool = field(default=None)
-    """Deprecated and will be removed from future versions.  Use
-    :class:`.FilterSentenceFeatureDocumentDecorator` instead.
-
-    """
-    reload_components: bool = field(default=False)
-    """Removes, then re-adds components for cached models.  This is helpful for
-    when there are component configurations that change on reruns with a
-    difference application context but in the same Python interpreter session.
-
-    A spaCy component can get other instances via :obj:`config_factory`, but if
-    this is ``False`` it will be paired with the first instance of this class
-    and not the new ones created with a new configuration factory.
-
-    """
-    def __post_init__(self):
-        super().__post_init__()
-        self._model = PersistedWork('_model', self)
-        if self.remove_empty_sentences is not None:
-            import warnings
-            warnings.warn(
-                'remove_empty_sentences is deprecated (use ' +
-                'FilterSentenceFeatureDocumentDecorator instead',
-                DeprecationWarning)
-
-    def _create_model_key(self) -> str:
-        """Create a unique key used for storing expensive-to-create spaCy language
-        models in :obj:`_MODELS`.
-
-        """
-        comps = sorted(map(lambda c: f'{c.pipe_name}:{hash(c)}',
-                           self.components))
-        comp_str = '-' + '|'.join(comps)
-        return f'{self.model_name}{comp_str}'
-
-    def _create_model(self) -> Language:
-        """Load, configure and return a new spaCy model instance."""
-        if logger.isEnabledFor(logging.DEBUG):
-            logger.debug(f'loading model: {self.model_name}')
-        nlp = spacy.load(self.model_name)
-        return nlp
-
-    def _add_components(self, nlp: Language):
-        """Add components to the pipeline that was just created."""
-        if self.components is not None:
-            comp: Component
-            for comp in self.components:
-                if comp.pipe_name in nlp.pipe_names:
-                    if logger.isEnabledFor(logging.DEBUG):
-                        logger.debug(f'{comp} already registered--skipping')
-                else:
-                    if logger.isEnabledFor(logging.DEBUG):
-                        logger.debug(f'adding {comp} ({id(comp)}) to pipeline')
-                    comp.init(nlp)
-
-    def _remove_components(self, nlp: Language):
-        for comp in self.components:
-            name, comp = nlp.remove_pipe(comp.pipe_name)
-            if logger.isEnabledFor(logging.DEBUG):
-                logger.debug(f'removed {name} ({id(comp)})')
-
-    @property
-    @persisted('_model')
-    def model(self) -> Language:
-        """The spaCy model.  On first access, this creates a new instance using
-        ``model_name``.
-
-        """
-        mkey: str = self._create_model_key()
-        if logger.isEnabledFor(logging.DEBUG):
-            logger.debug(f'model key: {mkey}')
-        if self.model_name is None:
-            self.model_name = f'{self.lang}_core_web_sm'
-        # cache model in class space
-        nlp: Language = self._MODELS.get(mkey)
-        if nlp is None:
-            nlp: Language = self._create_model()
-            # pipe components can create other application context instance via
-            # the :obj:`config_factory` with access to this instance
-            nlp.doc_parser = self
-            self._add_components(nlp)
-            if logger.isEnabledFor(logging.DEBUG):
-                logger.debug(
-                    f'adding {mkey} to cached models ({len(self._MODELS)})')
-            self._MODELS[mkey] = nlp
-            if logger.isEnabledFor(logging.DEBUG):
-                logger.debug(f'cached models: {len(self._MODELS)}')
-        else:
-            if logger.isEnabledFor(logging.DEBUG):
-                logger.debug(f'cached model: {mkey} ({self.model_name})')
-            if self.reload_components:
-                if logger.isEnabledFor(logging.DEBUG):
-                    logger.debug(f're-adding components to {id(self)}')
-                nlp.doc_parser = self
-                self._remove_components(nlp)
-                self._add_components(nlp)
-        if self.token_normalizer is None:
-            if logger.isEnabledFor(logging.DEBUG):
-                logger.debug('adding default tokenizer')
-            self.token_normalizer = TokenNormalizer()
-        for stok in self.special_case_tokens:
-            rule = [{ORTH: stok}]
-            if logger.isEnabledFor(logging.DEBUG):
-                logger.debug(f'adding special token: {stok} with rule: {rule}')
-            nlp.tokenizer.add_special_case(stok, rule)
-        return nlp
-
-    @classmethod
-    def clear_models(self):
-        """Clears all cached models."""
-        self._MODELS.clear()
-
-    def parse_spacy_doc(self, text: str) -> Doc:
-        """Parse ``text`` in to a Spacy document.
-
-        """
-        if logger.isEnabledFor(logging.DEBUG):
-            logger.debug(f'creating document with model: {self.model_name}, ' +
-                         f'disable components: {self.disable_component_names}')
-        if self.disable_component_names is None:
-            doc = self.model(text)
-        else:
-            doc = self.model(text, disable=self.disable_component_names)
-        if logger.isEnabledFor(logging.INFO):
-            logger.info(f'parsed text: <{self._trunc(text)}>')
-        if logger.isEnabledFor(logging.DEBUG):
-            doc_text = self._trunc(str(doc))
-            logger.debug(f'parsed document: <{doc_text}>')
-        return doc
-
-    def get_dictable(self, doc: Doc) -> Dictable:
-        """Return a dictionary object graph and pretty prints spaCy docs.
-
-        """
-        return _DictableDoc(doc)
-
-    def _normalize_tokens(self, doc: Doc, *args, **kwargs) -> \
-            Iterable[FeatureToken]:
-        """Generate an iterator of :class:`.FeatureToken` instances with features on a
-        per token level.
-
-        """
-        if logger.isEnabledFor(logging.DEBUG):
-            doc_text = self._trunc(str(doc))
-            logger.debug(f'normalizing features in {doc_text}')
-            logger.debug(f'args: <{args}>')
-            logger.debug(f'kwargs: <{kwargs}>')
-        tokens: Tuple[FeatureToken, ...] = \
-            map(lambda tup: self._create_token(*tup, *args, **kwargs),
-                self.token_normalizer.normalize(doc))
-        return tokens
-
-    def _decorate_token(self, spacy_tok: Token, feature_token: FeatureToken):
-        decorator: FeatureTokenDecorator
-        for decorator in self.token_decorators:
-            decorator.decorate(feature_token)
-
-    def _create_token(self, tok: Token, norm: Tuple[Token, str],
-                      *args, **kwargs) -> FeatureToken:
-        tp: Type[FeatureToken] = self.token_class
-        ft: FeatureToken = tp(tok, norm, *args, **kwargs)
-        self._decorate_token(tok, ft)
-        if logger.isEnabledFor(logging.DEBUG):
-            logger.debug(f'detaching using features: {self.token_feature_ids}')
-        return ft.detach(self.token_feature_ids)
-
-    def _decorate_sent(self, spacy_sent: Span, feature_sent: FeatureSentence):
-        decorator: FeatureSentenceDecorator
-        for decorator in self.sentence_decorators:
-            decorator.decorate(feature_sent)
-
-    def _create_sent(self, spacy_sent: Span, stoks: Iterable[FeatureToken],
-                     text: str) -> FeatureSentence:
-        sent: FeatureSentence = self.sent_class(tuple(stoks), text, spacy_sent)
-        self._decorate_sent(spacy_sent, sent)
-        return sent
-
-    def _create_sents(self, doc: Doc) -> List[FeatureSentence]:
-        """Create sentences from a spaCy doc."""
-        toks: Tuple[FeatureToken, ...] = tuple(self._normalize_tokens(doc))
-        sents: List[FeatureSentence] = []
-        ntoks: int = len(toks)
-        tix: int = 0
-        sent: Span
-        for sent in doc.sents:
-            e: int = sent[-1].i
-            stoks: List[FeatureToken] = []
-            while tix < ntoks:
-                tok = toks[tix]
-                if tok.i <= e:
-                    stoks.append(tok)
-                else:
-                    break
-                tix += 1
-            fsent: FeatureSentence = self._create_sent(sent, stoks, sent.text)
-            sents.append(fsent)
-        return sents
-
-    def from_spacy_doc(self, doc: Doc, *args, text: str = None,
-                       **kwargs) -> FeatureDocument:
-        """Create s :class:`.FeatureDocument` from a spaCy doc.
-
-        :param doc: the spaCy generated document to transform in to a feature
-                    document
-
-        :param text: either a string or a list of strings; if the former a
-                     document with one sentence will be created, otherwise a
-                     document is returned with a sentence for each string in
-                     the list
-
-        :param args: the arguments used to create the FeatureDocument instance
-
-        :param kwargs: the key word arguments used to create the
-                       FeatureDocument instance
-
-        """
-        text = doc.text if text is None else text
-        sents: List[FeatureSentence] = self._create_sents(doc)
-        try:
-            return self.doc_class(tuple(sents), text, doc, *args, **kwargs)
-        except Exception as e:
-            raise ParseError(
-                f'Could not parse <{text}> for {self.doc_class} ' +
-                f"with args {args} for parser '{self.name}'") from e
-
-    def _decorate_doc(self, spacy_doc: Span, feature_doc: FeatureDocument):
-        decorator: FeatureDocumentDecorator
-        for decorator in self.document_decorators:
-            decorator.decorate(feature_doc)
-
-    def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
-        if not isinstance(text, str):
-            raise ParseError(
-                f'Expecting string text but got: {text} ({type(str)})')
-        sdoc: Doc = self.parse_spacy_doc(text)
-        fdoc: FeatureDocument = self.from_spacy_doc(
-            sdoc, *args, text=text, **kwargs)
-        self._decorate_doc(sdoc, fdoc)
-        return fdoc
-
-    def to_spacy_doc(self, doc: FeatureDocument, norm: bool = True,
-                     add_features: Set[str] = None) -> Doc:
-        """Convert a feature document back in to a spaCy document.
-
-        **Note**: not all data is copied--only text, ``pos_``, ``tag_``,
-        ``lemma_`` and ``dep_``.
-
-        :param doc: the spaCy doc to convert
-
-        :param norm: whether to use the normalized text as the ``orth_`` spaCy
-                     token attribute or ``text``
-
-        :pram add_features: whether to add POS, NER tags, lemmas, heads and
-                            dependnencies
-
-        :return: the feature document with copied data from ``doc``
-
-        """
-        def conv_iob(t: FeatureToken) -> str:
-            if t.ent_iob_ == 'O':
-                return 'O'
-            return f'{t.ent_iob_}-{t.ent_}'
-
-        if norm:
-            words = list(doc.norm_token_iter())
-        else:
-            words = [t.text for t in doc.token_iter()]
-        if add_features is None:
-            add_features = set('pos tag lemma head dep ent'.split())
-        sent_starts = [False] * len(words)
-        sidx = 0
-        for sent in doc:
-            sent_starts[sidx] = True
-            sidx += len(sent)
-        params = dict(vocab=self.model.vocab,
-                      words=words,
-                      spaces=[True] * len(words),
-                      sent_starts=sent_starts)
-        if add_features and doc.token_len > 0:
-            assert len(words) == doc.token_len
-            tok = next(iter(doc.token_iter()))
-            if hasattr(tok, 'pos_') and 'pos' in add_features:
-                params['pos'] = [t.pos_ for t in doc.token_iter()]
-            if hasattr(tok, 'tag_') and 'tag' in add_features:
-                params['tags'] = [t.tag_ for t in doc.token_iter()]
-            if hasattr(tok, 'lemma_') and 'lemma' in add_features:
-                params['lemmas'] = [t.lemma_ for t in doc.token_iter()]
-            if hasattr(tok, 'head_') and 'head' in add_features:
-                params['heads'] = [t.head_ for t in doc.token_iter()]
-            if hasattr(tok, 'dep_') and 'dep' in add_features:
-                params['deps'] = [t.dep_ for t in doc.token_iter()]
-            if hasattr(tok, 'ent_') and 'ent' in add_features:
-                params['ents'] = [conv_iob(t) for t in doc.token_iter()]
-        return Doc(**params)
-
-
-@dataclass
 class DecoratedFeatureDocumentParser(FeatureDocumentParser):
-    """This class adapts the spaCy parser adaptors to the general case using a
-    GoF decorator pattern.  This is useful for any post processing needed on
-    existing configured document parsers.
+    """This class adapts the :class:`.FeatureDocumentParser` adaptors to the
+    general case using a GoF decorator pattern.  This is useful for any post
+    processing needed on existing configured document parsers.
 
     """
     delegate: FeatureDocumentParser = field()
     """Used to create the feature documents."""
 
     token_decorators: Sequence[FeatureTokenDecorator] = field(default=())
     """A list of decorators that can add, remove or modify features on a token.
@@ -719,29 +273,7 @@
     def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
         return self._load_or_parse(text, True, *args, **kwargs)[0]
 
     def clear(self):
         """Clear the caching stash."""
         if self.stash is not None:
             self.stash.clear()
-
-
-@dataclass
-class WhiteSpaceTokenizerFeatureDocumentParser(SpacyFeatureDocumentParser):
-    """This class parses text in to instances of :class:`.FeatureDocument`
-    instances using :meth:`parse`.  This parser does no sentence chunking so
-    documents have one and only one sentence for each parse.
-
-    """
-    _TOK_REGEX: ClassVar[re.Pattern] = re.compile(r'\S+')
-    """The whitespace regular expression for splitting tokens."""
-
-    def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
-        toks: List[FeatureToken] = []
-        m: re.Match
-        for i, m in zip(it.count(), re.finditer(self._TOK_REGEX, text)):
-            tok = FeatureToken(i, m.start(), 0, m.group(0))
-            tok.default_detached_feature_ids = \
-                FeatureToken.REQUIRED_FEATURE_IDS
-            toks.append(tok)
-        sent = self.sent_class(tokens=tuple(toks), text=text)
-        return self.doc_class(sents=(sent,), text=text, *args, **kwargs)
```

## Comparing `zensols.nlp-1.7.1.dist-info/METADATA` & `zensols.nlp-1.7.2.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 Metadata-Version: 2.1
 Name: zensols.nlp
-Version: 1.7.1
+Version: 1.7.2
 Summary: A utility library to assist in parsing natural language text.
 Home-page: https://github.com/plandes/nlparse
-Download-URL: https://github.com/plandes/nlparse/releases/download/v1.7.1/zensols.nlp-1.7.1-py3-none-any.whl
+Download-URL: https://github.com/plandes/nlparse/releases/download/v1.7.2/zensols.nlp-1.7.2-py3-none-any.whl
 Author: Paul Landes
 Author-email: landes@mailc.net
 Keywords: tooling
 Description-Content-Type: text/markdown
 Requires-Dist: spacy (~=3.2.0)
 Requires-Dist: msgpack (>=1.0.0)
 Requires-Dist: msgpack-numpy (>=0.4.7.1)
@@ -48,20 +48,48 @@
 ## Documentation
 
 * [Framework documentation]
 * [Natural Language Parsing]
 * [List Token Normalizers and Mappers]
 
 
+## Obtaining / Installing
+
+The easiest way to install the command line program is via the `pip`
+installer.  Since the package needs at least one spaCy module, the second
+command downloads the smallest model.
+```bash
+pip3 install --use-deprecated=legacy-resolver zensols.nlp
+python -m spacy download en_core_web_sm
+```
+
+Binaries are also available on [pypi].
+
+
 ## Usage
 
 A parser using the default configuration can be obtained by:
 ```python
 from zensols.nlp import FeatureDocumentParser
 parser: FeatureDocumentParser = FeatureDocumentParser.default_instance()
+doc = parser('Obama was the 44th president of the United States.')
+for tok in doc.tokens:
+    print(tok.norm, tok.pos_, tok.tag_)
+print(doc.entities)
+
+>>>
+Obama PROPN NNP
+was AUX VBD
+the DET DT
+45th ADJ JJ
+president NOUN NN
+of ADP IN
+the United States DET DT
+. PUNCT .
+(<Obama>, <45th>, <the United States>)
 ```
 
 However, minimal effort is needed to configure the parser using a [resource library]:
 ```python
 from io import StringIO
 from zensols.config import ImportIniConfig, ImportConfigFactory
 from zensols.nlp import FeatureDocument, FeatureDocumentParser
@@ -88,23 +116,14 @@
 This uses a [resource library] to source in the configuration from this package
 so minimal configuration is necessary.  More advanced configuration [examples]
 are also available.
 
 See the [feature documents] for more information.
 
 
-## Obtaining / Installing
-
-1. The easist way to install the command line program is via the `pip`
-   installer: `pip3 install zensols.nlp`
-2. Install at least one spaCy model: `python -m spacy download en_core_web_sm`
-
-Binaries are also available on [pypi].
-
-
 ## Attribution
 
 This project, or example code, uses:
 * [spaCy] for natural language parsing
 * [msgpack] and [smart-open] for Python disk serialization
 * [nltk] for the [porter stemmer] functionality
```

## Comparing `zensols.nlp-1.7.1.dist-info/RECORD` & `zensols.nlp-1.7.2.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,25 @@
-zensols/nlp/__init__.py,sha256=xtpP6eSU8a_-c-NEYKnLlEN_BTf2EVb2fe139RSfDzA,131
+zensols/nlp/__init__.py,sha256=p3q2UPt0Xa7RSaML_rT9ueu4DGUYoxnBJZeaGqF5lks,154
 zensols/nlp/combine.py,sha256=YT-Ga73qGidFyRwf5vWmnoLo-_QriAASdtxcuWq8jc8,9681
 zensols/nlp/component.py,sha256=rRj_NCIbMmbmZvaQSPo04jPDBHOeJYqNtbjS5-2OwGw,6724
 zensols/nlp/container.py,sha256=XPWydC89SX5MSqI-JuzDcXM9DYwrCCj5FZTCF9_GJDU,43376
 zensols/nlp/dataframe.py,sha256=Lue37IquvfVX6_hG_5L2pr-P8KrR0YaUNWQDhjyGg20,1395
 zensols/nlp/decorate.py,sha256=qUoqbVuaphcIDbMvc07jJFJtZx0wREQ2VTh3h4E08_M,4226
 zensols/nlp/domain.py,sha256=pwzkNbiJT8_beXXj2FYvTLAdbiwRZgxMuodfBM6hfQQ,8363
 zensols/nlp/nerscore.py,sha256=dzm9leret_KCyMAfU2Dj_gbfkpB6b6SlYdajCwvJ31w,6579
 zensols/nlp/norm.py,sha256=GTgZQzmBNNn_ZaxgulGAdhLCcqwzaPDnKgFJnd8jCZk,17214
-zensols/nlp/parser.py,sha256=L0SJmKDFQzR5LvCNhe5yH_qbtxcscTAhEQunX1pb3u0,27793
+zensols/nlp/parser.py,sha256=EDtX1lbLql3YtzIUjm5wLf2_dMzhL-e930lx5IvFNy0,9198
 zensols/nlp/score.py,sha256=lx1owBX6Ycq3h4FeENkxfVWLVsShj_RvpujbVsrVDLs,21289
 zensols/nlp/serial.py,sha256=hNmEJXybFw2ICnexBWJ5TWit7tRjBJMLUpWFVSmvvkc,5856
+zensols/nlp/sparser.py,sha256=p6EvNXeGf92QNEd4neTL1-Noxr-mpj5HAD94xSqzB2w,19633
 zensols/nlp/stemmer.py,sha256=ZHIbU5UfzpYI8cMj096EqY_DRSof-EfjWWMN7_bCMMk,581
 zensols/nlp/tok.py,sha256=h_8Q15OKdExK04U7ghB-SLj0Hlvc8XCyQYX-SvJy-l8,18311
 zensols/nlp/resources/component.conf,sha256=SU5Yx_pW8olTvOMmVBrWbUCQ1on5D8LvpBJFo48NPsA,398
 zensols/nlp/resources/decorator.conf,sha256=Ug_w7RB6ZrHkBVG8ji47BmJHoOL7naqjYMncN9U1lpA,897
 zensols/nlp/resources/mapper.conf,sha256=OQtpye4uA8t_woIdUUsWfpM5cZ_2c-ZlYFmaCt9vMtM,823
 zensols/nlp/resources/obj.conf,sha256=vF93wu6kN5uIn7gw5uPZEPX0S6ncW1zbY3HkKygsc-M,1328
 zensols/nlp/resources/score.yml,sha256=wmr2OUrnYZL_T3K9MgyMJho3rHiYXeS3pQuF6t_kOo4,1441
 zensols/nlp/resources/serial.conf,sha256=PfvMIiWvk0dr08YQmCodlcxGdMgaX6AMFzrXIr0mgCw,420
-zensols.nlp-1.7.1.dist-info/METADATA,sha256=YkaNBLWIRt9_NBLCTq9BZE6vdZFfdzx0j7hSUQBVHWw,6286
-zensols.nlp-1.7.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-zensols.nlp-1.7.1.dist-info/top_level.txt,sha256=NgD-P1jf-0xvfkWIxho1IVPkpl2JLBaIjIssgP_pkw8,12
-zensols.nlp-1.7.1.dist-info/RECORD,,
+zensols.nlp-1.7.2.dist-info/METADATA,sha256=_OcKmzPUK7JtU5P4ssfUNBJbZT2M4tuUjbpe1Q12lWk,6694
+zensols.nlp-1.7.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+zensols.nlp-1.7.2.dist-info/top_level.txt,sha256=NgD-P1jf-0xvfkWIxho1IVPkpl2JLBaIjIssgP_pkw8,12
+zensols.nlp-1.7.2.dist-info/RECORD,,
```

