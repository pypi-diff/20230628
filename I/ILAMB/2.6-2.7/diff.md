# Comparing `tmp/ILAMB-2.6.tar.gz` & `tmp/ILAMB-2.7.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "/home/nate/work/ILAMB/dist/tmpolycbo81/ILAMB-2.6.tar", last modified: Tue May 18 19:00:50 2021, max compression
+gzip compressed data, was "ILAMB-2.7.tar", last modified: Wed Jun 28 11:50:43 2023, max compression
```

## Comparing `ILAMB-2.6.tar` & `ILAMB-2.7.tar`

### file list

```diff
@@ -1,56 +1,62 @@
-drwxrwxr-x   0 nate      (1000) nate      (1000)        0 2021-05-18 19:00:50.476309 ILAMB-2.6/
--rw-rw-r--   0 nate      (1000) nate      (1000)     6365 2021-05-18 19:00:50.476309 ILAMB-2.6/PKG-INFO
--rw-rw-r--   0 nate      (1000) nate      (1000)     4760 2021-05-18 15:34:20.000000 ILAMB-2.6/README.rst
-drwxrwxr-x   0 nate      (1000) nate      (1000)        0 2021-05-18 19:00:50.462309 ILAMB-2.6/bin/
--rw-rw-r--   0 nate      (1000) nate      (1000)    10464 2020-04-03 15:19:29.000000 ILAMB-2.6/bin/ilamb-doctor
--rw-rw-r--   0 nate      (1000) nate      (1000)     4809 2021-02-03 18:10:15.000000 ILAMB-2.6/bin/ilamb-fetch
--rw-rw-r--   0 nate      (1000) nate      (1000)     8520 2020-04-03 15:19:29.000000 ILAMB-2.6/bin/ilamb-mean
--rw-rw-r--   0 nate      (1000) nate      (1000)    27247 2021-05-12 16:53:37.000000 ILAMB-2.6/bin/ilamb-run
--rw-rw-r--   0 nate      (1000) nate      (1000)      543 2021-02-03 18:10:15.000000 ILAMB-2.6/bin/ilamb-setup
--rw-rw-r--   0 nate      (1000) nate      (1000)     2516 2020-04-03 15:19:29.000000 ILAMB-2.6/bin/ilamb-table
--rw-rw-r--   0 nate      (1000) nate      (1000)       38 2021-05-18 19:00:50.476309 ILAMB-2.6/setup.cfg
--rw-rw-r--   0 nate      (1000) nate      (1000)     3499 2021-05-18 18:16:46.000000 ILAMB-2.6/setup.py
-drwxrwxr-x   0 nate      (1000) nate      (1000)        0 2021-05-18 19:00:50.460309 ILAMB-2.6/src/
-drwxrwxr-x   0 nate      (1000) nate      (1000)        0 2021-05-18 19:00:50.471309 ILAMB-2.6/src/ILAMB/
--rw-rw-r--   0 nate      (1000) nate      (1000)     8931 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfAlbedo.py
--rw-rw-r--   0 nate      (1000) nate      (1000)     1649 2021-02-03 18:10:15.000000 ILAMB-2.6/src/ILAMB/ConfBurntArea.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    52554 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfCO2.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    17662 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfDiurnal.py
--rw-rw-r--   0 nate      (1000) nate      (1000)     9638 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfEvapFraction.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    45785 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfIOMB.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    12530 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfNBP.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    14958 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfPermafrost.py
--rw-rw-r--   0 nate      (1000) nate      (1000)     9155 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfRunoff.py
--rw-rw-r--   0 nate      (1000) nate      (1000)      419 2020-04-03 15:19:29.000000 ILAMB-2.6/src/ILAMB/ConfSWE.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    10367 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfSoilCarbon.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    11334 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfTWSA.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    16825 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ConfUncertainty.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    57013 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/Confrontation.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    19411 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/ModelResult.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    50860 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/Post.py
--rw-rw-r--   0 nate      (1000) nate      (1000)     9297 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/Regions.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    19640 2020-04-03 15:19:29.000000 ILAMB-2.6/src/ILAMB/Relationship.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    37505 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/Scoreboard.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    80122 2021-05-12 16:53:37.000000 ILAMB-2.6/src/ILAMB/Variable.py
--rw-rw-r--   0 nate      (1000) nate      (1000)      757 2021-05-18 18:17:23.000000 ILAMB-2.6/src/ILAMB/__init__.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    37189 2021-02-03 18:10:15.000000 ILAMB-2.6/src/ILAMB/ccgfilt.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    11259 2021-02-03 18:10:15.000000 ILAMB-2.6/src/ILAMB/constants.py
-drwxrwxr-x   0 nate      (1000) nate      (1000)        0 2021-05-18 19:00:50.475309 ILAMB-2.6/src/ILAMB/data/
--rw-rw-r--   0 nate      (1000) nate      (1000)    14180 2021-05-18 18:24:25.000000 ILAMB-2.6/src/ILAMB/data/cmip.cfg
--rw-rw-r--   0 nate      (1000) nate      (1000)     1772 2020-04-03 15:19:29.000000 ILAMB-2.6/src/ILAMB/data/diurnal.cfg
--rw-rw-r--   0 nate      (1000) nate      (1000)    12377 2020-04-03 15:19:29.000000 ILAMB-2.6/src/ILAMB/data/ilamb.cfg
--rw-rw-r--   0 nate      (1000) nate      (1000)     5477 2020-04-03 15:19:29.000000 ILAMB-2.6/src/ILAMB/data/iomb.cfg
--rw-rw-r--   0 nate      (1000) nate      (1000)      351 2020-04-03 15:19:29.000000 ILAMB-2.6/src/ILAMB/data/sample.cfg
--rw-rw-r--   0 nate      (1000) nate      (1000)      279 2021-05-18 19:00:50.000000 ILAMB-2.6/src/ILAMB/generated_version.py
--rw-rw-r--   0 nate      (1000) nate      (1000)    90738 2021-05-11 16:48:25.000000 ILAMB-2.6/src/ILAMB/ilamblib.py
--rw-rw-r--   0 nate      (1000) nate      (1000)     5909 2020-04-03 15:19:29.000000 ILAMB-2.6/src/ILAMB/run.py
-drwxrwxr-x   0 nate      (1000) nate      (1000)        0 2021-05-18 19:00:50.473309 ILAMB-2.6/src/ILAMB.egg-info/
--rw-rw-r--   0 nate      (1000) nate      (1000)     6365 2021-05-18 19:00:50.000000 ILAMB-2.6/src/ILAMB.egg-info/PKG-INFO
--rw-rw-r--   0 nate      (1000) nate      (1000)     1102 2021-05-18 19:00:50.000000 ILAMB-2.6/src/ILAMB.egg-info/SOURCES.txt
--rw-rw-r--   0 nate      (1000) nate      (1000)        1 2021-05-18 19:00:50.000000 ILAMB-2.6/src/ILAMB.egg-info/dependency_links.txt
--rw-rw-r--   0 nate      (1000) nate      (1000)        1 2021-05-18 18:21:03.000000 ILAMB-2.6/src/ILAMB.egg-info/not-zip-safe
--rw-rw-r--   0 nate      (1000) nate      (1000)      131 2021-05-18 19:00:50.000000 ILAMB-2.6/src/ILAMB.egg-info/requires.txt
--rw-rw-r--   0 nate      (1000) nate      (1000)        6 2021-05-18 19:00:50.000000 ILAMB-2.6/src/ILAMB.egg-info/top_level.txt
-drwxrwxr-x   0 nate      (1000) nate      (1000)        0 2021-05-18 19:00:50.475309 ILAMB-2.6/test/
--rw-rw-r--   0 nate      (1000) nate      (1000)     5728 2020-04-03 15:19:29.000000 ILAMB-2.6/test/test_Variable.py
--rw-rw-r--   0 nate      (1000) nate      (1000)      352 2020-04-03 15:19:29.000000 ILAMB-2.6/test/test_run_script.py
+drwxr-xr-x   0 nate      (1000) nate      (1000)        0 2023-06-28 11:50:43.114793 ILAMB-2.7/
+-rw-r--r--   0 nate      (1000) nate      (1000)     1466 2023-06-21 18:51:45.000000 ILAMB-2.7/LICENSE.rst
+-rw-r--r--   0 nate      (1000) nate      (1000)     4150 2023-06-28 11:50:43.114793 ILAMB-2.7/PKG-INFO
+-rw-r--r--   0 nate      (1000) nate      (1000)     3396 2023-06-28 11:32:53.000000 ILAMB-2.7/README.md
+drwxr-xr-x   0 nate      (1000) nate      (1000)        0 2023-06-28 11:50:43.103793 ILAMB-2.7/bin/
+-rw-r--r--   0 nate      (1000) nate      (1000)     6117 2023-06-21 18:51:45.000000 ILAMB-2.7/bin/ilamb-fetch
+-rw-r--r--   0 nate      (1000) nate      (1000)     8654 2023-06-21 18:51:45.000000 ILAMB-2.7/bin/ilamb-mean
+-rw-r--r--   0 nate      (1000) nate      (1000)    33027 2023-06-27 23:05:00.000000 ILAMB-2.7/bin/ilamb-run
+-rw-r--r--   0 nate      (1000) nate      (1000)      550 2023-06-21 18:51:45.000000 ILAMB-2.7/bin/ilamb-setup
+-rw-r--r--   0 nate      (1000) nate      (1000)       38 2023-06-28 11:50:43.115793 ILAMB-2.7/setup.cfg
+-rw-r--r--   0 nate      (1000) nate      (1000)     3531 2023-06-28 11:50:06.000000 ILAMB-2.7/setup.py
+drwxr-xr-x   0 nate      (1000) nate      (1000)        0 2023-06-28 11:50:43.101793 ILAMB-2.7/src/
+drwxr-xr-x   0 nate      (1000) nate      (1000)        0 2023-06-28 11:50:43.111793 ILAMB-2.7/src/ILAMB/
+-rw-r--r--   0 nate      (1000) nate      (1000)     7687 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfAlbedo.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    10073 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfBasin.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     1502 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfBurntArea.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    55956 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfCO2.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     1213 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfContentChange.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     1343 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfDepthGradient.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    17813 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfDiurnal.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     8410 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfEvapFraction.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     5087 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfGSNF.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    13197 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfNBP.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    15334 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfPermafrost.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     9273 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfRunoff.py
+-rw-r--r--   0 nate      (1000) nate      (1000)      418 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfSWE.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    10972 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfSoilCarbon.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    11360 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfTWSA.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    16046 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfUSGS.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    17412 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ConfUncertainty.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    54126 2023-06-27 23:05:00.000000 ILAMB-2.7/src/ILAMB/Confrontation.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    20569 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ModelResult.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    54072 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/Post.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    15692 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/Regions.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    19906 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/Relationship.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    40232 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/Scoreboard.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    83196 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/Variable.py
+-rw-r--r--   0 nate      (1000) nate      (1000)       73 2023-06-22 12:17:59.000000 ILAMB-2.7/src/ILAMB/__init__.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    38068 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/ccgfilt.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     7999 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/constants.py
+drwxr-xr-x   0 nate      (1000) nate      (1000)        0 2023-06-28 11:50:43.114793 ILAMB-2.7/src/ILAMB/data/
+-rw-r--r--   0 nate      (1000) nate      (1000)    14822 2023-06-27 23:05:00.000000 ILAMB-2.7/src/ILAMB/data/cmip.cfg
+-rw-rw-r--   0 nate      (1000) nate      (1000)     1772 2020-04-03 15:19:29.000000 ILAMB-2.7/src/ILAMB/data/diurnal.cfg
+-rw-r--r--   0 nate      (1000) nate      (1000)    17551 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/data/ilamb.cfg
+-rw-r--r--   0 nate      (1000) nate      (1000)     4077 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/data/iomb.cfg
+-rw-r--r--   0 nate      (1000) nate      (1000)    36366 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/data/quantiles_Whittaker_cmip5v6.parquet
+-rw-rw-r--   0 nate      (1000) nate      (1000)      351 2020-04-03 15:19:29.000000 ILAMB-2.7/src/ILAMB/data/sample.cfg
+-rw-r--r--   0 nate      (1000) nate      (1000)     9143 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/e3sm_result.py
+-rw-rw-r--   0 nate      (1000) nate      (1000)      246 2023-06-28 11:50:43.000000 ILAMB-2.7/src/ILAMB/generated_version.py
+-rw-r--r--   0 nate      (1000) nate      (1000)    99368 2023-06-22 11:00:50.000000 ILAMB-2.7/src/ILAMB/ilamblib.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     6339 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/point_result.py
+-rw-r--r--   0 nate      (1000) nate      (1000)     6356 2023-06-21 18:51:45.000000 ILAMB-2.7/src/ILAMB/run.py
+drwxr-xr-x   0 nate      (1000) nate      (1000)        0 2023-06-28 11:50:43.113793 ILAMB-2.7/src/ILAMB.egg-info/
+-rw-r--r--   0 nate      (1000) nate      (1000)     4150 2023-06-28 11:50:43.000000 ILAMB-2.7/src/ILAMB.egg-info/PKG-INFO
+-rw-r--r--   0 nate      (1000) nate      (1000)     1289 2023-06-28 11:50:43.000000 ILAMB-2.7/src/ILAMB.egg-info/SOURCES.txt
+-rw-r--r--   0 nate      (1000) nate      (1000)        1 2023-06-28 11:50:43.000000 ILAMB-2.7/src/ILAMB.egg-info/dependency_links.txt
+-rw-r--r--   0 nate      (1000) nate      (1000)        1 2023-03-24 20:56:20.000000 ILAMB-2.7/src/ILAMB.egg-info/not-zip-safe
+-rw-r--r--   0 nate      (1000) nate      (1000)      221 2023-06-28 11:50:43.000000 ILAMB-2.7/src/ILAMB.egg-info/requires.txt
+-rw-r--r--   0 nate      (1000) nate      (1000)        6 2023-06-28 11:50:43.000000 ILAMB-2.7/src/ILAMB.egg-info/top_level.txt
+drwxr-xr-x   0 nate      (1000) nate      (1000)        0 2023-06-28 11:50:43.114793 ILAMB-2.7/test/
+-rw-rw-r--   0 nate      (1000) nate      (1000)     5728 2020-04-03 15:19:29.000000 ILAMB-2.7/test/test_Variable.py
+-rw-rw-r--   0 nate      (1000) nate      (1000)      352 2020-04-03 15:19:29.000000 ILAMB-2.7/test/test_run_script.py
```

### Comparing `ILAMB-2.6/bin/ilamb-fetch` & `ILAMB-2.7/bin/ilamb-fetch`

 * *Files 26% similar despite different names*

```diff
@@ -1,123 +1,203 @@
 #!/usr/bin/env python
-import hashlib,argparse,os,sys
+import argparse
+import hashlib
+import os
+import sys
+
 import requests
 
 # Here I am disabling warnings as they pollute long download
 # screens. However, I am passing a more readable warning to the user
 # instructing them.
 from requests.packages.urllib3.exceptions import InsecureRequestWarning
+from tqdm import tqdm
+
 requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
 
+
 def BuildDirectories(filepath):
     d = os.path.dirname(filepath)
     if not os.path.isdir(d):
         os.makedirs(d)
-        
+
+
 def filehash(filepath):
-    blocksize = 64*1024
+    blocksize = 64 * 1024
     sha = hashlib.sha1()
-    with open(filepath, 'rb') as fp:
+    with open(filepath, "rb") as fp:
         while True:
             data = fp.read(blocksize)
             if not data:
                 break
             sha.update(data)
     return sha.hexdigest()
 
-def GenerateSha1sumFile(root,suffix=".nc"):
-    
+
+def GenerateSha1sumFile(root, suffix=".nc"):
     lines = ""
     for topdir, dirs, files in os.walk(root):
+        if topdir.startswith("_"):
+            continue
+        if topdir.startswith("./_"):
+            continue
         for fpath in [os.path.join(topdir, f) for f in files]:
-            if not fpath.endswith(suffix): continue
+            if not fpath.endswith(suffix):
+                continue
             size = os.path.getsize(fpath)
             sha = filehash(fpath)
             name = os.path.relpath(fpath, root)
-            lines += '%s  %s\n' % (sha,name)
+            lines += "%s  %s\n" % (sha, name)
     return lines
 
-def CheckSha1sumFile(sha1sumfile,root):
 
+def CheckSha1sumFile(sha1sumfile, root):
     needs_updating = []
     with open(sha1sumfile) as f:
         lines = f.readlines()
         for line in lines:
             line = line.split()
-            sha1sum,filename = line
-            fpath = os.path.join(root,filename)
+            sha1sum, filename = line
+            fpath = os.path.join(root, filename)
             if os.path.isfile(fpath):
-                if sha1sum != filehash(fpath): needs_updating.append(filename)
+                if sha1sum != filehash(fpath):
+                    needs_updating.append(filename)
             else:
                 needs_updating.append(filename)
     return needs_updating
-    
+
+
 # default value is ILAMB_ROOT if set
 local_root = "./"
-if "ILAMB_ROOT" in os.environ: local_root = os.environ["ILAMB_ROOT"]
+if "ILAMB_ROOT" in os.environ:
+    local_root = os.environ["ILAMB_ROOT"]
 
 # parse options
 parser = argparse.ArgumentParser(description=__doc__)
-parser.add_argument('--local_root', dest="local_root", metavar="PATH", type=str, default=local_root,
-                    help='Location on your system.')
-parser.add_argument('--remote_root', dest="remote_root", metavar="PATH", type=str, default="https://www.ilamb.org/ILAMB-Data/",
-                    help='Location on the remote system.')
-parser.add_argument('-c', '--create', dest="create", action="store_true",
-                    help='Enable to create a sha1sum check file of the contents of the local root')
-parser.add_argument('--no-check-certificate', dest="check", action="store_true",
-                    help='Enable to skip checking authenticity of the downloaded certificate')
-parser.add_argument('-y', dest="auto_accept", action="store_true",
-                    help='Enable to automatically accept the query to download files')
+parser.add_argument(
+    "--local_root",
+    dest="local_root",
+    metavar="PATH",
+    type=str,
+    default=local_root,
+    help="Location on your system.",
+)
+parser.add_argument(
+    "--remote_root",
+    dest="remote_root",
+    metavar="PATH",
+    type=str,
+    default="https://www.ilamb.org/ILAMB-Data/",
+    help="Location on the remote system.",
+)
+parser.add_argument(
+    "-c",
+    "--create",
+    dest="create",
+    action="store_true",
+    help="Enable to create a sha1sum check file of the contents of the local root",
+)
+parser.add_argument(
+    "--no-check-certificate",
+    dest="check",
+    action="store_true",
+    help="Enable to skip checking authenticity of the downloaded certificate",
+)
+parser.add_argument(
+    "-y",
+    dest="auto_accept",
+    action="store_true",
+    help="Enable to automatically accept the query to download files",
+)
 args = parser.parse_args()
 
 # use create mode if you want to make a checksum file of a directory
 if args.create:
-    with open(args.local_root + "/SHA1SUM",mode="w") as f:
+    with open(args.local_root + "/SHA1SUM", mode="w") as f:
         f.write(GenerateSha1sumFile(args.local_root))
     sys.exit()
-    
-print("\nComparing remote location:\n\n\t%s\n\nTo local location:\n\n\t%s" % (args.remote_root,args.local_root))
+
+print(
+    "\nComparing remote location:\n\n\t%s\n\nTo local location:\n\n\t%s"
+    % (args.remote_root, args.local_root)
+)
 
 # download and build the sha1sum check files
 try:
-    resp = requests.get(args.remote_root + "/SHA1SUM", verify= (not args.check))
+    resp = requests.get(args.remote_root + "/SHA1SUM", verify=(not args.check))
 except requests.exceptions.SSLError:
-    print("""
+    print(
+        """
 SSLError: The certificate from the remote site you contacted could not
 be verified. If you trust this site (for example if you are connecting
 to our server https://www.ilamb.org) then you may rerun ilamb-fetch
 with the --no-check-certificate option which will bypass the
 certificate check step.
-""")
+"""
+    )
     sys.exit(1)
 
-with open(args.local_root  + "/SHA1SUM", "wb") as f:
+with open(args.local_root + "/SHA1SUM", "wb") as f:
     f.write(resp.content)
-if "404 Not Found" in open(args.local_root  + "/SHA1SUM").read():
-    raise ValueError("Could not find the sha1 sum file: %s" % (args.remote_root + "/SHA1SUM"))
-needs_updating = CheckSha1sumFile(args.local_root  + "/SHA1SUM",args.local_root)
+if "404 Not Found" in open(args.local_root + "/SHA1SUM").read():
+    raise ValueError(
+        "Could not find the sha1 sum file: %s" % (args.remote_root + "/SHA1SUM")
+    )
+needs_updating = CheckSha1sumFile(args.local_root + "/SHA1SUM", args.local_root)
 
 if len(needs_updating) == 0:
     print("\nAll your data is up-to-date and clean.\n")
-    os.system("rm -f " + args.local_root  + "/SHA1SUM")
+    os.system("rm -f " + args.local_root + "/SHA1SUM")
     sys.exit()
 
 print("\nI found the following files which are missing, out of date, or corrupt:\n")
+
 for key in needs_updating:
-    print("\t%s/%s" % (args.local_root,key))
+    print("\t%s%s" % (args.local_root, key))
+
+if args.auto_accept:
+    reply = "y"
+else:
+    reply = str(input("\nCalculate Total Download size? [y/n] ")).lower().strip()
+
+if reply[0] == "y":
+    total_download_size = 0
+    with tqdm(total=len(needs_updating)) as pbar:
+        for key in needs_updating:
+            resp = requests.get(
+                args.remote_root + "/" + key, stream=True, verify=(not args.check)
+            )
+            total_download_size += int(resp.headers.get("content-length"))
+            pbar.update(1)
+    print("\nTotal download size: %6.1f MB" % (total_download_size / 1e6))
 
 if args.auto_accept:
-    reply = 'y'
+    reply = "y"
 else:
-    reply = str(input('\nDownload replacements? [y/n] ')).lower().strip()
-if reply[0] == 'y':
+    reply = str(input("\nDownload replacements? [y/n] ")).lower().strip()
+if reply[0] == "y":
     print(" ")
     for key in needs_updating:
-        print("\tDownloading %s/%s..." % (args.remote_root,key))
-        BuildDirectories(args.local_root  + "/" + key)
-        resp = requests.get(args.remote_root + "/" + key, verify=(not args.check))
-        with open(args.local_root  + "/" + key, "wb") as f:
-            f.write(resp.content)
+        print("\tDownloading %s/%s..." % (args.remote_root, key))
+        BuildDirectories(args.local_root + "/" + key)
+        resp = requests.get(
+            args.remote_root + "/" + key, stream=True, verify=(not args.check)
+        )
+        total_size = int(resp.headers.get("content-length"))
+        initial_pos = 0
+        file = args.local_root + "/" + key
+        with open(file, "wb") as f:
+            with tqdm(
+                total=total_size,
+                unit="B",
+                unit_scale=True,
+                desc=file,
+                initial=initial_pos,
+                ascii=True,
+            ) as pbar:
+                for ch in resp.iter_content(chunk_size=1024):
+                    if ch:
+                        f.write(ch)
+                        pbar.update(len(ch))
     print("\nDownload complete. Rerun ilamb-fetch to check file integrity.\n")
-    
-os.system("rm -f " + args.local_root  + "/SHA1SUM")
 
+os.system("rm -f " + args.local_root + "/SHA1SUM")
```

### Comparing `ILAMB-2.6/bin/ilamb-mean` & `ILAMB-2.7/bin/ilamb-mean`

 * *Files 14% similar despite different names*

```diff
@@ -1,203 +1,335 @@
 #!/usr/bin/env python
 """
 Computes a multimodel mean.
 """
-from ILAMB.Scoreboard import Scoreboard
+import argparse
+import os
+import pickle
+import sys
+
+import ILAMB.ilamblib as il
+import ILAMB.run as r
+import numpy as np
 from ILAMB.constants import bnd_months
+from ILAMB.Scoreboard import Scoreboard
 from ILAMB.Variable import Variable
-import ILAMB.ilamblib as il
 from netCDF4 import Dataset
 from sympy import sympify
-import ILAMB.run as r
-import numpy as np
-import argparse
-import pickle
-import sys
-import os
 
-def Interpolate(v,t,d,lat,lon):
+
+def Interpolate(v, t, d, lat, lon):
     args = []
-    if   t is not None: args.append((np.abs(  t[:,np.newaxis]-v.time )).argmin(axis=1))
-    if   d is not None: args.append((np.abs(  d[:,np.newaxis]-v.depth)).argmin(axis=1))
-    if lat is not None: args.append((np.abs(lat[:,np.newaxis]-v.lat  )).argmin(axis=1))
-    if lon is not None: args.append((np.abs(lon[:,np.newaxis]-v.lon  )).argmin(axis=1))
+    if t is not None:
+        args.append((np.abs(t[:, np.newaxis] - v.time)).argmin(axis=1))
+    if d is not None:
+        args.append((np.abs(d[:, np.newaxis] - v.depth)).argmin(axis=1))
+    if lat is not None:
+        args.append((np.abs(lat[:, np.newaxis] - v.lat)).argmin(axis=1))
+    if lon is not None:
+        args.append((np.abs(lon[:, np.newaxis] - v.lon)).argmin(axis=1))
     return v.data[np.ix_(*args)]
 
-def MultiModelMean(M,vname,wgt_file=None,maxV=20):
 
+def MultiModelMean(M, vname, wgt_file=None, maxV=20):
     wgts = None
     if wgt_file is not None:
-        with open(wgt_file[0],"rb") as f:
+        with open(wgt_file[0], "rb") as f:
             wgts = pickle.load(f)
-            print("Loaded weights:",wgts)
-            
-    sys.stdout.write(('{0:>%d} ' % maxV).format(vname)); sys.stdout.flush()
+            print("Loaded weights:", wgts)
+
+    sys.stdout.write(("{0:>%d} " % maxV).format(vname))
+    sys.stdout.flush()
 
     # Based on the variable, we need to set up a space-time grid. So
     # first we need find a model with the variable and then check what
     # kind of variable it is.
     Ms = [m for m in M if vname in m.variables]
     if len(Ms) == 0:
-        sys.stdout.write('\n'); sys.stdout.flush()
+        sys.stdout.write("\n")
+        sys.stdout.flush()
         return
     if vname == "sftlf":
-        v = Variable(filename = Ms[0].variables['sftlf'][0],variable_name = 'sftlf').convert("1")
+        v = Variable(
+            filename=Ms[0].variables["sftlf"][0], variable_name="sftlf"
+        ).convert("1")
     else:
         v = Ms[0].extractTimeSeries(vname)
     unit = v.unit
     shp = []
 
     # If the variable is temporal
-    t = None; t_bnd = None; t0 = -1e20; tf = 1e20
+    t = None
+    t_bnd = None
+    t0 = -1e20
+    tf = 1e20
     if v.temporal:
         y0 = 1850 if vname == "nbp" else 1950
-        yrs = (np.asarray(range(y0,2016),dtype=float)-1850)*365
-        yrs = (yrs[:,np.newaxis] + bnd_months[:12]).flatten()
-        t_bnd = np.asarray([yrs[:-1],yrs[1:]]).T
+        yrs = (np.asarray(range(y0, 2016), dtype=float) - 1850) * 365
+        yrs = (yrs[:, np.newaxis] + bnd_months[:12]).flatten()
+        t_bnd = np.asarray([yrs[:-1], yrs[1:]]).T
         t = t_bnd.mean(axis=1)
-        shp += [t.size,]
-        t0 = t_bnd[0,0]; tf = t_bnd[-1,1]
+        shp += [
+            t.size,
+        ]
+        t0 = t_bnd[0, 0]
+        tf = t_bnd[-1, 1]
 
     # If the variable is layered
-    d = None; d_bnd = None
+    d = None
+    d_bnd = None
     if v.layered:
-        d_bnd = np.asarray([[0.000, 0.025],
-                            [0.025, 0.065],
-                            [0.065, 0.125],
-                            [0.125, 0.21],
-                            [0.21, 0.33],
-                            [0.33, 0.49],
-                            [0.49, 0.69],
-                            [0.69, 0.9299999],
-                            [0.9299999, 1.21],
-                            [1.21, 1.53],
-                            [1.53, 1.89],
-                            [1.89, 2.29],
-                            [2.29, 2.745],
-                            [2.745, 3.285],
-                            [3.285, 3.925],
-                            [3.925, 4.665],
-                            [4.665, 5.505]])
+        d_bnd = np.asarray(
+            [
+                [0.000, 0.025],
+                [0.025, 0.065],
+                [0.065, 0.125],
+                [0.125, 0.21],
+                [0.21, 0.33],
+                [0.33, 0.49],
+                [0.49, 0.69],
+                [0.69, 0.9299999],
+                [0.9299999, 1.21],
+                [1.21, 1.53],
+                [1.53, 1.89],
+                [1.89, 2.29],
+                [2.29, 2.745],
+                [2.745, 3.285],
+                [3.285, 3.925],
+                [3.925, 4.665],
+                [4.665, 5.505],
+            ]
+        )
         d = d_bnd.mean(axis=1)
-        shp += [d.size,]
+        shp += [
+            d.size,
+        ]
 
     # If the variable is spatial
-    lat = None; lat_bnd = None
-    lon = None; lon_bnd = None
+    lat = None
+    lat_bnd = None
+    lon = None
+    lon_bnd = None
     if v.spatial:
-        lat_bnd,lon_bnd,lat,lon = il.GlobalLatLonGrid(args.res)
-        lat_bnd = np.asarray([lat_bnd[:-1],lat_bnd[1:]]).T
-        lon_bnd = np.asarray([lon_bnd[:-1],lon_bnd[1:]]).T
-        shp += [lat.size,lon.size]
+        lat_bnd, lon_bnd, lat, lon = il.GlobalLatLonGrid(args.res)
+        lat_bnd = np.asarray([lat_bnd[:-1], lat_bnd[1:]]).T
+        lon_bnd = np.asarray([lon_bnd[:-1], lon_bnd[1:]]).T
+        shp += [lat.size, lon.size]
 
     # Let's start summing it up
-    data  = np.zeros(shp)
-    count = np.zeros(shp,dtype=int)
-    sumw  = 0.
+    data = np.zeros(shp)
+    count = np.zeros(shp, dtype=int)
+    sumw = 0.0
     for m in M:
-        sys.stdout.write('.'); sys.stdout.flush()
-        w = wgts[m.name] if wgts is not None else 1.
-        
+        sys.stdout.write(".")
+        sys.stdout.flush()
+        w = wgts[m.name] if wgts is not None else 1.0
+
         vname0 = vname
-        if vname == "cSoil" and "cSoilAbove1m" in m.variables: vname = "cSoilAbove1m"
-        if vname == "burntArea" and "burntFractionAll" in m.variables: vname = "burntFractionAll"
-        
+        if vname == "cSoil" and "cSoilAbove1m" in m.variables:
+            vname = "cSoilAbove1m"
+        if vname == "burntArea" and "burntFractionAll" in m.variables:
+            vname = "burntFractionAll"
+
         try:
             if vname == "sftlf":
-                v = Variable(filename = m.variables['sftlf'][0],variable_name = 'sftlf').convert("1")
+                v = Variable(
+                    filename=m.variables["sftlf"][0], variable_name="sftlf"
+                ).convert("1")
             else:
-                v = m.extractTimeSeries(vname,initial_time=t0,final_time=tf).convert(unit)
+                v = m.extractTimeSeries(vname, initial_time=t0, final_time=tf).convert(
+                    unit
+                )
         except:
             continue
-        v = Interpolate(v,t,d,lat,lon)
-        data  += v.data*(v.mask==False)*w
-        count +=        (v.mask==False)
-        sumw  += w
+        v = Interpolate(v, t, d, lat, lon)
+        data += v.data * (v.mask == False) * w
+        count += v.mask == False
+        sumw += w
         vname = vname0
 
     # Take the mean and write it out
-    with np.errstate(all='ignore'):
-        data = np.ma.masked_array(data=(data/sumw),mask=(count==0))
-    with Dataset(os.path.join(args.build_dir[0],"%s.nc" % vname),mode="w") as dset:
-        Variable(data       = data,
-                 unit       = unit,
-                 name       = vname,
-                 time       = t,
-                 time_bnds  = t_bnd,
-                 lat        = lat,
-                 lat_bnds   = lat_bnd,
-                 lon        = lon,
-                 lon_bnds   = lon_bnd,
-                 depth      = d,
-                 depth_bnds = d_bnd).toNetCDF4(dset)
+    with np.errstate(all="ignore"):
+        data = np.ma.masked_array(data=(data / sumw), mask=(count == 0))
+    with Dataset(os.path.join(args.build_dir[0], "%s.nc" % vname), mode="w") as dset:
+        Variable(
+            data=data,
+            unit=unit,
+            name=vname,
+            time=t,
+            time_bnds=t_bnd,
+            lat=lat,
+            lat_bnds=lat_bnd,
+            lon=lon,
+            lon_bnds=lon_bnd,
+            depth=d,
+            depth_bnds=d_bnd,
+        ).toNetCDF4(dset)
+
+    sys.stdout.write("\n")
+    sys.stdout.flush()
 
-    sys.stdout.write('\n'); sys.stdout.flush()
 
 parser = argparse.ArgumentParser(description=__doc__)
-parser.add_argument('--model_root', dest="model_root", metavar='root', type=str, nargs=1, default=["./"],
-                    help='root at which to search for models')
-parser.add_argument('--build_dir', dest="build_dir", metavar='build_dir', type=str, nargs=1,default=["./_build"],
-                    help='path of where to save the output')
-parser.add_argument('--config', dest="config", metavar='config', type=str, nargs=1,
-                    help='path to configuration file to use')
-parser.add_argument('--wgt', dest="wgt", metavar='wgt', type=str, nargs=1,
-                    help='path to weight file to use')
-parser.add_argument('--models', dest="models", metavar='m', type=str, nargs='+',default=[],
-                    help='specify which models to run, list model names with no quotes and only separated by a space.')
-parser.add_argument('--vars', dest="vars", metavar='v', type=str, nargs='+',default=[],
-                    help='list of the variables to take the mean of')
-parser.add_argument('-q','--quiet', dest="quiet", action="store_true",
-                    help='enable to silence screen output')
-parser.add_argument('--filter', dest="filter", metavar='filter', type=str, nargs=1, default=[""],
-                    help='a string which much be in the model filenames')
-parser.add_argument('--regex', dest="regex", metavar='regex', type=str, nargs=1, default=[""],
-                    help='a regular expression which filenames must conform to in order to be included')
-parser.add_argument('--model_setup', dest="model_setup", type=str, nargs='+',default=None,
-                    help='list files model setup information')
-parser.add_argument('-g','--same_grid', dest="same_grid", action="store_true",
-                    help='enable if all models are on the same grid')
-parser.add_argument('-r','--res', dest="res", type=float, default=1.0,
-                    help='enable if all models are on the same grid')
+parser.add_argument(
+    "--model_root",
+    dest="model_root",
+    metavar="root",
+    type=str,
+    nargs=1,
+    default=["./"],
+    help="root at which to search for models",
+)
+parser.add_argument(
+    "--build_dir",
+    dest="build_dir",
+    metavar="build_dir",
+    type=str,
+    nargs=1,
+    default=["./_build"],
+    help="path of where to save the output",
+)
+parser.add_argument(
+    "--config",
+    dest="config",
+    metavar="config",
+    type=str,
+    nargs=1,
+    help="path to configuration file to use",
+)
+parser.add_argument(
+    "--wgt",
+    dest="wgt",
+    metavar="wgt",
+    type=str,
+    nargs=1,
+    help="path to weight file to use",
+)
+parser.add_argument(
+    "--models",
+    dest="models",
+    metavar="m",
+    type=str,
+    nargs="+",
+    default=[],
+    help="specify which models to run, list model names with no quotes and only separated by a space.",
+)
+parser.add_argument(
+    "--vars",
+    dest="vars",
+    metavar="v",
+    type=str,
+    nargs="+",
+    default=[],
+    help="list of the variables to take the mean of",
+)
+parser.add_argument(
+    "-q",
+    "--quiet",
+    dest="quiet",
+    action="store_true",
+    help="enable to silence screen output",
+)
+parser.add_argument(
+    "--filter",
+    dest="filter",
+    metavar="filter",
+    type=str,
+    nargs=1,
+    default=[""],
+    help="a string which much be in the model filenames",
+)
+parser.add_argument(
+    "--regex",
+    dest="regex",
+    metavar="regex",
+    type=str,
+    nargs=1,
+    default=[""],
+    help="a regular expression which filenames must conform to in order to be included",
+)
+parser.add_argument(
+    "--model_setup",
+    dest="model_setup",
+    type=str,
+    nargs="+",
+    default=None,
+    help="list files model setup information",
+)
+parser.add_argument(
+    "-g",
+    "--same_grid",
+    dest="same_grid",
+    action="store_true",
+    help="enable if all models are on the same grid",
+)
+parser.add_argument(
+    "-r",
+    "--res",
+    dest="res",
+    type=float,
+    default=1.0,
+    help="enable if all models are on the same grid",
+)
 
 # Setup models to be used
 args = parser.parse_args()
-if not os.path.isdir(args.build_dir[0]): os.makedirs(args.build_dir[0])
+if not os.path.isdir(args.build_dir[0]):
+    os.makedirs(args.build_dir[0])
 if args.model_setup is None:
-    M = r.InitializeModels(args.model_root[0],
-                           args.models,
-                           False,
-                           filter=args.filter[0],
-                           regex=args.regex[0],
-                           models_path=args.build_dir[0])
+    M = r.InitializeModels(
+        args.model_root[0],
+        args.models,
+        False,
+        filter=args.filter[0],
+        regex=args.regex[0],
+        models_path=args.build_dir[0],
+    )
 else:
-    M = r.ParseModelSetup(args.model_setup[0],args.models,False,filter=args.filter[0],models_path=args.build_dir[0])
+    M = r.ParseModelSetup(
+        args.model_setup[0],
+        args.models,
+        False,
+        filter=args.filter[0],
+        models_path=args.build_dir[0],
+    )
 M = [m for m in M if "Mean" not in m.name]
 if not args.quiet:
     print("\nTaking mean of the following models:\n")
-    for m in M: print("{0:>20}".format(m.name))
+    for m in M:
+        print("{0:>20}".format(m.name))
 
 # What variables do we need the means of?
 Vs = []
 if args.config is None:
-    for m in M: Vs += [v for v in m.variables.keys() if v not in Vs]
+    for m in M:
+        Vs += [v for v in m.variables.keys() if v not in Vs]
 else:
-    if not args.quiet: print("\nParsing config file %s...\n" % args.config[0])
-    S = Scoreboard(args.config[0],
-                   master    = True,
-                   verbose   = not args.quiet,
-                   build_dir = args.build_dir[0])
+    if not args.quiet:
+        print("\nParsing config file %s...\n" % args.config[0])
+    S = Scoreboard(
+        args.config[0], master=True, verbose=not args.quiet, build_dir=args.build_dir[0]
+    )
     for c in S.list():
-        vs  = [c.variable,]
-        vs +=  c.alternate_vars
-        if c.derived is not None: vs += [str(s) for s in sympify(c.derived).free_symbols]
+        vs = [
+            c.variable,
+        ]
+        vs += c.alternate_vars
+        if c.derived is not None:
+            vs += [str(s) for s in sympify(c.derived).free_symbols]
         Vs += [v for v in vs if v not in Vs]
-    if "co2" in Vs: Vs.pop(Vs.index("co2"))
-    if "cSoilAbove1m" in Vs: Vs.pop(Vs.index("cSoilAbove1m"))
-    if "burntFractionAll" in Vs: Vs.pop(Vs.index("burntFractionAll"))
-if len(args.vars) > 0: Vs = args.vars
-Vs = ['sftlf',] + Vs
-if not args.quiet: print("\nTaking the mean of [%s]...\n" % (", ".join(Vs)))
+    if "co2" in Vs:
+        Vs.pop(Vs.index("co2"))
+    if "cSoilAbove1m" in Vs:
+        Vs.pop(Vs.index("cSoilAbove1m"))
+    if "burntFractionAll" in Vs:
+        Vs.pop(Vs.index("burntFractionAll"))
+if len(args.vars) > 0:
+    Vs = args.vars
+Vs = [
+    "sftlf",
+] + Vs
+if not args.quiet:
+    print("\nTaking the mean of [%s]...\n" % (", ".join(Vs)))
 
 for vname in Vs:
-    MultiModelMean(M,vname,args.wgt)
-
+    MultiModelMean(M, vname, args.wgt)
```

### Comparing `ILAMB-2.6/bin/ilamb-run` & `ILAMB-2.7/bin/ilamb-run`

 * *Files 17% similar despite different names*

```diff
@@ -1,41 +1,76 @@
 #!/usr/bin/env python
 """
 Runs an ILAMB study.
 """
-import mpi4py.rc; mpi4py.rc.threads = False
+import mpi4py.rc
+
+mpi4py.rc.threads = False
 import logging
+
+import yaml
 from ILAMB.ModelResult import ModelResult
-from ILAMB.Scoreboard import Scoreboard
-from ILAMB.Regions import Regions
-from ILAMB import ilamblib as il
+
+try:
+    from ILAMB.point_result import ModelPointResult
+except:
+    ModelPointResult = None
+try:
+    from ILAMB.e3sm_result import E3SMResult
+except:
+    E3SMResult = None
+import argparse
+import datetime
+import glob
+import inspect
+import os
+import pickle
+import re
+import sys
+import time
 from traceback import format_exc
-import os,time,sys,argparse,pickle
-from mpi4py import MPI
+
+import matplotlib.colors as clr
+import matplotlib.pyplot as plt
 import numpy as np
-import datetime,glob
-from netCDF4 import Dataset
-import pylab as plt
-import re
+import pandas as pd
+from ILAMB import ilamblib as il
 from ILAMB.Post import RegisterCustomColormaps
-if "wetdry" not in plt.colormaps(): RegisterCustomColormaps()
+from ILAMB.Regions import Regions
+from ILAMB.Scoreboard import Scoreboard
+from mpi4py import MPI
+from netCDF4 import Dataset
+
+if "wetdry" not in plt.colormaps():
+    RegisterCustomColormaps()
+import platform
 
 # MPI stuff
 comm = MPI.COMM_WORLD
 size = comm.Get_size()
 rank = comm.Get_rank()
 proc = np.zeros(size)
 ierr = np.zeros(size)
 
 # Some color constants for printing to the terminal
-OK   = '\033[92m'
-FAIL = '\033[91m'
-ENDC = '\033[0m'
+OK = "\033[92m"
+FAIL = "\033[91m"
+ENDC = "\033[0m"
 
-def InitializeModels(model_root,models=[],verbose=False,filter="",regex="",model_year=[],log=True,models_path="./"):
+
+def InitializeModels(
+    model_root,
+    models=[],
+    verbose=False,
+    filter="",
+    regex="",
+    model_year=[],
+    log=True,
+    models_path="./",
+):
     """Initializes a list of models
 
     Initializes a list of models where each model is the subdirectory
     beneath the given model root directory. The global list of models
     will exist on each processor.
 
     Parameters
@@ -51,64 +86,161 @@
 
     Returns
     -------
     M : list of ILAMB.ModelResults.ModelsResults
        a list of the model results, sorted alphabetically by name
 
     """
-    # initialize the models    
+    # initialize the models
     M = []
-    if len(model_year) != 2: model_year = None
+    if len(model_year) != 2:
+        model_year = None
     max_model_name_len = 0
-    if rank == 0 and verbose: print("\nSearching for model results in %s\n" % model_root)
+    if rank == 0 and verbose:
+        print("\nSearching for model results in %s\n" % model_root)
     for subdir, dirs, files in os.walk(model_root):
         for mname in dirs:
-            if len(models) > 0 and mname not in models: continue
-            pkl_file = os.path.join(models_path,"%s.pkl" % mname)
+            if len(models) > 0 and mname not in models:
+                continue
+            pkl_file = os.path.join(models_path, "%s.pkl" % mname)
             if os.path.isfile(pkl_file):
-                with open(pkl_file,'rb') as infile:
+                with open(pkl_file, "rb") as infile:
                     m = pickle.load(infile)
             else:
                 try:
-                    m = ModelResult(os.path.join(subdir,mname), modelname = mname, filter=filter, regex=regex, model_year = model_year)
+                    m = ModelResult(
+                        os.path.join(subdir, mname),
+                        modelname=mname,
+                        filter=filter,
+                        regex=regex,
+                        model_year=model_year,
+                    )
                 except Exception as ex:
-                    if log: logger.debug("[%s]" % mname,format_exc())
+                    if log:
+                        logger.debug("[%s]" % mname, format_exc())
                     continue
             M.append(m)
-            max_model_name_len = max(max_model_name_len,len(mname))
+            max_model_name_len = max(max_model_name_len, len(mname))
         break
-    M = sorted(M,key=lambda m: m.name.upper())
-    
+    M = sorted(M, key=lambda m: m.name.upper())
+
     # assign unique colors
     clrs = il.GenerateDistinctColors(len(M))
     for m in M:
-        clr     = clrs.pop(0)
-        m.color = clr
+        m.color = clrs.pop(0)
 
     # save model objects as pickle files
     comm.Barrier()
     if rank == 0:
         for m in M:
-            pkl_file = os.path.join(models_path,"%s.pkl" % m.name)
-            with open(pkl_file,'wb') as out:
-                pickle.dump(m,out,pickle.HIGHEST_PROTOCOL)
-        
+            pkl_file = os.path.join(models_path, "%s.pkl" % m.name)
+            with open(pkl_file, "wb") as out:
+                pickle.dump(m, out, pickle.HIGHEST_PROTOCOL)
+
     # optionally output models which were found
     if rank == 0 and verbose:
         for m in M:
             print(("    {0:>45}").format(m.name))
 
     if len(M) == 0:
-        if verbose and rank == 0: print("No model results found")
+        if verbose and rank == 0:
+            print("No model results found")
         comm.Barrier()
         comm.Abort(0)
 
     return M
 
-def ParseModelSetup(model_setup,models=[],verbose=False,filter="",regex="",models_path="./"):
+
+def _parse_model_yaml(filename: str, cache_path: str = "./", only_models: list = []):
+    """Setup models using a yaml file."""
+    model_classes = {
+        "ModelPointResult": ModelPointResult,
+        "E3SMResult": E3SMResult,
+        "ModelResult": ModelResult,
+    }
+    models = []
+    with open(filename, encoding="utf-8") as fin:
+        yml = yaml.safe_load(fin)
+    for name, opts in yml.items():
+        # optionally filter models
+        if len(only_models) > 0 and name not in only_models:
+            continue
+
+        if "name" not in opts:
+            opts["name"] = name
+
+        # if the model_year option is given, convert to lits of floats
+        if "model_year" in opts:
+            opts["model_year"] = [
+                float(y.strip()) for y in opts["model_year"].split(",")
+            ]
+
+        # select the class type
+        cls = model_classes[opts["type"]] if "type" in opts else ModelResult
+        if cls is None:
+            typ = opts["type"]
+            raise ValueError(f"The model type '{typ}' is not available")
+        fcns = dir(cls)
+
+        # if the pickle file exists, just load it
+        cache = os.path.join(cache_path, f"{name}.pkl")
+        if os.path.exists(cache):
+            if "read_pickle" in fcns:
+                model = cls().read_pickle(cache)
+            else:
+                with open(cache, mode="rb") as fin:
+                    model = pickle.load(fin)
+            models.append(model)
+            continue
+
+        # call the constructor using keywords defined in the YAML file
+        cls = model_classes[opts["type"]] if "type" in opts else ModelResult
+        model = cls(
+            **{
+                key: opts[key]
+                for key in inspect.getfullargspec(cls).args
+                if key in opts
+            }
+        )
+
+        # some model types have a find_files() method, call if present loading
+        # proper keywords from the YAML file
+        if "find_files" in fcns:
+            model.find_files(
+                **{
+                    key: opts[key]
+                    for key in inspect.getfullargspec(model.find_files).args
+                    if key in opts
+                }
+            )
+
+        # some model types allow you to specify snynonms
+        if "add_synonym" in fcns and "synonyms" in opts:
+            for mvar, syn in opts["synonyms"].items():
+                model.add_synonym(mvar, syn)
+
+        # cache the model result
+        if rank == 0:
+            if "read_pickle" in fcns:
+                model.to_pickle(cache)
+            else:
+                with open(cache, mode="wb") as fin:
+                    pickle.dump(model, fin)
+
+        models.append(model)
+
+    for model in models:
+        if isinstance(model.color, str) and model.color.startswith("#"):
+            model.color = clr.hex2color(model.color)
+    return models
+
+
+def ParseModelSetup(
+    model_setup, models=[], verbose=False, filter="", regex="", models_path="./"
+):
     """Initializes a list of models
 
     Initializes a list of models where each model is the subdirectory
     beneath the given model root directory. The global list of models
     will exist on each processor.
 
     Parameters
@@ -122,75 +254,100 @@
 
     Returns
     -------
     M : list of ILAMB.ModelResults.ModelsResults
        a list of the model results, sorted alphabetically by name
 
     """
+    if rank == 0 and verbose:
+        print("\nSetting up model results from %s\n" % model_setup)
+
+    # intercept if this is a yaml file
+    if model_setup.endswith(".yaml"):
+        M = _parse_model_yaml(model_setup, cache_path=models_path, only_models=models)
+        if rank == 0 and verbose:
+            for m in M:
+                print(("    {0:>45}").format(m.name))
+            if len(M) == 0:
+                print("No model results found")
+                comm.Barrier()
+                comm.Abort(0)
+        return M
+
     # initialize the models
     M = []
     max_model_name_len = 0
-    if rank == 0 and verbose: print("\nSetting up model results from %s\n" % model_setup)
     with open(model_setup) as f:
         for line in f.readlines():
-            if line.strip().startswith("#"): continue
-            line       = line.split(",")
-            mname      = None
-            mdir       = None
+            if line.strip().startswith("#"):
+                continue
+            line = line.split(",")
+            mname = None
+            mdir = None
             model_year = None
-            mgrp       = ""
+            mgrp = ""
             if len(line) >= 2:
-                mname  = line[0].strip()
-                mdir   = line[1].strip()
+                mname = line[0].strip()
+                mdir = line[1].strip()
                 # if mdir not a directory, then maybe path is relative to ILAMB_ROOT
                 if not os.path.isdir(mdir):
-                    mdir = os.path.join(os.environ["ILAMB_ROOT"],mdir).strip()
-                if len(line) == 3: mgrp = line[2].strip()
+                    mdir = os.path.join(os.environ["ILAMB_ROOT"], mdir).strip()
+                if len(line) == 3:
+                    mgrp = line[2].strip()
             if len(line) == 4:
-                model_year = [float(line[2].strip()),float(line[3].strip())]
-            max_model_name_len = max(max_model_name_len,len(mname))
-            if (len(models) > 0 and mname not in models) or (mname is None): continue
-            pkl_file = os.path.join(models_path,"%s.pkl" % mname)
+                model_year = [float(line[2].strip()), float(line[3].strip())]
+            max_model_name_len = max(max_model_name_len, len(mname))
+            if (len(models) > 0 and mname not in models) or (mname is None):
+                continue
+            pkl_file = os.path.join(models_path, "%s.pkl" % mname)
             if os.path.isfile(pkl_file):
-                with open(pkl_file,'rb') as infile:
+                with open(pkl_file, "rb") as infile:
                     m = pickle.load(infile)
             else:
                 try:
-                    m = ModelResult(mdir, modelname = mname, filter=filter, regex=regex, model_year = model_year, group = mgrp)
+                    m = ModelResult(
+                        mdir,
+                        modelname=mname,
+                        filter=filter,
+                        regex=regex,
+                        model_year=model_year,
+                        group=mgrp,
+                    )
                 except Exception as ex:
-                    if log: logger.debug("[%s]" % mname,format_exc())
+                    logger.debug("[%s]" % mname, format_exc())
                     continue
             M.append(m)
 
     # assign unique colors
     clrs = il.GenerateDistinctColors(len(M))
     for m in M:
-        clr     = clrs.pop(0)
-        m.color = clr
+        m.color = clrs.pop(0)
 
     # save model objects as pickle files
     comm.Barrier()
     if rank == 0:
         for m in M:
-            pkl_file = os.path.join(models_path,"%s.pkl" % m.name)
-            with open(pkl_file,'wb') as out:
-                pickle.dump(m,out,pickle.HIGHEST_PROTOCOL)
-                
+            pkl_file = os.path.join(models_path, "%s.pkl" % m.name)
+            with open(pkl_file, "wb") as out:
+                pickle.dump(m, out, pickle.HIGHEST_PROTOCOL)
+
     # optionally output models which were found
     if rank == 0 and verbose:
         for m in M:
             print(("    {0:>45}").format(m.name))
 
     if len(M) == 0:
-        if verbose and rank == 0: print("No model results found")
+        if verbose and rank == 0:
+            print("No model results found")
         comm.Barrier()
         comm.Abort(0)
 
     return M
 
+
 def InitializeRegions(filenames):
     """Initialize regions from a list of files.
 
     If the file is a netCDF4 file, see documentation in
     ILAMB.Regions.addRegionNetCDF4 for details on the required
     format. If the file defines regions by latitude/longitude bounds,
     then we anticipate comma delimited rows in the following form:
@@ -209,21 +366,25 @@
     r = Regions()
     for filename in filenames:
         try:
             r.addRegionNetCDF4(filename)
         except IOError:
             for line in open(filename):
                 line = line.strip()
-                if line.startswith("#"): continue
+                if line.startswith("#"):
+                    continue
                 line = line.split(",")
                 if len(line) == 6:
-                    r.addRegionLatLonBounds(line[0].strip(),
-                                            line[1].strip(),
-                                            [float(line[2]),float(line[3])],
-                                            [float(line[4]),float(line[5])])
+                    r.addRegionLatLonBounds(
+                        line[0].strip(),
+                        line[1].strip(),
+                        [float(line[2]), float(line[3])],
+                        [float(line[4]), float(line[5])],
+                    )
+
 
 def MatchRelationshipConfrontation(C):
     """Match relationship strings to confrontation longnames
 
     We allow for relationships to be studied by specifying the
     confrontation longname in the configure file. This routine loops
     over all defined relationships and finds the matching
@@ -237,24 +398,26 @@
 
     Returns
     -------
     C : list of ILAMB.Confrontation.Confrontation
         the same list with relationships linked to confrontations
     """
     for c in C:
-        if c.relationships is None: continue
-        for i,longname in enumerate(c.relationships):
+        if c.relationships is None:
+            continue
+        for i, longname in enumerate(c.relationships):
             found = False
             for cor in C:
                 if longname.lower() == cor.longname.lower():
                     c.relationships[i] = cor
                     found = True
     return C
 
-def FilterConfrontationList(C,match_list):
+
+def FilterConfrontationList(C, match_list):
     """Filter the confrontation list
 
     Filter the confrontation list by requiring that at least one
     string in the input list is found in the longname in the
     confrontation.
 
     Parameters
@@ -265,22 +428,25 @@
        the list of strings
 
     Returns
     -------
     Cf : list of ILAMB.Confrontation.Confrontation
         the list of filtered confrontations
     """
-    if len(match_list) == 0: return C
+    if len(match_list) == 0:
+        return C
     Cf = []
     for c in C:
         for match in match_list:
-            if match in c.longname: Cf.append(c)
+            if match in c.longname:
+                Cf.append(c)
     return Cf
 
-def BuildLocalWorkList(M,C,skip_cache=False):
+
+def BuildLocalWorkList(M, C, skip_cache=False):
     """Build the local work list
 
     We enumerate a list of work by taking combinations of model
     results and confrontations. This list is partitioned evenly among
     processes preferring to cluster as many confrontations with the
     same name together. While the work of the model-confrontation pair
     is local, some post-processing operations need performed once per
@@ -301,58 +467,60 @@
     """
 
     # Evenly divide up the work among processes
     W = []
     for c in C:
         for m in M:
             if skip_cache:
-                
-                # if we want to skip we have to check that it is complete                
-                fname = os.path.join(c.output_path,"%s_%s.nc" % (c.name,m.name))
+                # if we want to skip we have to check that it is complete
+                fname = os.path.join(c.output_path, "%s_%s.nc" % (c.name, m.name))
                 complete = False
                 if os.path.isfile(fname):
                     try:
                         with Dataset(fname) as dset:
                             if "complete" in dset.ncattrs():
-                                if dset.complete: complete = True
+                                if dset.complete:
+                                    complete = True
                     except:
                         pass
                 if not complete:
                     os.system("rm -f %s" % fname)
-                    W.append([m,c])
+                    W.append([m, c])
             else:
-                W.append([m,c])
+                W.append([m, c])
 
-
-    wpp    = float(len(W))/size
-    begin  = int(round( rank   *wpp))
-    end    = int(round((rank+1)*wpp))
+    wpp = float(len(W)) / size
+    begin = int(round(rank * wpp))
+    end = int(round((rank + 1) * wpp))
     localW = W[begin:end]
 
     # Determine who is the master of each confrontation
     for c in C:
-        sendbuf = np.zeros(size,dtype='int')
+        sendbuf = np.zeros(size, dtype="int")
         for w in localW:
-            if c is w[1]: sendbuf[rank] += 1
+            if c is w[1]:
+                sendbuf[rank] += 1
         recvbuf = None
-        if rank == 0: recvbuf = np.empty([size, sendbuf.size],dtype='int')
-        comm.Gather(sendbuf,recvbuf,root=0)
+        if rank == 0:
+            recvbuf = np.empty([size, sendbuf.size], dtype="int")
+        comm.Gather(sendbuf, recvbuf, root=0)
         if rank == 0:
             numc = recvbuf.sum(axis=1)
         else:
-            numc = np.empty(size,dtype='int')
-        comm.Bcast(numc,root=0)
+            numc = np.empty(size, dtype="int")
+        comm.Bcast(numc, root=0)
         if rank == numc.argmax():
             c.master = True
         else:
             c.master = False
 
     return localW
 
-def WorkConfront(W,verbose=False,clean=False):
+
+def WorkConfront(W, verbose=False, clean=False):
     """Performs the confrontation analysis
 
     For each model-confrontation pair (m,c) in the input work list,
     this routine will call c.confront(m) and keep track of the time
     required as well as any exceptions which are thrown.
 
     Parameters
@@ -361,46 +529,66 @@
         the list of work
     verbose : bool, optional
         enable to print output to the screen monitoring progress
     clean : bool, optional
         enable to perform the confrontation again, overwriting previous results
 
     """
-    maxCL = 45; maxML = 20
+    maxCL = 45
+    maxML = 20
 
     # Run analysis on your local work model-confrontation pairs
-    for i,w in enumerate(W):
-        m,c = w
+    for i, w in enumerate(W):
+        m, c = w
 
         # if the results file exists, skip this confrontation unless we want to clean
-        if os.path.isfile(os.path.join(c.output_path,"%s_%s.nc" % (c.name,m.name))) and clean is False:
+        if (
+            os.path.isfile(os.path.join(c.output_path, "%s_%s.nc" % (c.name, m.name)))
+            and clean is False
+        ):
             if verbose:
-                print(("    {0:>%d} {1:<%d} %sUsingCachedData%s " % (maxCL,maxML,OK,ENDC)).format(c.longname,m.name))
+                print(
+                    (
+                        "    {0:>%d} {1:<%d} %sUsingCachedData%s "
+                        % (maxCL, maxML, OK, ENDC)
+                    ).format(c.longname, m.name)
+                )
                 sys.stdout.flush()
             continue
 
         # try to run the confrontation
         try:
-            t0 = time.time()            
+            t0 = time.time()
             c.confront(m)
-            dt = time.time()-t0
+            dt = time.time() - t0
             proc[rank] += dt
             if verbose:
-                dt = datetime.timedelta(seconds=max(1,int(np.round(dt))))
-                print(("    {0:>%d} {1:<%d} %sCompleted%s {2:>8}" % (maxCL,maxML,OK,ENDC)).format(c.longname,m.name,str(dt)))
+                dt = datetime.timedelta(seconds=max(1, int(np.round(dt))))
+                print(
+                    (
+                        "    {0:>%d} {1:<%d} %sCompleted%s {2:>8}"
+                        % (maxCL, maxML, OK, ENDC)
+                    ).format(c.longname, m.name, str(dt))
+                )
                 sys.stdout.flush()
 
         # if things do not work out, print the exception so the user has some idea
         except Exception as ex:
             ierr[rank] = 1
-            logger.debug("[%s][%s]\n%s" % (c.longname,m.name,format_exc()))
+            logger.debug("[%s][%s]\n%s" % (c.longname, m.name, format_exc()))
             if verbose:
-                print(("    {0:>%d} {1:<%d} %s%s%s" % (maxCL,maxML,FAIL,ex.__class__.__name__,ENDC)).format(c.longname,m.name))
-                
-def WorkPost(M,C,W,S,verbose=False,skip_plots=False):
+                print(
+                    (
+                        "    {0:>%d} {1:<%d} %s%s%s"
+                        % (maxCL, maxML, FAIL, ex.__class__.__name__, ENDC)
+                    ).format(c.longname, m.name)
+                )
+
+
+def WorkPost(M, C, W, S, verbose=False, skip_plots=False):
     """Performs the post-processing
 
     Determines plot limits across all models, makes plots, generates
     other forms of HTML output.
 
     Parameters
     ----------
@@ -413,86 +601,107 @@
     S : ILAMB.Scoreboard.Scoreboard
         the scoreboard context
     verbose : bool, optional
         enable to print output to the screen monitoring progress
     skip_plots : bool, optional
         enable to skip plotting
     """
-    maxCL = 45; maxML = 20
-    for c in C: c.determinePlotLimits()
-    for i,w in enumerate(W):
-        m,c = w
+    maxCL = 45
+    maxML = 20
+    for c in C:
+        c.determinePlotLimits()
+    for i, w in enumerate(W):
+        m, c = w
         try:
             t0 = time.time()
             c.modelPlots(m)
             c.sitePlots(m)
             c.computeOverallScore(m)
-            dt = time.time()-t0
+            dt = time.time() - t0
             proc[rank] += dt
             if verbose:
-                dt = datetime.timedelta(seconds=max(1,int(np.round(dt))))
-                print(("    {0:>%d} {1:<%d} %sCompleted%s {2:>8}" % (maxCL,maxML,OK,ENDC)).format(c.longname,m.name,str(dt)))
+                dt = datetime.timedelta(seconds=max(1, int(np.round(dt))))
+                print(
+                    (
+                        "    {0:>%d} {1:<%d} %sCompleted%s {2:>8}"
+                        % (maxCL, maxML, OK, ENDC)
+                    ).format(c.longname, m.name, str(dt))
+                )
                 sys.stdout.flush()
         except Exception as ex:
             ierr[rank] = 1
-            logger.debug("[%s][%s]\n%s" % (c.longname,m.name,format_exc()))
+            logger.debug("[%s][%s]\n%s" % (c.longname, m.name, format_exc()))
             if verbose:
-                print(("    {0:>%d} {1:<%d} %s%s%s" % (maxCL,maxML,FAIL,ex.__class__.__name__,ENDC)).format(c.longname,m.name))
+                print(
+                    (
+                        "    {0:>%d} {1:<%d} %s%s%s"
+                        % (maxCL, maxML, FAIL, ex.__class__.__name__, ENDC)
+                    ).format(c.longname, m.name)
+                )
                 sys.stdout.flush()
 
-    sys.stdout.flush(); comm.Barrier()
-    
-    for i,c in enumerate(C):
+    sys.stdout.flush()
+    comm.Barrier()
+
+    for i, c in enumerate(C):
         try:
             c.compositePlots()
         except Exception as ex:
             ierr[rank] = 1
-            logger.debug("[compositePlots][%s]\n%s" % (c.longname,format_exc()))
+            logger.debug("[compositePlots][%s]\n%s" % (c.longname, format_exc()))
         c.generateHtml()
 
-    sys.stdout.flush(); comm.Barrier()
+    sys.stdout.flush()
+    comm.Barrier()
+
 
-def RestrictiveModelExtents(M,eps=2.):
-    extents0 = np.asarray([[-90.,+90.],[-180.,+180.]])
-    extents  = extents0.copy()
+def RestrictiveModelExtents(M, eps=2.0):
+    extents0 = np.asarray([[-90.0, +90.0], [-180.0, +180.0]])
+    extents = extents0.copy()
     for m in M:
+        if not hasattr(m, "extents"):
+            continue
         for i in range(2):
-            extents[i,0] = max(extents[i,0],m.extents[i,0])
-            extents[i,1] = min(extents[i,1],m.extents[i,1])
-    diff    = np.abs(extents0-extents)
-    extents = (diff<=eps)*extents0 + (diff>eps)*extents
+            extents[i, 0] = max(extents[i, 0], m.extents[i, 0])
+            extents[i, 1] = min(extents[i, 1], m.extents[i, 1])
+    diff = np.abs(extents0 - extents)
+    extents = (diff <= eps) * extents0 + (diff > eps) * extents
     return extents
 
+
 class MPIFileHandler(logging.FileHandler):
     """
     Class written by Di Cheng for parallel logging.
 
     https://gist.github.com/chengdi123000/42ec8ed2cbef09ee050766c2f25498cb
 
     """
-    def __init__(self,
-                 filename,
-                 mode=MPI.MODE_WRONLY|MPI.MODE_CREATE|MPI.MODE_APPEND ,
-                 encoding='utf-8',
-                 delay=False,
-                 comm=MPI.COMM_WORLD ):
+
+    def __init__(
+        self,
+        filename,
+        mode=MPI.MODE_WRONLY | MPI.MODE_CREATE | MPI.MODE_APPEND,
+        encoding="utf-8",
+        delay=False,
+        comm=MPI.COMM_WORLD,
+    ):
         self.baseFilename = os.path.abspath(filename)
         self.mode = mode
         self.encoding = encoding
         self.comm = comm
         if delay:
-            #We don't open the stream, but we still need to call the
-            #Handler constructor to set level, formatter, lock etc.
+            # We don't open the stream, but we still need to call the
+            # Handler constructor to set level, formatter, lock etc.
             logging.Handler.__init__(self)
             self.stream = None
         else:
-           logging.StreamHandler.__init__(self, self._open())
+            logging.StreamHandler.__init__(self, self._open())
 
     def _open(self):
-        stream = MPI.File.Open( self.comm, self.baseFilename, self.mode )
+        stream = MPI.File.Open(self.comm, self.baseFilename, self.mode)
         stream.Set_atomicity(True)
         return stream
 
     def emit(self, record):
         """
         Emit a record.
         If a formatter is specified, it is used to format the record.
@@ -507,206 +716,409 @@
             than `write` method. And `Write_shared` method only accept
             bytestring, so `encode` is used. `Write_shared` should be invoked
             only once in each all of this emit function to keep atomicity.
         """
         try:
             msg = self.format(record)
             stream = self.stream
-            stream.Write_shared((msg+self.terminator).encode(self.encoding))
-            #self.flush()
+            stream.Write_shared((msg + self.terminator).encode(self.encoding))
+            # self.flush()
         except Exception:
             self.handleError(record)
 
     def close(self):
         if self.stream:
             self.stream.Sync()
             self.stream.Close()
             self.stream = None
 
+
 def ParseRunOptions(filename):
     run_opts = {}
     for line in open(filename).readlines():
         line = line.strip()
         if line.startswith("#!"):
-            m3 = re.search(r"#!(.*)=(.*)",line)
+            m3 = re.search(r"#!(.*)=(.*)", line)
             if m3:
                 keyword = m3.group(1).strip()
-                value   = m3.group(2).strip().replace('"','')
+                value = m3.group(2).strip().replace('"', "")
                 run_opts[keyword] = value
     return run_opts
 
+
 parser = argparse.ArgumentParser(description=__doc__)
-parser.add_argument('--model_root', dest="model_root", metavar='root', type=str, nargs=1, default=["./"],
-                    help='root at which to search for models')
-parser.add_argument('--config', dest="config", metavar='config', type=str, nargs=1,
-                    help='path to configuration file to use')
-parser.add_argument('--models', dest="models", metavar='m', type=str, nargs='+',default=[],
-                    help='specify which models to run, list model names with no quotes and only separated by a space.')
-parser.add_argument('--model_year', dest="model_year", metavar='y0 yf', type=int, nargs='+',default=[],
-                    help='set to shift model years, "--model_year y0 yf" will shift years from y0 to yf')
-parser.add_argument('--study_limits', dest="study_limits", metavar='y0 yf', type=int, nargs='+',default=[],
-                    help='set study period, "--study_limits y0 yf" will limit run from y0 thru yf')
-parser.add_argument('--confrontations', dest="confront", metavar='c', type=str, nargs='+',default=[],
-                    help='specify which confrontations to run, list confrontation names with no quotes and only separated by a space.')
-parser.add_argument('--regions', dest="regions", metavar='r', type=str, nargs='+',default=['global'],
-                    help='specify which regions to compute over')
-parser.add_argument('--clean', dest="clean", action="store_true",
-                    help='enable to remove analysis files and recompute')
-parser.add_argument('--disable_logging', dest="logging", action="store_false",
-                    help='disables logging')
-parser.add_argument('-q','--quiet', dest="quiet", action="store_true",
-                    help='enable to silence screen output')
-parser.add_argument('--filter', dest="filter", metavar='filter', type=str, nargs=1, default=[""],
-                    help='a string which much be in the model filenames')
-parser.add_argument('--regex', dest="regex", metavar='regex', type=str, nargs=1, default=[""],
-                    help='a regular expression which filenames must conform to in order to be included')
-parser.add_argument('--build_dir', dest="build_dir", metavar='build_dir', type=str, nargs=1,default=["./_build"],
-                    help='path of where to save the output')
-parser.add_argument('--define_regions', dest="define_regions", type=str, nargs='+',default=[],
-                    help='list files containing user-defined regions')
-parser.add_argument('--model_setup', dest="model_setup", type=str, nargs='+',default=None,
-                    help='list files model setup information')
-parser.add_argument('--skip_plots', dest="skip_plots", action="store_true",
-                    help='enable to skip the plotting phase')
-parser.add_argument('--rel_only', dest="rel_only", action="store_true",
-                    help='enable only display relative differences in overall scores')
-parser.add_argument('--mem_per_pair', dest="mem_per_pair", metavar='MEM', type=float, default=100000.,
-                    help='maximum memory for IOMB model-confrontation pairs')
-parser.add_argument('--title', dest="run_title", metavar='title', type=str, nargs=1,
-                    help='title of the study to use in the HTML output')
-parser.add_argument('--rmse_score_basis', dest="rmse_score_basis", metavar='basis', type=str, default="cycle",
-                    help='base the RMSE score on the full time series with "series" or just the annual cycle with "cycle"')
+parser.add_argument(
+    "--model_root",
+    dest="model_root",
+    metavar="root",
+    type=str,
+    nargs=1,
+    default=["./"],
+    help="root at which to search for models",
+)
+parser.add_argument(
+    "--config",
+    dest="config",
+    metavar="config",
+    type=str,
+    nargs=1,
+    help="path to configuration file to use",
+)
+parser.add_argument(
+    "--models",
+    dest="models",
+    metavar="m",
+    type=str,
+    nargs="+",
+    default=[],
+    help="specify which models to run, list model names with no quotes and only separated by a space.",
+)
+parser.add_argument(
+    "--model_year",
+    dest="model_year",
+    metavar="y0 yf",
+    type=int,
+    nargs="+",
+    default=[],
+    help='set to shift model years, "--model_year y0 yf" will shift years from y0 to yf',
+)
+parser.add_argument(
+    "--study_limits",
+    dest="study_limits",
+    metavar="y0 yf",
+    type=int,
+    nargs="+",
+    default=[],
+    help='set study period, "--study_limits y0 yf" will limit run from y0 thru yf',
+)
+parser.add_argument(
+    "--confrontations",
+    dest="confront",
+    metavar="c",
+    type=str,
+    nargs="+",
+    default=[],
+    help="specify which confrontations to run, list confrontation names with no quotes and only separated by a space.",
+)
+parser.add_argument(
+    "--regions",
+    dest="regions",
+    metavar="r",
+    type=str,
+    nargs="+",
+    default=["global"],
+    help="specify which regions to compute over",
+)
+parser.add_argument(
+    "--clean",
+    dest="clean",
+    action="store_true",
+    help="enable to remove analysis files and recompute",
+)
+parser.add_argument(
+    "--disable_logging", dest="logging", action="store_false", help="disables logging"
+)
+parser.add_argument(
+    "-q",
+    "--quiet",
+    dest="quiet",
+    action="store_true",
+    help="enable to silence screen output",
+)
+parser.add_argument(
+    "--filter",
+    dest="filter",
+    metavar="filter",
+    type=str,
+    nargs=1,
+    default=[""],
+    help="a string which much be in the model filenames",
+)
+parser.add_argument(
+    "--regex",
+    dest="regex",
+    metavar="regex",
+    type=str,
+    nargs=1,
+    default=[""],
+    help="a regular expression which filenames must conform to in order to be included",
+)
+parser.add_argument(
+    "--build_dir",
+    dest="build_dir",
+    metavar="build_dir",
+    type=str,
+    nargs=1,
+    default=["./_build"],
+    help="path of where to save the output",
+)
+parser.add_argument(
+    "--define_regions",
+    dest="define_regions",
+    type=str,
+    nargs="+",
+    default=[],
+    help="list files containing user-defined regions",
+)
+parser.add_argument(
+    "--model_setup",
+    dest="model_setup",
+    type=str,
+    nargs="+",
+    default=None,
+    help="list files model setup information",
+)
+parser.add_argument(
+    "--skip_plots",
+    dest="skip_plots",
+    action="store_true",
+    help="enable to skip the plotting phase",
+)
+parser.add_argument(
+    "--rel_only",
+    dest="rel_only",
+    action="store_true",
+    help="enable only display relative differences in overall scores",
+)
+parser.add_argument(
+    "--mem_per_pair",
+    dest="mem_per_pair",
+    metavar="MEM",
+    type=float,
+    default=100000.0,
+    help="maximum memory for IOMB model-confrontation pairs",
+)
+parser.add_argument(
+    "--title",
+    dest="run_title",
+    metavar="title",
+    type=str,
+    nargs=1,
+    help="title of the study to use in the HTML output",
+)
+parser.add_argument(
+    "--rmse_score_basis",
+    dest="rmse_score_basis",
+    metavar="basis",
+    type=str,
+    default="cycle",
+    help='base the RMSE score on the full time series with "series" or just the annual cycle with "cycle"',
+)
+parser.add_argument(
+    "--df_errs",
+    dest="df_errs",
+    metavar="df_errs",
+    type=str,
+    default=None,
+    help="the pandas dataframe with the quantiles to use in scoring.",
+)
+parser.add_argument(
+    "-g",
+    "--global_region",
+    dest="global_region",
+    type=str,
+    default=None,
+    help="the ILAMB region to be the default (global) analysis region.",
+)
 args = parser.parse_args()
 if args.config is None:
     if rank == 0:
-        print("\nError: You must specify a configuration file using the option --config\n")
+        print(
+            "\nError: You must specify a configuration file using the option --config\n"
+        )
     comm.Barrier()
     comm.Abort(1)
 
 # Additional options could be in the configure file
 run_opts = ParseRunOptions(args.config[0])
 for key in run_opts:
-    if key in ['define_regions']:
-        define_regions = [os.path.join(os.environ['ILAMB_ROOT'],r) for r in run_opts[key].split(",")]
+    if key in ["define_regions"]:
+        define_regions = [
+            os.path.join(os.environ["ILAMB_ROOT"], r) for r in run_opts[key].split(",")
+        ]
         args.__dict__[key] += define_regions
-assert args.rmse_score_basis in ['series','cycle']
+assert args.rmse_score_basis in ["series", "cycle"]
 
 # Setup regions
 r = Regions()
 InitializeRegions(args.define_regions)
 missing = []
 for region in args.regions:
-    if region not in r.regions: missing.append(region)
+    if region not in r.regions:
+        missing.append(region)
 if len(missing) > 0:
-    raise ValueError("Unable to find the following regions %s from the following list of possible regions %s" % (missing,r.regions))
+    raise ValueError(
+        "Unable to find the following regions %s from the following list of possible regions %s"
+        % (missing, r.regions)
+    )
+if args.global_region:
+    r.setGlobalRegion(args.global_region)
 
 # Setup study
 T0 = time.time()
-if rank==0:
-    if not os.path.isdir(args.build_dir[0]): os.makedirs(args.build_dir[0])
+if rank == 0:
+    if not os.path.isdir(args.build_dir[0]):
+        os.makedirs(args.build_dir[0])
 if args.model_setup is None:
-    M = InitializeModels(args.model_root[0],
-                         args.models,
-                         not args.quiet,
-                         filter=args.filter[0],
-                         regex=args.regex[0],
-                         model_year=args.model_year,
-                         models_path=args.build_dir[0])
+    M = InitializeModels(
+        args.model_root[0],
+        args.models,
+        not args.quiet,
+        filter=args.filter[0],
+        regex=args.regex[0],
+        model_year=args.model_year,
+        models_path=args.build_dir[0],
+    )
 else:
-    M = ParseModelSetup(args.model_setup[0],args.models,not args.quiet,filter=args.filter[0],models_path=args.build_dir[0])
-if rank == 0 and not args.quiet: print("\nParsing config file %s...\n" % args.config[0])
-S = Scoreboard(args.config[0],
-               regions   = args.regions,
-               master    = rank==0,
-               verbose   = not args.quiet,
-               build_dir = args.build_dir[0],
-               extents   = RestrictiveModelExtents(M),
-               rel_only  = args.rel_only,
-               mem_per_pair = args.mem_per_pair,
-               run_title = args.run_title,
-               rmse_score_basis = args.rmse_score_basis)
-C  = MatchRelationshipConfrontation(S.list())
+    M = ParseModelSetup(
+        args.model_setup[0],
+        args.models,
+        not args.quiet,
+        filter=args.filter[0],
+        models_path=args.build_dir[0],
+    )
+
+
+try:
+    df_errs = None
+    if args.df_errs is not None:
+        df_errs = pd.read_parquet(args.df_errs)
+except:
+    if rank == 0:
+        print("Unable to read quantiles")
+    comm.Abort(0)
+
+
+if rank == 0 and not args.quiet:
+    print("\nParsing config file %s...\n" % args.config[0])
+S = Scoreboard(
+    args.config[0],
+    regions=args.regions,
+    master=rank == 0,
+    verbose=not args.quiet,
+    build_dir=args.build_dir[0],
+    extents=RestrictiveModelExtents(M),
+    rel_only=args.rel_only,
+    mem_per_pair=args.mem_per_pair,
+    run_title=args.run_title,
+    rmse_score_basis=args.rmse_score_basis,
+    df_errs=df_errs,
+)
+C = MatchRelationshipConfrontation(S.list())
 if len(args.study_limits) == 2:
     args.study_limits[1] += 1
-    for c in C: c.study_limits = (np.asarray(args.study_limits)-1850)*365.
-Cf = FilterConfrontationList(C,args.confront)
-if rank == 0: os.system("cp %s %s" % (args.config[0],os.path.join(args.build_dir[0],"ilamb.cfg")))
+    for c in C:
+        c.study_limits = (np.asarray(args.study_limits) - 1850) * 365.0
+Cf = FilterConfrontationList(C, args.confront)
+if rank == 0:
+    os.system(
+        "cp %s %s" % (args.config[0], os.path.join(args.build_dir[0], "ilamb.cfg"))
+    )
 
 # Setup logging
-logger    = logging.getLogger("%i" % comm.rank)
-logname   = ""
-formatter = logging.Formatter('[%(levelname)s][%(name)s][%(funcName)s]%(message)s')
+logger = logging.getLogger("%i" % comm.rank)
+logname = ""
+formatter = logging.Formatter("[%(levelname)s][%(name)s][%(funcName)s]%(message)s")
 logger.setLevel(logging.DEBUG)
 if args.logging:
-    logname = '%s/ILAMB%02d.log' % (S.build_dir,len(glob.glob("%s/*.log" % S.build_dir))+1)
+    logname = "%s/ILAMB%02d.log" % (
+        S.build_dir,
+        len(glob.glob("%s/*.log" % S.build_dir)) + 1,
+    )
     mh = MPIFileHandler(logname)
     mh.setFormatter(formatter)
     logger.addHandler(mh)
 
 if rank == 0:
-    logger.info(" " + " ".join(os.uname()))
-    for key in ["ILAMB","numpy","matplotlib","netCDF4","cf_units","sympy","mpi4py"]:
-        pkg  = __import__(key)
+    logger.info(" " + " ".join(platform.uname()))
+    for key in [
+        "ILAMB",
+        "numpy",
+        "matplotlib",
+        "netCDF4",
+        "cf_units",
+        "sympy",
+        "mpi4py",
+    ]:
+        pkg = __import__(key)
         try:
             path = pkg.__path__[0]
         except:
             path = key
-        logger.info(" %s (%s)" % (path,pkg.__version__))
+        logger.info(" %s (%s)" % (path, pkg.__version__))
     logger.info(" %s" % datetime.datetime.now())
 
 if rank == 0 and not args.quiet and len(Cf) != len(C):
     print("\nWe filtered some confrontations, actually running...\n")
-    for c in Cf: print(("    {0:>45}").format(c.longname))
+    for c in Cf:
+        print(("    {0:>45}").format(c.longname))
 C = Cf
 
-sys.stdout.flush(); comm.Barrier()
+sys.stdout.flush()
+comm.Barrier()
 
-if rank==0 and not args.quiet: print("\nRunning model-confrontation pairs...\n")
+if rank == 0 and not args.quiet:
+    print("\nRunning model-confrontation pairs...\n")
 
-sys.stdout.flush(); comm.Barrier()
+sys.stdout.flush()
+comm.Barrier()
 
-W = BuildLocalWorkList(M,C,skip_cache=True)
-WorkConfront(W,not args.quiet,args.clean)
+W = BuildLocalWorkList(M, C, skip_cache=True)
+WorkConfront(W, not args.quiet, args.clean)
 
-sys.stdout.flush(); comm.Barrier()
+sys.stdout.flush()
+comm.Barrier()
 
 if not args.skip_plots:
-    
-    if rank==0 and not args.quiet: print("\nFinishing post-processing which requires collectives...\n")
+    if rank == 0 and not args.quiet:
+        print("\nFinishing post-processing which requires collectives...\n")
 
-    sys.stdout.flush(); comm.Barrier()
-    
-    W = BuildLocalWorkList(M,C,skip_cache=False)
-    WorkPost(M,C,W,S,not args.quiet)
+    sys.stdout.flush()
+    comm.Barrier()
 
-if rank==0:
+    W = BuildLocalWorkList(M, C, skip_cache=False)
+    WorkPost(M, C, W, S, not args.quiet)
+
+if rank == 0:
     S.createHtml(M)
     S.createUDDashboard()
 
-sys.stdout.flush(); comm.Barrier()
+sys.stdout.flush()
+comm.Barrier()
 
 # Runtime information
 proc_reduced = np.zeros(proc.shape)
 ierr_reduced = np.zeros(ierr.shape)
-comm.Reduce(proc,proc_reduced,root=0)
-comm.Reduce(ierr,ierr_reduced,root=0)
-if size > 1: logger.info("[process time] %.1f s" % proc[rank])
-if rank==0:
-    logger.info("[total time] %.1f s" % (time.time()-T0))
+comm.Reduce(proc, proc_reduced, root=0)
+comm.Reduce(ierr, ierr_reduced, root=0)
+if size > 1:
+    logger.info("[process time] %.1f s" % proc[rank])
+if rank == 0:
+    logger.info("[total time] %.1f s" % (time.time() - T0))
     if size > 1:
         if proc_reduced.min() > 1e-6:
-            logger.info("[process balance] %.2f" % (proc_reduced.max()/proc_reduced.min()))
+            logger.info(
+                "[process balance] %.2f" % (proc_reduced.max() / proc_reduced.min())
+            )
         else:
             logger.info("[process balance] nan")
-        logger.info("[parallel efficiency] %.0f%%" % (100.*proc_reduced.sum()/float(size)/(time.time()-T0)))
+        logger.info(
+            "[parallel efficiency] %.0f%%"
+            % (100.0 * proc_reduced.sum() / float(size) / (time.time() - T0))
+        )
 
-if rank==0:
-    S.dumpScores(M,"scores.csv")
+if rank == 0:
+    S.dumpScores(M, "scores.csv")
     S.harvestInformation(M)
-    
-if (rank == 0              and
-    ierr_reduced.max() > 0 and
-    args.logging           and
-    not args.quiet):
-    print("\nErrors occurred in the run, please consult %s for more detailed information" % logname)
 
-if rank==0 and not args.quiet: print("\nCompleted in {0:>8}\n".format(str(datetime.timedelta(seconds=int(np.round((time.time()-T0)))))))
+if rank == 0 and ierr_reduced.max() > 0 and args.logging and not args.quiet:
+    print(
+        "\nErrors occurred in the run, please consult %s for more detailed information"
+        % logname
+    )
+
+if rank == 0 and not args.quiet:
+    print(
+        "\nCompleted in {0:>8}\n".format(
+            str(datetime.timedelta(seconds=int(np.round((time.time() - T0)))))
+        )
+    )
```

### Comparing `ILAMB-2.6/bin/ilamb-setup` & `ILAMB-2.7/bin/ilamb-setup`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python
 """Cartopy needs to download assets when it plots. If you first launch
 ilamb-run in parallel the server blocks your downloads and then
-plotting will fail. 
+plotting will fail.
 """
-import matplotlib.pyplot as plt
+import os
+
 import cartopy.crs as ccrs
 import cartopy.feature as cfeature
-import os
-fig,ax = plt.subplots(subplot_kw={'projection':ccrs.Robinson()})
-ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m'))
-ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m'))
+import matplotlib.pyplot as plt
+
+fig, ax = plt.subplots(subplot_kw={"projection": ccrs.Robinson()})
+ax.add_feature(cfeature.NaturalEarthFeature("physical", "land", "110m"))
+ax.add_feature(cfeature.NaturalEarthFeature("physical", "ocean", "110m"))
 fig.savefig("junk.png")
 os.system("rm junk.png")
```

### Comparing `ILAMB-2.6/setup.py` & `ILAMB-2.7/setup.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,112 +1,129 @@
 #!/usr/bin/env python
-from setuptools import setup
-from codecs import open
-import subprocess
 import os
+import subprocess
+from codecs import open
+
+from setuptools import setup
+
+VERSION = "2.7"
 
-VERSION    = '2.6'
 
 def git_version():
     """
     Return the sha1 of local git HEAD as a string.
     """
+
     def _minimal_ext_cmd(cmd):
         # construct minimal environment
         env = {}
-        for k in ['SYSTEMROOT', 'PATH', 'PYTHONPATH']:
+        for k in ["SYSTEMROOT", "PATH", "PYTHONPATH"]:
             v = os.environ.get(k)
             if v is not None:
                 env[k] = v
         # LANGUAGE is used on win32
-        env['LANGUAGE'] = 'C'
-        env['LANG'] = 'C'
-        env['LC_ALL'] = 'C'
-        out = subprocess.Popen(
-            cmd,
-            stdout=subprocess.PIPE,
-            env=env
-        ).communicate()[0]
+        env["LANGUAGE"] = "C"
+        env["LANG"] = "C"
+        env["LC_ALL"] = "C"
+        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]
         return out
+
     try:
-        out = _minimal_ext_cmd(['git', 'rev-parse', 'HEAD'])
-        git_revision = out.strip().decode('ascii')
+        out = _minimal_ext_cmd(["git", "rev-parse", "HEAD"])
+        git_revision = out.strip().decode("ascii")
     except OSError:
         git_revision = "unknown-git"
-    return git_revision
+    return git_revision[:7]
+
 
 def write_text(filename, text):
     try:
-        with open(filename, 'w') as a:
+        with open(filename, "w") as a:
             a.write(text)
     except Exception as e:
         print(e)
-        
-def write_version_py(filename=os.path.join('src/ILAMB', 'generated_version.py')):
+
+
+def write_version_py(filename=os.path.join("src/ILAMB", "generated_version.py")):
     cnt = """
 # THIS FILE IS GENERATED FROM ILAMB SETUP.PY
 short_version = '%(version)s'
 version = '%(version)s'
 git_revision = '%(git_revision)s'
 full_version = '%(version)s (%%(git_revision)s)' %% {
     'git_revision': git_revision}
 release = %(isrelease)s
 if not release:
     version = full_version
 """
     FULL_VERSION = VERSION
-    if os.path.isdir('.git'):
+    if os.path.isdir(".git"):
         GIT_REVISION = git_version()
-        ISRELEASED   = False
+        ISRELEASED = False
     else:
         GIT_REVISION = "RELEASE"
-        ISRELEASED   = True
+        ISRELEASED = True
 
-    FULL_VERSION += '.dev-' + GIT_REVISION
-    text = cnt % {'version': VERSION,
-                  'full_version': FULL_VERSION,
-                  'git_revision': GIT_REVISION,
-                  'isrelease': str(ISRELEASED)}
+    FULL_VERSION += ".dev-" + GIT_REVISION
+    text = cnt % {
+        "version": VERSION,
+        "full_version": FULL_VERSION,
+        "git_revision": GIT_REVISION,
+        "isrelease": str(ISRELEASED),
+    }
     write_text(filename, text)
 
-    
+
 here = os.path.abspath(os.path.dirname(__file__))
-with open(os.path.join(here, 'README.rst'), encoding='utf-8') as f:
+with open(os.path.join(here, "README.md"), encoding="utf-8") as f:
     long_description = f.read()
 
-write_version_py()    
+write_version_py()
 setup(
-    name='ILAMB',
+    name="ILAMB",
     version=VERSION,
-    description='The International Land Model Benchmarking Package',
+    description="The International Land Model Benchmarking Package",
     long_description=long_description,
-    url='https://github.com/rubisco-sfa/ILAMB.git',
-    author='Nathan Collier',
-    author_email='nathaniel.collier@gmail.com',
-    #license='MIT',
+    url="https://github.com/rubisco-sfa/ILAMB.git",
+    author="Nathan Collier",
+    author_email="nathaniel.collier@gmail.com",
+    license="BSD-3-Clause",
     classifiers=[
-        'Development Status :: 5 - Production/Stable',
-        'Intended Audience :: Science/Research',
-        'Topic :: Scientific/Engineering',
-        #'License :: OSI Approved :: MIT License',
-        'Operating System :: MacOS',
-        'Operating System :: POSIX',
-        'Operating System :: POSIX :: Linux',
-        'Programming Language :: Python :: 3',
+        "Development Status :: 5 - Production/Stable",
+        "Intended Audience :: Science/Research",
+        "Topic :: Scientific/Engineering",
+        "License :: OSI Approved :: BSD License",
+        "Operating System :: MacOS",
+        "Operating System :: POSIX",
+        "Operating System :: POSIX :: Linux",
+        "Programming Language :: Python :: 3",
     ],
-    keywords=['benchmarking','earth system modeling','climate modeling','model intercomparison'],
-    packages=['ILAMB'],
-    package_dir={'' : 'src'},
-    package_data={'ILAMB' : ['data/*.cfg']},
-    scripts=['bin/ilamb-run','bin/ilamb-fetch','bin/ilamb-mean','bin/ilamb-doctor','bin/ilamb-table',
-             'bin/ilamb-setup'],
+    keywords=[
+        "benchmarking",
+        "earth system modeling",
+        "climate modeling",
+        "model intercomparison",
+    ],
+    packages=["ILAMB"],
+    package_dir={"": "src"},
+    package_data={"ILAMB": ["data/*.cfg", "data/*.parquet"]},
+    scripts=["bin/ilamb-run", "bin/ilamb-fetch", "bin/ilamb-mean", "bin/ilamb-setup"],
     zip_safe=False,
-    install_requires=['numpy>=1.11.0',
-                      'pandas>=1.0.0',
-                      'matplotlib>=2.2',
-                      'cartopy>=0.17.0',
-                      'netCDF4>=1.1.4',
-                      'cf_units>=2.0.0',
-                      'sympy>=0.7.6',
-                      'mpi4py>=1.3.1',
-                      'scipy>=0.9.0']
+    install_requires=[
+        "numpy>=1.11.0, != 1.24.3",
+        "pandas>=1.0.0",
+        "matplotlib>=2.2",
+        "cartopy>=0.17.0",
+        "netCDF4>=1.1.4",
+        "cf_units>=2.0.0",
+        "sympy>=0.7.6",
+        "mpi4py>=1.3.1",
+        "scipy>=0.9.0",
+        "cftime",
+        "tqdm",
+        "pyarrow",
+        "pyyaml",
+    ],
+    extras_require={
+        "watershed": ["contextily", "geopandas", "dataretrieval", "pynhd"],
+    },
 )
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfAlbedo.py` & `ILAMB-2.7/src/ILAMB/ConfAlbedo.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,177 +1,222 @@
-from .Confrontation import Confrontation
-from .Variable import Variable
-from netCDF4 import Dataset
-from . import ilamblib as il
-import numpy as np
+import logging
 import os
+
+import numpy as np
 from mpi4py import MPI
 
-import logging
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Variable import Variable
+
 logger = logging.getLogger("%i" % MPI.COMM_WORLD.rank)
 
-def _albedo(dn,up,vname,energy_threshold):
-    mask    = (dn.data < energy_threshold)
-    dn.data = np.ma.masked_array(dn.data,mask=mask)
-    up.data = np.ma.masked_array(up.data,mask=mask)
-    np.seterr(over='ignore',under='ignore')
-    al      = np.ma.masked_array(up.data/dn.data,mask=mask)
-    np.seterr(over='warn',under='warn')
-    al      = Variable(name      = vname,
-                       unit      = "1",
-                       data      = al,
-                       lat       = dn.lat,
-                       lat_bnds  = dn.lat_bnds,
-                       lon       = dn.lon,
-                       lon_bnds  = dn.lon_bnds,
-                       time      = dn.time,
-                       time_bnds = dn.time_bnds)
-    return dn,up,al
 
-class ConfAlbedo(Confrontation):
+def _albedo(dn, up, vname, energy_threshold):
+    mask = dn.data < energy_threshold
+    dn.data = np.ma.masked_array(dn.data, mask=mask)
+    up.data = np.ma.masked_array(up.data, mask=mask)
+    np.seterr(over="ignore", under="ignore")
+    al = np.ma.masked_array(up.data / dn.data, mask=mask)
+    np.seterr(over="warn", under="warn")
+    al = Variable(
+        name=vname,
+        unit="1",
+        data=al,
+        lat=dn.lat,
+        lat_bnds=dn.lat_bnds,
+        lon=dn.lon,
+        lon_bnds=dn.lon_bnds,
+        time=dn.time,
+        time_bnds=dn.time_bnds,
+    )
+    return dn, up, al
+
 
-    def __init__(self,**keywords):
-        super(ConfAlbedo,self).__init__(**keywords)
+class ConfAlbedo(Confrontation):
+    def __init__(self, **keywords):
+        super(ConfAlbedo, self).__init__(**keywords)
         self.derived = "rsus / rsds"
-        
-    def stageData(self,m):
 
-        energy_threshold = float(self.keywords.get("energy_threshold",10))
+    def stageData(self, m):
+        energy_threshold = float(self.keywords.get("energy_threshold", 10))
 
         # Handle obs data
-        dn_obs = Variable(filename      = self.source.replace("albedo","rsds"),
-                          variable_name = "rsds",
-                          t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                          tf = None if len(self.study_limits) != 2 else self.study_limits[1])
-        up_obs = Variable(filename      = self.source.replace("albedo","rsus"),
-                          variable_name = "rsus",
-                          t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                          tf = None if len(self.study_limits) != 2 else self.study_limits[1])
-        dn_obs,up_obs,obs = _albedo(dn_obs,up_obs,self.variable,energy_threshold)
+        dn_obs = Variable(
+            filename=self.source.replace("albedo", "rsds"),
+            variable_name="rsds",
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        )
+        up_obs = Variable(
+            filename=self.source.replace("albedo", "rsus"),
+            variable_name="rsus",
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        )
+        dn_obs, up_obs, obs = _albedo(dn_obs, up_obs, self.variable, energy_threshold)
 
         # Prune out uncovered regions
-        if obs.time is None: raise il.NotTemporalVariable()
+        if obs.time is None:
+            raise il.NotTemporalVariable()
         self.pruneRegions(obs)
 
         # Handle model data
-        dn_mod = m.extractTimeSeries("rsds",
-                                     alt_vars = ["FSDS"],
-                                     initial_time = obs.time_bnds[ 0,0],
-                                     final_time   = obs.time_bnds[-1,1],
-                                     lats         = None if obs.spatial else obs.lat,
-                                     lons         = None if obs.spatial else obs.lon)
-        up_mod = m.extractTimeSeries("rsus",
-                                     alt_vars = ["FSR"],
-                                     initial_time = obs.time_bnds[ 0,0],
-                                     final_time   = obs.time_bnds[-1,1],
-                                     lats         = None if obs.spatial else obs.lat,
-                                     lons         = None if obs.spatial else obs.lon)
-        dn_mod,up_mod,mod = _albedo(dn_mod,up_mod,self.variable,energy_threshold)
+        dn_mod = m.extractTimeSeries(
+            "rsds",
+            alt_vars=["FSDS"],
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+            lats=None if obs.spatial else obs.lat,
+            lons=None if obs.spatial else obs.lon,
+        )
+        up_mod = m.extractTimeSeries(
+            "rsus",
+            alt_vars=["FSR"],
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+            lats=None if obs.spatial else obs.lat,
+            lons=None if obs.spatial else obs.lon,
+        )
+        dn_mod, up_mod, mod = _albedo(dn_mod, up_mod, self.variable, energy_threshold)
 
         # Make variables comparable
-        obs,mod = il.MakeComparable(obs,mod,
-                                    mask_ref  = True,
-                                    clip_ref  = True,
-                                    logstring = "[%s][%s]" % (self.longname,m.name))
-        dn_obs,dn_mod = il.MakeComparable(dn_obs,dn_mod,
-                                          mask_ref  = True,
-                                          clip_ref  = True,
-                                          logstring = "[%s][%s]" % (self.longname,m.name))
-        up_obs,up_mod = il.MakeComparable(up_obs,up_mod,
-                                          mask_ref  = True,
-                                          clip_ref  = True,
-                                          logstring = "[%s][%s]" % (self.longname,m.name))
+        obs, mod = il.MakeComparable(
+            obs,
+            mod,
+            mask_ref=True,
+            clip_ref=True,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
+        dn_obs, dn_mod = il.MakeComparable(
+            dn_obs,
+            dn_mod,
+            mask_ref=True,
+            clip_ref=True,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
+        up_obs, up_mod = il.MakeComparable(
+            up_obs,
+            up_mod,
+            mask_ref=True,
+            clip_ref=True,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
 
         # Compute the mean albedo
         dn_obs = dn_obs.integrateInTime(mean=True)
         up_obs = up_obs.integrateInTime(mean=True)
-        np.seterr(over='ignore',under='ignore')
-        obs_timeint = np.ma.masked_array(up_obs.data/dn_obs.data,mask=(dn_obs.data.mask+up_obs.data.mask))
-        np.seterr(over='warn',under='warn')
-        obs_timeint = Variable(name      = self.variable,
-                               unit      = "1",
-                               data      = obs_timeint,
-                               lat       = dn_obs.lat,
-                               lat_bnds  = dn_obs.lat_bnds,
-                               lon       = dn_obs.lon,
-                               lon_bnds  = dn_obs.lon_bnds)
+        np.seterr(over="ignore", under="ignore")
+        obs_timeint = np.ma.masked_array(
+            up_obs.data / dn_obs.data, mask=(dn_obs.data.mask + up_obs.data.mask)
+        )
+        np.seterr(over="warn", under="warn")
+        obs_timeint = Variable(
+            name=self.variable,
+            unit="1",
+            data=obs_timeint,
+            lat=dn_obs.lat,
+            lat_bnds=dn_obs.lat_bnds,
+            lon=dn_obs.lon,
+            lon_bnds=dn_obs.lon_bnds,
+        )
         dn_mod = dn_mod.integrateInTime(mean=True)
         up_mod = up_mod.integrateInTime(mean=True)
-        np.seterr(over='ignore',under='ignore')
-        mod_timeint = np.ma.masked_array(up_mod.data/dn_mod.data,mask=(dn_mod.data.mask+up_mod.data.mask))
-        np.seterr(over='warn',under='warn')
-        mod_timeint = Variable(name      = self.variable,
-                               unit      = "1",
-                               data      = mod_timeint,
-                               lat       = dn_mod.lat,
-                               lat_bnds  = dn_mod.lat_bnds,
-                               lon       = dn_mod.lon,
-                               lon_bnds  = dn_mod.lon_bnds)
+        np.seterr(over="ignore", under="ignore")
+        mod_timeint = np.ma.masked_array(
+            up_mod.data / dn_mod.data, mask=(dn_mod.data.mask + up_mod.data.mask)
+        )
+        np.seterr(over="warn", under="warn")
+        mod_timeint = Variable(
+            name=self.variable,
+            unit="1",
+            data=mod_timeint,
+            lat=dn_mod.lat,
+            lat_bnds=dn_mod.lat_bnds,
+            lon=dn_mod.lon,
+            lon_bnds=dn_mod.lon_bnds,
+        )
 
-        return obs,mod,obs_timeint,mod_timeint
+        return obs, mod, obs_timeint, mod_timeint
 
     def requires(self):
-        return ['rsus','rsds'],[]
+        return ["rsus", "rsds"], []
 
-    def confront(self,m):
+    def confront(self, m):
         r"""Confronts the input model with the observational data.
 
         This routine is exactly the same as Confrontation except that
         user-provided period means are passed as options to the analysis.
 
         Parameters
         ----------
         m : ILAMB.ModelResult.ModelResult
             the model results
 
         """
         # Grab the data
-        obs,mod,obs_timeint,mod_timeint = self.stageData(m)
-
-        mod_file = os.path.join(self.output_path,"%s_%s.nc"        % (self.name,m.name))
-        obs_file = os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name,      ))
-        with il.FileContextManager(self.master,mod_file,obs_file) as fcm:
+        obs, mod, obs_timeint, mod_timeint = self.stageData(m)
 
+        mod_file = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
+        obs_file = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name,))
+        with il.FileContextManager(self.master, mod_file, obs_file) as fcm:
             # Encode some names and colors
-            fcm.mod_dset.setncatts({"name" :m.name,
-                                    "color":m.color,
-                                    "weight":self.cweight,
-                                    "complete":0})
+            fcm.mod_dset.setncatts(
+                {
+                    "name": m.name,
+                    "color": m.color,
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
             if self.master:
-                fcm.obs_dset.setncatts({"name" :"Benchmark",
-                                        "color":np.asarray([0.5,0.5,0.5]),
-                                        "weight":self.cweight,
-                                        "complete":0})
+                fcm.obs_dset.setncatts(
+                    {
+                        "name": "Benchmark",
+                        "color": np.asarray([0.5, 0.5, 0.5]),
+                        "weight": self.cweight,
+                        "complete": 0,
+                    }
+                )
 
             # Read in some options and run the mean state analysis
-            mass_weighting = self.keywords.get("mass_weighting",False)
-            skip_rmse      = self.keywords.get("skip_rmse"     ,False)
-            skip_iav       = self.keywords.get("skip_iav"      ,True )
-            skip_cycle     = self.keywords.get("skip_cycle"    ,False)
+            mass_weighting = self.keywords.get("mass_weighting", False)
+            skip_rmse = self.keywords.get("skip_rmse", False)
+            skip_iav = self.keywords.get("skip_iav", True)
+            skip_cycle = self.keywords.get("skip_cycle", False)
             if obs.spatial:
-                il.AnalysisMeanStateSpace(obs,mod,dataset   = fcm.mod_dset,
-                                          regions           = self.regions,
-                                          benchmark_dataset = fcm.obs_dset,
-                                          table_unit        = self.table_unit,
-                                          plot_unit         = self.plot_unit,
-                                          space_mean        = self.space_mean,
-                                          skip_rmse         = skip_rmse,
-                                          skip_iav          = skip_iav,
-                                          skip_cycle        = skip_cycle,
-                                          mass_weighting    = mass_weighting,
-                                          ref_timeint       = obs_timeint,
-                                          com_timeint       = mod_timeint)
+                il.AnalysisMeanStateSpace(
+                    obs,
+                    mod,
+                    dataset=fcm.mod_dset,
+                    regions=self.regions,
+                    benchmark_dataset=fcm.obs_dset,
+                    table_unit=self.table_unit,
+                    plot_unit=self.plot_unit,
+                    space_mean=self.space_mean,
+                    skip_rmse=skip_rmse,
+                    skip_iav=skip_iav,
+                    skip_cycle=skip_cycle,
+                    mass_weighting=mass_weighting,
+                    ref_timeint=obs_timeint,
+                    com_timeint=mod_timeint,
+                )
             else:
-                il.AnalysisMeanStateSites(obs,mod,dataset   = fcm.mod_dset,
-                                          regions           = self.regions,
-                                          benchmark_dataset = fcm.obs_dset,
-                                          table_unit        = self.table_unit,
-                                          plot_unit         = self.plot_unit,
-                                          space_mean        = self.space_mean,
-                                          skip_rmse         = skip_rmse,
-                                          skip_iav          = skip_iav,
-                                          skip_cycle        = skip_cycle,
-                                          mass_weighting    = mass_weighting)
-            fcm.mod_dset.setncattr("complete",1)
-            if self.master: fcm.obs_dset.setncattr("complete",1)
+                il.AnalysisMeanStateSites(
+                    obs,
+                    mod,
+                    dataset=fcm.mod_dset,
+                    regions=self.regions,
+                    benchmark_dataset=fcm.obs_dset,
+                    table_unit=self.table_unit,
+                    plot_unit=self.plot_unit,
+                    space_mean=self.space_mean,
+                    skip_rmse=skip_rmse,
+                    skip_iav=skip_iav,
+                    skip_cycle=skip_cycle,
+                    mass_weighting=mass_weighting,
+                )
+            fcm.mod_dset.setncattr("complete", 1)
+            if self.master:
+                fcm.obs_dset.setncattr("complete", 1)
 
-        logger.info("[%s][%s] Success" % (self.longname,m.name))
+        logger.info("[%s][%s] Success" % (self.longname, m.name))
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfBurntArea.py` & `ILAMB-2.7/src/ILAMB/ConfBurntArea.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,36 +1,47 @@
-from .Confrontation import Confrontation
-from .Variable import Variable
-from . import ilamblib as il
 import numpy as np
 
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Variable import Variable
+
+
 class ConfBurntArea(Confrontation):
     """
     Burnt area is sometimes expressed as a total per month, and
     sometimes as the mean rate in the month. This specialized code
     detects the difference and converts.
     """
-    def stageData(self,m):
-        obs = Variable(filename       = self.source,
-                       variable_name  = self.variable,
-                       alternate_vars = self.alternate_vars,
-                       t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                       tf = None if len(self.study_limits) != 2 else self.study_limits[1])
-        mod = m.extractTimeSeries(self.variable,
-                                  alt_vars     = self.alternate_vars,
-                                  expression   = self.derived,
-                                  initial_time = obs.time_bnds[ 0,0],
-                                  final_time   = obs.time_bnds[-1,1])
+
+    def stageData(self, m):
+        obs = Variable(
+            filename=self.source,
+            variable_name=self.variable,
+            alternate_vars=self.alternate_vars,
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        )
+        mod = m.extractTimeSeries(
+            self.variable,
+            alt_vars=self.alternate_vars,
+            expression=self.derived,
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+        )
         try:
             mod.convert(obs.unit)
         except:
             mod.convert("d-1")
-            dt = mod.time_bnds[:,1]-mod.time_bnds[:,0]
-            for i in range(mod.data.ndim-1): dt = np.expand_dims(dt,axis=-1)
+            dt = mod.time_bnds[:, 1] - mod.time_bnds[:, 0]
+            for i in range(mod.data.ndim - 1):
+                dt = np.expand_dims(dt, axis=-1)
             mod.data *= dt
-            mod.unit  = "1"
-        obs,mod = il.MakeComparable(obs,mod,
-                                    mask_ref  = True,
-                                    clip_ref  = True,
-                                    extents   = self.extents,
-                                    logstring = "[%s][%s]" % (self.longname,m.name))
-        return obs,mod
+            mod.unit = "1"
+        obs, mod = il.MakeComparable(
+            obs,
+            mod,
+            mask_ref=True,
+            clip_ref=True,
+            extents=self.extents,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
+        return obs, mod
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfCO2.py` & `ILAMB-2.7/src/ILAMB/ConfCO2.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,648 +1,748 @@
-from .Confrontation import Confrontation,create_data_header
-from scipy.interpolate import CubicSpline
+import copy
+import os
+
 import cartopy.crs as ccrs
 import cartopy.feature as cfeature
-from .Variable import Variable
-from .Regions import Regions
-from .constants import mid_months,lbl_months,bnd_months
-from . import Post as post
-from . import ilamblib as il
-from netCDF4 import Dataset
-import pylab as plt
 import numpy as np
-import os
-import copy
-from . import ccgfilt
+import pylab as plt
+from netCDF4 import Dataset
+from scipy.interpolate import CubicSpline
 
+from ILAMB import Post as post
+from ILAMB import ccgfilt
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation, create_data_header
+from ILAMB.constants import bnd_months, lbl_months, mid_months
+from ILAMB.Regions import Regions
+from ILAMB.Variable import Variable
 
 
-def _phaseWellDefined(t,v):
+def _phaseWellDefined(t, v):
     """The phase of a site is considered well defined if:
 
     * there is at least 2 years of contiguous data in the time series
     * if the frequency corresponding to the peak in the power spectrum
     is within the sampling frequency of 1/365.
 
     """
-    b,e = 0,0
+    b, e = 0, 0
     for s in np.ma.flatnotmasked_contiguous(v):
-        if (s.stop-s.start) > e-b:
+        if (s.stop - s.start) > e - b:
             b = s.start
             e = s.stop
-    if (e-b) < 24: return False
-    T  = t     [b:e]
-    V  = v.data[b:e]
-    P  = np.abs(np.fft.fft(V))**2
+    if (e - b) < 24:
+        return False
+    T = t[b:e]
+    V = v.data[b:e]
+    P = np.abs(np.fft.fft(V)) ** 2
     dt = np.diff(T).mean()
-    F  = np.fft.fftfreq(V.size,dt)
-    P  = P[np.where(F>=0)]
-    F  = F[np.where(F>=0)]
-    i  = np.argsort(F)
-    P  = P[i]; F = F[i]
+    F = np.fft.fftfreq(V.size, dt)
+    P = P[np.where(F >= 0)]
+    F = F[np.where(F >= 0)]
+    i = np.argsort(F)
+    P = P[i]
+    F = F[i]
     dF = np.diff(F).mean()
-    f0 = 1./365
-    f  = F[P.argmax()]
-    return (f > (f0-0.9*dF))*(f < (f0+0.9*dF))
+    f0 = 1.0 / 365
+    f = F[P.argmax()]
+    return (f > (f0 - 0.9 * dF)) * (f < (f0 + 0.9 * dF))
+
 
 def _meanDay(d):
-    """Computes the average Julian day by the angle of the resultant vector.
-    """
-    x  = (np.cos(d/365.*2*np.pi)).mean()
-    y  = (np.sin(d/365.*2*np.pi)).mean()
-    a  = np.arctan(y/x)/(2.*np.pi)*365.
-    a += (x<=0)*365*0.5
-    a += (a<=0)*365
+    """Computes the average Julian day by the angle of the resultant vector."""
+    x = (np.cos(d / 365.0 * 2 * np.pi)).mean()
+    y = (np.sin(d / 365.0 * 2 * np.pi)).mean()
+    a = np.arctan(y / x) / (2.0 * np.pi) * 365.0
+    a += (x <= 0) * 365 * 0.5
+    a += (a <= 0) * 365
     return a
 
-def _cycleShape(var,period=365.):
-    """Reshape the variable data for computing a cycle on the specified period.
-    """
-    dt    = (var.time_bnds[:,1]-var.time_bnds[:,0]).mean()
-    spd   = int(round(period/dt))
-    begin = np.argmin(var.time[:spd]%period)
-    end   = begin+int(var.time[begin:].size/float(spd))*spd
-    shp   = (-1,spd) + var.data.shape[1:]
+
+def _cycleShape(var, period=365.0):
+    """Reshape the variable data for computing a cycle on the specified period."""
+    dt = (var.time_bnds[:, 1] - var.time_bnds[:, 0]).mean()
+    spd = int(round(period / dt))
+    begin = np.argmin(var.time[:spd] % period)
+    end = begin + int(var.time[begin:].size / float(spd)) * spd
+    shp = (-1, spd) + var.data.shape[1:]
     cycle = var.data[begin:end].reshape(shp)
-    tbnd  = var.time_bnds[begin:end,:].reshape((-1,spd,2)) % period
-    tbnd  = tbnd[0,...]
-    tbnd[-1,1] = period
-    t     = tbnd.mean(axis=1)
-    return cycle,t,tbnd
+    tbnd = var.time_bnds[begin:end, :].reshape((-1, spd, 2)) % period
+    tbnd = tbnd[0, ...]
+    tbnd[-1, 1] = period
+    t = tbnd.mean(axis=1)
+    return cycle, t, tbnd
 
-def _siteCharacteristics(t,v):
-    """Compute the mean amplitude, cycle, and time of maximum and minimum.
-    """
-    with np.errstate(under='ignore'):
-        amp   = (v.max (axis=1)-v.min(axis=1)).mean()
-        cyc   =  v.mean(axis=0)
-        fun   = CubicSpline(np.hstack([t  ,t  [0]+365.]),
-                            np.hstack([cyc,cyc[0]     ]),
-                            bc_type="periodic")
+
+def _siteCharacteristics(t, v):
+    """Compute the mean amplitude, cycle, and time of maximum and minimum."""
+    with np.errstate(under="ignore"):
+        amp = (v.max(axis=1) - v.min(axis=1)).mean()
+        cyc = v.mean(axis=0)
+        fun = CubicSpline(
+            np.hstack([t, t[0] + 365.0]), np.hstack([cyc, cyc[0]]), bc_type="periodic"
+        )
         troot = fun.derivative().solve()
-        troot = troot[(troot>=0)*(troot<=365.)]
-        tmax  = troot[fun(troot).argmax()]
-        tmin  = troot[fun(troot).argmin()]
-        vs    = fun(np.linspace(0,365,366))
-    return amp,tmax,tmin,vs
+        troot = troot[(troot >= 0) * (troot <= 365.0)]
+        tmax = troot[fun(troot).argmax()]
+        tmin = troot[fun(troot).argmin()]
+        vs = fun(np.linspace(0, 365, 366))
+    return amp, tmax, tmin, vs
+
 
 def _detrend(var):
-    """Detrend the variable by subtracting the best fit quadratic polynomial.
-    """
+    """Detrend the variable by subtracting the best fit quadratic polynomial."""
     for i in range(var.ndata):
-        j = np.where(var.data[:,i].mask==False)
+        j = np.where(var.data[:, i].mask == False)
         t = var.time[j]
-        x = var.data[j,i][0]
-        p = np.polyfit(t,x,2)
-        var.data.data[:,i] -= np.polyval(p,var.time)
+        x = var.data[j, i][0]
+        p = np.polyfit(t, x, 2)
+        var.data.data[:, i] -= np.polyval(p, var.time)
     return var
 
-def _computeShift(x,y):
-    """Given the timing of two variables, compute a shift score
-    """
-    shift  = np.abs(x.data-y.data) # how many days off are we
-    shift  = (shift>0.5*365)*(365-shift) + (shift<=0.5*365)*shift
-    shift /= (0.5*365)
-    shift  = 1.0-shift
+
+def _computeShift(x, y):
+    """Given the timing of two variables, compute a shift score"""
+    shift = np.abs(x.data - y.data)  # how many days off are we
+    shift = (shift > 0.5 * 365) * (365 - shift) + (shift <= 0.5 * 365) * shift
+    shift /= 0.5 * 365
+    shift = 1.0 - shift
     return shift
 
+
 class ConfCO2(Confrontation):
-    """
-    """
-    def __init__(self,**keywords):
+    """ """
 
+    def __init__(self, **keywords):
         # Ugly, but this is how we call the Confrontation constructor
-        super(ConfCO2,self).__init__(**keywords)
-        self.regions = ['global']
-        self.derived = self.keywords.get("emulated_flux","nbp")
-
-        self.lat_bands = np.asarray(self.keywords.get("lat_bands","-90,-60,-23,0,+23,+60,+90").split(","),dtype=float)
+        super(ConfCO2, self).__init__(**keywords)
+        self.regions = ["global"]
+        self.derived = self.keywords.get("emulated_flux", "nbp")
+
+        self.lat_bands = np.asarray(
+            self.keywords.get("lat_bands", "-90,-60,-23,0,+23,+60,+90").split(","),
+            dtype=float,
+        )
         sec = []
-        for i in range(len(self.lat_bands)-1):
-            sec.append("Latitude Band %d to %d [ppm]" % (self.lat_bands[i],self.lat_bands[i+1]))
+        for i in range(len(self.lat_bands) - 1):
+            sec.append(
+                "Latitude Band %d to %d [ppm]"
+                % (self.lat_bands[i], self.lat_bands[i + 1])
+            )
         sec = sec[::-1]
 
         # Setup a html layout for generating web views of the results
         pages = []
 
         # Mean State page
-        pages.append(post.HtmlPage("MeanState","Mean State"))
+        pages.append(post.HtmlPage("MeanState", "Mean State"))
         pages[-1].setHeader("CNAME / RNAME / MNAME")
-        pages[-1].setSections(["Summary",] + sec)
-        pages.append(post.HtmlAllModelsPage("AllModels","All Models"))
+        pages[-1].setSections(
+            [
+                "Summary",
+            ]
+            + sec
+        )
+        pages.append(post.HtmlAllModelsPage("AllModels", "All Models"))
         pages[-1].setHeader("CNAME / RNAME")
         pages[-1].setSections([])
         pages[-1].setRegions(self.regions)
-        pages.append(post.HtmlPage("DataInformation","Data Information"))
+        pages.append(post.HtmlPage("DataInformation", "Data Information"))
         pages[-1].setSections([])
         pages[-1].text = "\n"
 
         def _attribute_sort(attr):
             # If the attribute begins with one of the ones we
             # specifically order, return the index into order. If
             # it does not, return the number of entries in the
             # list and the file's order will be preserved.
-            order = ['title','version','institution','source','history','references','comments','convention']
-            for i,a in enumerate(order):
-                if attr.lower().startswith(a): return i
+            order = [
+                "title",
+                "version",
+                "institution",
+                "source",
+                "history",
+                "references",
+                "comments",
+                "convention",
+            ]
+            for i, a in enumerate(order):
+                if attr.lower().startswith(a):
+                    return i
             return len(order)
+
         with Dataset(self.source) as dset:
             attrs = dset.ncattrs()
-            attrs = sorted(attrs,key=_attribute_sort)
+            attrs = sorted(attrs, key=_attribute_sort)
             for attr in attrs:
                 try:
                     val = dset.getncattr(attr)
-                    if type(val) != str: val = str(val)
-                    pages[-1].text += create_data_header(attr,val)
+                    if type(val) != str:
+                        val = str(val)
+                    pages[-1].text += create_data_header(attr, val)
                 except:
                     pass
 
-        self.layout = post.HtmlLayout(pages,self.longname)
+        self.layout = post.HtmlLayout(pages, self.longname)
 
         # Adding a member variable called basins, add them as regions
         r = Regions()
-        self.pulse_dir = "/".join(self.source.split("/")[:-2]+["PulseEmulation"])
-        self.pulse_regions = r.addRegionNetCDF4(os.path.join(self.pulse_dir,"AtmosphericPulseRegions.nc"))
+        self.pulse_dir = "/".join(self.source.split("/")[:-2] + ["PulseEmulation"])
+        self.pulse_regions = r.addRegionNetCDF4(
+            os.path.join(self.pulse_dir, "AtmosphericPulseRegions.nc")
+        )
 
         # Emulation specific initialization
-        self.sites = [site.strip() for site in self.keywords.get("sites",None).upper().split(",")]
-        self.map   = None
+        self.sites = [
+            site.strip() for site in self.keywords.get("sites", None).upper().split(",")
+        ]
+        self.map = None
         if self.sites:
-            self.map  = [self.lbls.index(site) for site in self.sites if site in self.lbls]
+            self.map = [
+                self.lbls.index(site) for site in self.sites if site in self.lbls
+            ]
             self.lbls = [self.lbls[i] for i in self.map]
 
-    def emulatedModelResult(self,m,obs):
-
+    def emulatedModelResult(self, m, obs):
         # Emulation parameters
-        emulated_flux = self.keywords.get("emulated_flux","nbp")
+        emulated_flux = self.keywords.get("emulated_flux", "nbp")
         spinup = 12
-        Ninf   = 60
-        ilev   = 1
+        Ninf = 60
+        ilev = 1
 
         # Get the model result
-        mod = m.extractTimeSeries(emulated_flux,
-                                  initial_time = obs.time_bnds[ 0,0]-float(Ninf)/12*365+29.,
-                                  final_time   = obs.time_bnds[-1,1])
-
+        mod = m.extractTimeSeries(
+            emulated_flux,
+            initial_time=obs.time_bnds[0, 0] - float(Ninf) / 12 * 365 + 29.0,
+            final_time=obs.time_bnds[-1, 1],
+        )
 
         # What if I don't have Ninf leadtime?
-        tf = min(obs.time_bnds[-1,1],mod.time_bnds[-1,1])
-        if (tf % 365 > 2): tf -= (tf % 365) # needs to end in integer years
-        obs.trim(t=[-1e20,tf])
-        mod.trim(t=[-1e20,tf])
+        tf = min(obs.time_bnds[-1, 1], mod.time_bnds[-1, 1])
+        if tf % 365 > 2:
+            tf -= tf % 365  # needs to end in integer years
+        obs.trim(t=[-1e20, tf])
+        mod.trim(t=[-1e20, tf])
 
         # Integrate the emulated flux over each pulse region
         region_int = {}
-        for region in self.pulse_regions: region_int[region] = mod.integrateInSpace(region=region).convert("Pg yr-1")
+        for region in self.pulse_regions:
+            region_int[region] = mod.integrateInSpace(region=region).convert("Pg yr-1")
 
         # Load the operator from the files
-        lat,lon,H = None,None,None
+        lat, lon, H = None, None, None
         for i in range(12):
             # FIX: move pulses into one file to avoid requiring a naming convention
-            with Dataset(os.path.join(self.pulse_dir,"Pulse%02d.nc" % (i+1))) as dset:
-                if lat is None: lat = dset.variables["lat"][...]
-                if lon is None: lon = dset.variables["lon"][...]
-                if H   is None: H   = np.zeros((22,12,Ninf+12,lat.size,lon.size))
+            with Dataset(
+                os.path.join(self.pulse_dir, "Pulse%02d.nc" % (i + 1))
+            ) as dset:
+                if lat is None:
+                    lat = dset.variables["lat"][...]
+                if lon is None:
+                    lon = dset.variables["lon"][...]
+                if H is None:
+                    H = np.zeros((22, 12, Ninf + 12, lat.size, lon.size))
                 for j in range(22):
-                    T = dset.variables['T%d' % (j+1)]
-                    H[j,i,...] = T[spinup:,ilev,...]-T[:spinup,...].mean()
+                    T = dset.variables["T%d" % (j + 1)]
+                    H[j, i, ...] = T[spinup:, ilev, ...] - T[:spinup, ...].mean()
 
         # Where are our sites?
-        ilat = np.abs(lat[:,np.newaxis]-obs.lat).argmin(axis=0)
-        ilon = np.abs(lon[:,np.newaxis]-obs.lon).argmin(axis=0)
+        ilat = np.abs(lat[:, np.newaxis] - obs.lat).argmin(axis=0)
+        ilon = np.abs(lon[:, np.newaxis] - obs.lon).argmin(axis=0)
 
         # Apply the operator
-        Nyrs  = int(mod.time.size/12)
-        Ntot  = 12*Nyrs + Ninf
-        eflux = np.zeros((obs.ndata,22,Ntot))
+        Nyrs = int(mod.time.size / 12)
+        Ntot = 12 * Nyrs + Ninf
+        eflux = np.zeros((obs.ndata, 22, Ntot))
         for j in range(20):
             for s in range(obs.ndata):
-                Htemp = H[j,...,ilat[s],ilon[s]]
-                Htrac = np.zeros((22,Ntot,12*Nyrs))
+                Htemp = H[j, ..., ilat[s], ilon[s]]
+                Htrac = np.zeros((22, Ntot, 12 * Nyrs))
                 for i in range(Nyrs):
-                    pb = 12*i
-                    pe = 12*(i+1)
+                    pb = 12 * i
+                    pe = 12 * (i + 1)
                     re = pe + Ninf
-                    Htrac[j,pb:re,pb:pe] = Htemp.T
-                    Htrac[j,re:  ,pb:pe] = np.tile(Htemp[:,-1],[12*(Nyrs-i-1),1])
-                eflux[s,j,:] = np.dot(Htrac[j,...],region_int["pulse_region_%d" % (j+1)].data)*(-1e-3) # H is [] ?
+                    Htrac[j, pb:re, pb:pe] = Htemp.T
+                    Htrac[j, re:, pb:pe] = np.tile(
+                        Htemp[:, -1], [12 * (Nyrs - i - 1), 1]
+                    )
+                eflux[s, j, :] = np.dot(
+                    Htrac[j, ...], region_int["pulse_region_%d" % (j + 1)].data
+                ) * (
+                    -1e-3
+                )  # H is [] ?
 
         eflux = eflux.sum(axis=1).T
         eflux = eflux[Ninf:-Ninf]
-        eflux = np.ma.masked_array(eflux,mask=obs.data.mask)
-        mod = Variable(name      = "co2",
-                       unit      = obs.unit,
-                       lat       = obs.lat,
-                       lon       = obs.lon,
-                       ndata     = obs.ndata,
-                       time      = obs.time,
-                       time_bnds = obs.time_bnds,
-                       data      = eflux)
+        eflux = np.ma.masked_array(eflux, mask=obs.data.mask)
+        mod = Variable(
+            name="co2",
+            unit=obs.unit,
+            lat=obs.lat,
+            lon=obs.lon,
+            ndata=obs.ndata,
+            time=obs.time,
+            time_bnds=obs.time_bnds,
+            data=eflux,
+        )
         return mod
 
-    def stageData(self,m):
-
+    def stageData(self, m):
         # Get the observational data
-        obs = Variable(filename       = self.source,
-                       variable_name  = self.variable,
-                       alternate_vars = self.alternate_vars,
-                       t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                       tf = None if len(self.study_limits) != 2 else self.study_limits[1])
+        obs = Variable(
+            filename=self.source,
+            variable_name=self.variable,
+            alternate_vars=self.alternate_vars,
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        )
 
         # Reduce the sites
         if self.map:
-            obs.lat   = obs.lat  [  self.map]
-            obs.lon   = obs.lon  [  self.map]
-            obs.depth = obs.depth[  self.map]
-            obs.data  = obs.data [:,self.map]
+            obs.lat = obs.lat[self.map]
+            obs.lon = obs.lon[self.map]
+            obs.depth = obs.depth[self.map]
+            obs.data = obs.data[:, self.map]
             obs.ndata = len(self.map)
 
         # Get the model result
-        force_emulation = self.keywords.get("force_emulation","False").lower() == "true"
-        never_emulation = self.keywords.get("never_emulation","False").lower() == "true"
-        no_co2          = False
-        emulated_co2    = False
-        mod             = None
-
+        force_emulation = (
+            self.keywords.get("force_emulation", "False").lower() == "true"
+        )
+        never_emulation = (
+            self.keywords.get("never_emulation", "False").lower() == "true"
+        )
+        no_co2 = False
+        emulated_co2 = False
+        mod = None
 
         if not force_emulation:
             try:
-                mod = m.extractTimeSeries(self.variable,
-                                          alt_vars     = self.alternate_vars,
-                                          initial_time = obs.time_bnds[ 0,0],
-                                          final_time   = obs.time_bnds[-1,1],
-                                          lats         = None if obs.spatial else obs.lat,
-                                          lons         = None if obs.spatial else obs.lon)
-
+                mod = m.extractTimeSeries(
+                    self.variable,
+                    alt_vars=self.alternate_vars,
+                    initial_time=obs.time_bnds[0, 0],
+                    final_time=obs.time_bnds[-1, 1],
+                    lats=None if obs.spatial else obs.lat,
+                    lons=None if obs.spatial else obs.lon,
+                )
 
             except il.VarNotInModel:
                 no_co2 = True
 
-        if (((mod is None) or no_co2) and (not never_emulation)):
-            mod = self.emulatedModelResult(m,obs)
+        if ((mod is None) or no_co2) and (not never_emulation):
+            mod = self.emulatedModelResult(m, obs)
             emulated_co2 = True
 
-        if mod is None: raise il.VarNotInModel()
+        if mod is None:
+            raise il.VarNotInModel()
 
         # Get the right layering, closest to the layer elevation where all aren't masked.
         if mod.layered:
-            ind = (np.abs(obs.depth[:,np.newaxis]-mod.depth)).argmin(axis=1)
+            ind = (np.abs(obs.depth[:, np.newaxis] - mod.depth)).argmin(axis=1)
             for i in range(ind.size):
-                while (mod.data[:,ind[i],i].mask.sum() > 0.5*mod.data.shape[0]):
+                while mod.data[:, ind[i], i].mask.sum() > 0.5 * mod.data.shape[0]:
                     ind[i] += 1
             data = []
             for i in range(ind.size):
-                data.append(mod.data[:,ind[i],i])
+                data.append(mod.data[:, ind[i], i])
             mod.data = np.ma.masked_array(data).T
             mod.depth = None
             mod.depth_bnds = None
             mod.layered = False
 
-            obs,mod = il.MakeComparable(obs,mod,
-                                        mask_ref  = True,
-                                        clip_ref  = True)
+            obs, mod = il.MakeComparable(obs, mod, mask_ref=True, clip_ref=True)
             mod.data.mask += obs.data.mask
 
-
-
         # if emulated_co2 is true, subtract mod by TakahashiFFco2 and FFco2
         if emulated_co2:
-           #Read in Fosil fuel CO2 concentration from GEOSChem output
-           filename = os.path.join(self.pulse_dir,"GEOSChemOcnFfCo2_32yr_360daytime.nc")
-
-           FFco2Emu = Variable(filename = filename, variable_name = "FFco2" )
-           FFco2Emu = FFco2Emu.extractDatasites(lat = None if obs.spatial else obs.lat,
-                                                lon = None if obs.spatial else obs.lon )
-           OCNco2Emu = Variable(filename = filename, variable_name = "OCNco2" )
-           OCNco2Emu = OCNco2Emu.extractDatasites(lat = None if obs.spatial else obs.lat,
-                                                  lon = None if obs.spatial else obs.lon)
-
-           # Get the right layering, closest to the layer elevation where all aren't masked
-           if OCNco2Emu.layered:
-              ind = (np.abs(obs.depth[:,np.newaxis]-OCNco2Emu.depth)).argmin(axis=1)
-              for i in range(ind.size):
-                  while (OCNco2Emu.data[:,ind[i],i].mask.sum() > 0.5*OCNco2Emu.data.shape[0]):
-                      ind[i] += 1
-
-              data = []
-              dataFF = []
-              for i in range(ind.size):
-                  data.append(OCNco2Emu.data[:,ind[i],i])
-                  dataFF.append(FFco2Emu.data[:,ind[i],i])
-
-              OCNco2Emu.data = np.ma.masked_array(data).T
-              OCNco2Emu.depth = None
-              OCNco2Emu.depth_bnds = None
-              OCNco2Emu.layered = False
-              OCNco2Emu.unit = "mol mol-1"
-
-              FFco2Emu.data = np.ma.masked_array(dataFF).T
-              FFco2Emu.depth = None
-              FFco2Emu.depth_bnds = None
-              FFco2Emu.layered = False
-              FFco2Emu.unit = "mol mol-1"
-
-
-           # actual processing substract OCNco2 and FFco2 from mod CO2
-           obs, OCNco2Emu = il.MakeComparable(obs, OCNco2Emu,
-                                              mask_ref = True,
-                                              clip_ref = True)
-
-           obs, FFco2Emu = il.MakeComparable(obs, FFco2Emu,
-                                             mask_ref = True,
-                                             clip_ref = True)
-
-           #trim data in time domain
-           tmin = max(OCNco2Emu.time_bnds[0,0],obs.time_bnds[0,0])
-           tmax = min(OCNco2Emu.time_bnds[-1,1],obs.time_bnds[-1,1])
-
-           if tmax >= tmin:
-               OCNco2Emu.trim(t=[tmin, tmax])
-               FFco2Emu.trim(t=[tmin, tmax])
-               obs.trim(t=[tmin, tmax])
-               obs.data = obs.data - OCNco2Emu.data - FFco2Emu.data
-               mod.trim(t=[tmin, tmax])
-
+            # Read in Fosil fuel CO2 concentration from GEOSChem output
+            filename = os.path.join(
+                self.pulse_dir, "GEOSChemOcnFfCo2_32yr_360daytime.nc"
+            )
+
+            FFco2Emu = Variable(filename=filename, variable_name="FFco2")
+            FFco2Emu = FFco2Emu.extractDatasites(
+                lat=None if obs.spatial else obs.lat,
+                lon=None if obs.spatial else obs.lon,
+            )
+            OCNco2Emu = Variable(filename=filename, variable_name="OCNco2")
+            OCNco2Emu = OCNco2Emu.extractDatasites(
+                lat=None if obs.spatial else obs.lat,
+                lon=None if obs.spatial else obs.lon,
+            )
+
+            # Get the right layering, closest to the layer elevation where all aren't masked
+            if OCNco2Emu.layered:
+                ind = (np.abs(obs.depth[:, np.newaxis] - OCNco2Emu.depth)).argmin(
+                    axis=1
+                )
+                for i in range(ind.size):
+                    while (
+                        OCNco2Emu.data[:, ind[i], i].mask.sum()
+                        > 0.5 * OCNco2Emu.data.shape[0]
+                    ):
+                        ind[i] += 1
+
+                data = []
+                dataFF = []
+                for i in range(ind.size):
+                    data.append(OCNco2Emu.data[:, ind[i], i])
+                    dataFF.append(FFco2Emu.data[:, ind[i], i])
+
+                OCNco2Emu.data = np.ma.masked_array(data).T
+                OCNco2Emu.depth = None
+                OCNco2Emu.depth_bnds = None
+                OCNco2Emu.layered = False
+                OCNco2Emu.unit = "mol mol-1"
+
+                FFco2Emu.data = np.ma.masked_array(dataFF).T
+                FFco2Emu.depth = None
+                FFco2Emu.depth_bnds = None
+                FFco2Emu.layered = False
+                FFco2Emu.unit = "mol mol-1"
+
+            # actual processing substract OCNco2 and FFco2 from mod CO2
+            obs, OCNco2Emu = il.MakeComparable(
+                obs, OCNco2Emu, mask_ref=True, clip_ref=True
+            )
+
+            obs, FFco2Emu = il.MakeComparable(
+                obs, FFco2Emu, mask_ref=True, clip_ref=True
+            )
+
+            # trim data in time domain
+            tmin = max(OCNco2Emu.time_bnds[0, 0], obs.time_bnds[0, 0])
+            tmax = min(OCNco2Emu.time_bnds[-1, 1], obs.time_bnds[-1, 1])
+
+            if tmax >= tmin:
+                OCNco2Emu.trim(t=[tmin, tmax])
+                FFco2Emu.trim(t=[tmin, tmax])
+                obs.trim(t=[tmin, tmax])
+                obs.data = obs.data - OCNco2Emu.data - FFco2Emu.data
+                mod.trim(t=[tmin, tmax])
 
         # Remove the trend via quadradic polynomial
         obs = _detrend(obs)
         mod = _detrend(mod)
 
+        return obs, mod
 
-        return obs,mod
-
-    def relationshipInd(self,m):
-        #Before plotting relationship between iav of co2 or co2 growth rate and iav of other variables, e.g. tas in this case, prepare the iav of independent variable at the same time.
+    def relationshipInd(self, m):
+        # Before plotting relationship between iav of co2 or co2 growth rate and iav of other variables, e.g. tas in this case, prepare the iav of independent variable at the same time.
 
-        #get obs of independent variable
-        indObs = Variable(filename       = os.path.join(self.pulse_dir, "/DATA/tas/CRU/tas_0.5x0.5.nc"),
-                          variable_name  = "tas",
-                          #alternate_vars = self.alternate_vars,
-                          t0 = None ,
-                          tf = None)
+        # get obs of independent variable
+        indObs = Variable(
+            filename=os.path.join(self.pulse_dir, "/DATA/tas/CRU/tas_0.5x0.5.nc"),
+            variable_name="tas",
+            # alternate_vars = self.alternate_vars,
+            t0=None,
+            tf=None,
+        )
 
-        #grab tropical area
+        # grab tropical area
         latTro = indObs.lat[(indObs.lat > -23) * (indObs.lat < 23)]
         maskLat = np.repeat(latTro, len(indObs.lon))
         maskLon = np.tile(indObs.lon, len(latTro))
-        indObs =  indObs.extractDatasites(lat = maskLat,
-                                          lon = maskLon)
+        indObs = indObs.extractDatasites(lat=maskLat, lon=maskLon)
 
-        #grab tropical land area
-        whrLand = np.where(indObs.data[1,:].mask==False)
+        # grab tropical land area
+        whrLand = np.where(indObs.data[1, :].mask == False)
         indObsLand = copy.deepcopy(indObs)
         indObsLand.data = indObs.data[:, whrLand]
 
-
-        #detrending for indObsLand
+        # detrending for indObsLand
         cc = copy.deepcopy(indObsLand)
-        cc.data = np.mean(indObsLand.data, axis=2, keepdims = False)
+        cc.data = np.mean(indObsLand.data, axis=2, keepdims=False)
         cc.ndata = 1
         var = copy.deepcopy(cc)
-        j = np.where(var.data.mask==False)[0]
+        j = np.where(var.data.mask == False)[0]
         t = var.time[j]
-        x = var.data[j,0]
-        p = np.polyfit(t,x,2)
-        bb = np.polyval(p,var.time)
-        var.data[j,0] -= bb
+        x = var.data[j, 0]
+        p = np.polyfit(t, x, 2)
+        bb = np.polyval(p, var.time)
+        var.data[j, 0] -= bb
 
         indObsLandDtr = copy.deepcopy(var)
 
+        # -------model output tas ---------------------
 
+        # get reference co2 obs to grab the time
+        co2Obs = Variable(
+            filename=self.source,
+            variable_name=self.variable,
+            alternate_vars=self.alternate_vars,
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        )
+
+        # get the model output of independent variable
+        indModLand = m.extractTimeSeries(
+            "tas",
+            # alt_vars     = self.alternate_vars,
+            initial_time=co2Obs.time_bnds[0, 0],
+            final_time=co2Obs.time_bnds[-1, 1],
+            lats=indObs.lat[whrLand],
+            lons=indObs.lon[whrLand],
+        )
 
-        #-------model output tas ---------------------
-
-        #get reference co2 obs to grab the time
-        co2Obs = Variable(filename       = self.source,
-                       variable_name  = self.variable,
-                       alternate_vars = self.alternate_vars,
-                       t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                       tf = None if len(self.study_limits) != 2 else self.study_limits[1])
-
-
-        #get the model output of independent variable
-        indModLand = m.extractTimeSeries("tas",
-                                         #alt_vars     = self.alternate_vars,
-                                         initial_time = co2Obs.time_bnds[ 0,0],
-                                         final_time   = co2Obs.time_bnds[-1,1],
-                                         lats         = indObs.lat[whrLand],
-                                         lons         = indObs.lon[whrLand])
-
-
-        #detrending for mod independent-----------
+        # detrending for mod independent-----------
         cc = copy.deepcopy(indModLand)
-        cc.data = np.mean(indModLand.data, axis=1, keepdims= True)
+        cc.data = np.mean(indModLand.data, axis=1, keepdims=True)
         cc.ndata = 1
         var = copy.deepcopy(cc)
-        j = np.where(var.data.mask==False)[0]
+        j = np.where(var.data.mask == False)[0]
         t = var.time[j]
-        x = var.data[j,0]
-        p = np.polyfit(t,x,2)
-        #var.data.data[:,i] -= np.polyval(p,var.time)
-        bb = np.polyval(p,var.time)
-        var.data[j,0] -= bb
+        x = var.data[j, 0]
+        p = np.polyfit(t, x, 2)
+        # var.data.data[:,i] -= np.polyval(p,var.time)
+        bb = np.polyval(p, var.time)
+        var.data[j, 0] -= bb
         indModLandDtr = copy.deepcopy(var)
 
-
-
-
         ###calcualte iav of ind obs and ind mod----------
 
         # Compute harmonics first
-        ocyc,ot,otb = _cycleShape(indObsLandDtr)
-        mcyc,mt,mtb = _cycleShape(indModLandDtr)
+        ocyc, ot, otb = _cycleShape(indObsLandDtr)
+        mcyc, mt, mtb = _cycleShape(indModLandDtr)
 
-        n           = 1
-        obs_amp     = np.zeros(n); obs_maxp = np.zeros(n); obs_minp = np.zeros(n)
-        mod_amp     = np.zeros(n); mod_maxp = np.zeros(n); mod_minp = np.zeros(n)
-        obs_cyc     = np.zeros((366,n)); mod_cyc = np.zeros((366,n));
+        n = 1
+        obs_amp = np.zeros(n)
+        obs_maxp = np.zeros(n)
+        obs_minp = np.zeros(n)
+        mod_amp = np.zeros(n)
+        mod_maxp = np.zeros(n)
+        mod_minp = np.zeros(n)
+        obs_cyc = np.zeros((366, n))
+        mod_cyc = np.zeros((366, n))
         well_define = np.zeros(n)
 
-
-        for i in range(1,n):
-            obs_amp[i],obs_maxp[i],obs_minp[i],obs_cyc[:,i] = _siteCharacteristics(ot,ocyc[...,i])
-            mod_amp[i],mod_maxp[i],mod_minp[i],mod_cyc[:,i] = _siteCharacteristics(mt,mcyc[...,i])
+        for i in range(1, n):
+            obs_amp[i], obs_maxp[i], obs_minp[i], obs_cyc[:, i] = _siteCharacteristics(
+                ot, ocyc[..., i]
+            )
+            mod_amp[i], mod_maxp[i], mod_minp[i], mod_cyc[:, i] = _siteCharacteristics(
+                mt, mcyc[..., i]
+            )
 
         obs = copy.deepcopy(indObsLandDtr)
         mod = copy.deepcopy(indModLandDtr)
 
-
-        with np.errstate(under='ignore'):
-            ocyc     = Variable(name  = "cycle", # mean annual cycle
-                                unit  = obs.unit,
-                                data  = ocyc.mean(axis=0),
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon,
-                                time  = ot,
-                                time_bnds = otb)
-
-            oiav     = Variable(name  = "iav", # deseasonalized interannual variability
-                                unit  = obs.unit,
-                                data  = obs.data-il.ExtendAnnualCycle(obs.time,ocyc.data,ocyc.time),
-                                time  = obs.time,
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon,
-                                time_bnds = obs.time_bnds)
-
+        with np.errstate(under="ignore"):
+            ocyc = Variable(
+                name="cycle",  # mean annual cycle
+                unit=obs.unit,
+                data=ocyc.mean(axis=0),
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+                time=ot,
+                time_bnds=otb,
+            )
+
+            oiav = Variable(
+                name="iav",  # deseasonalized interannual variability
+                unit=obs.unit,
+                data=obs.data - il.ExtendAnnualCycle(obs.time, ocyc.data, ocyc.time),
+                time=obs.time,
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+                time_bnds=obs.time_bnds,
+            )
 
         # Write out ILAMB variables for modeled quantities
-        mcyc     = Variable(name  = "cycle", # mean annual cycle
-                            unit  = mod.unit,
-                            data  = mcyc.mean(axis=0),
-                            ndata = mod.ndata,
-                            lat   = mod.lat,
-                            lon   = mod.lon,
-                            time  = mt,
-                            time_bnds = mtb)
-        miav     = Variable(name  = "iav", # deseasonalized interannual variability
-                            unit  = mod.unit,
-                            data  = mod.data-il.ExtendAnnualCycle(mod.time,mcyc.data,mcyc.time),
-                            time  = mod.time,
-                            ndata = mod.ndata,
-                            lat   = mod.lat,
-                            lon   = mod.lon,
-                            time_bnds = mod.time_bnds)
-
-
-
+        mcyc = Variable(
+            name="cycle",  # mean annual cycle
+            unit=mod.unit,
+            data=mcyc.mean(axis=0),
+            ndata=mod.ndata,
+            lat=mod.lat,
+            lon=mod.lon,
+            time=mt,
+            time_bnds=mtb,
+        )
+        miav = Variable(
+            name="iav",  # deseasonalized interannual variability
+            unit=mod.unit,
+            data=mod.data - il.ExtendAnnualCycle(mod.time, mcyc.data, mcyc.time),
+            time=mod.time,
+            ndata=mod.ndata,
+            lat=mod.lat,
+            lon=mod.lon,
+            time_bnds=mod.time_bnds,
+        )
 
         # Write out the intermediate variables
-        #write out independent variable, here is tas iav
+        # write out independent variable, here is tas iav
         flag_write = False
         if flag_write:
             if not os.path.exists((os.path.join(self.output_path, "tas/"))):
                 os.mkdir((os.path.join(self.output_path, "tas/")))
-            with Dataset(os.path.join(self.output_path,"tas/%s_%s.nc" % ("tas",m.name)),mode="w") as results:
-                results.setncatts({"name" :m.name, "color":m.color})
-                for v in [mod,mcyc,miav]:
-                    v.toNetCDF4(results,group="MeanState")
-
-            with Dataset(os.path.join(self.output_path,"tas/tas_CRU_Benchmark.nc"),mode="w") as results:
-                results.setncatts({"name" :"Benchmark", "color":np.asarray([0.5,0.5,0.5])})
-                for v in [obs,ocyc,oiav]:
-                    v.toNetCDF4(results,group="MeanState")
-
-        return oiav,miav
-
-
-    def confront(self,m):
+            with Dataset(
+                os.path.join(self.output_path, "tas/%s_%s.nc" % ("tas", m.name)),
+                mode="w",
+            ) as results:
+                results.setncatts({"name": m.name, "color": m.color})
+                for v in [mod, mcyc, miav]:
+                    v.toNetCDF4(results, group="MeanState")
+
+            with Dataset(
+                os.path.join(self.output_path, "tas/tas_CRU_Benchmark.nc"), mode="w"
+            ) as results:
+                results.setncatts(
+                    {"name": "Benchmark", "color": np.asarray([0.5, 0.5, 0.5])}
+                )
+                for v in [obs, ocyc, oiav]:
+                    v.toNetCDF4(results, group="MeanState")
 
+        return oiav, miav
 
+    def confront(self, m):
         # Grab the data
-        obs,mod= self.stageData(m)
+        obs, mod = self.stageData(m)
 
         # Compute amplitude, min and max phase, and annual cycle as numpy data arrays
-        ocyc,ot,otb = _cycleShape(obs)
-        mcyc,mt,mtb = _cycleShape(mod)
-        n           = len(self.lbls)
-        obs_amp     = np.zeros(n); obs_maxp = np.zeros(n); obs_minp = np.zeros(n)
-        mod_amp     = np.zeros(n); mod_maxp = np.zeros(n); mod_minp = np.zeros(n)
-        obs_cyc     = np.zeros((366,n)); mod_cyc = np.zeros((366,n));
+        ocyc, ot, otb = _cycleShape(obs)
+        mcyc, mt, mtb = _cycleShape(mod)
+        n = len(self.lbls)
+        obs_amp = np.zeros(n)
+        obs_maxp = np.zeros(n)
+        obs_minp = np.zeros(n)
+        mod_amp = np.zeros(n)
+        mod_maxp = np.zeros(n)
+        mod_minp = np.zeros(n)
+        obs_cyc = np.zeros((366, n))
+        mod_cyc = np.zeros((366, n))
         well_define = np.zeros(n)
-        for i,site in enumerate(self.lbls):
-            obs_amp[i],obs_maxp[i],obs_minp[i],obs_cyc[:,i] = _siteCharacteristics(ot,ocyc[...,i])
-            mod_amp[i],mod_maxp[i],mod_minp[i],mod_cyc[:,i] = _siteCharacteristics(mt,mcyc[...,i])
-            well_define[i] = _phaseWellDefined(obs.time,obs.data[:,i])
+        for i, site in enumerate(self.lbls):
+            obs_amp[i], obs_maxp[i], obs_minp[i], obs_cyc[:, i] = _siteCharacteristics(
+                ot, ocyc[..., i]
+            )
+            mod_amp[i], mod_maxp[i], mod_minp[i], mod_cyc[:, i] = _siteCharacteristics(
+                mt, mcyc[..., i]
+            )
+            well_define[i] = _phaseWellDefined(obs.time, obs.data[:, i])
         well_define /= well_define.sum()
 
         # Write out ILAMB variables for observed quantities
-        with np.errstate(under='ignore'):
-            ocyc     = Variable(name  = "cycle", # mean annual cycle
-                                unit  = obs.unit,
-                                data  = ocyc.mean(axis=0),
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon,
-                                time  = ot,
-                                time_bnds = otb)
-            oiav     = Variable(name  = "iav", # deseasonalized interannual variability
-                                unit  = obs.unit,
-                                data  = obs.data-il.ExtendAnnualCycle(obs.time,ocyc.data,ocyc.time),
-                                time  = obs.time,
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon,
-                                time_bnds = obs.time_bnds)
-
-
-            ocycf    = Variable(name  = "cycle_fine", # finely sampled cycle from cubic interpolation
-                                unit  = obs.unit,
-                                data  = obs_cyc,
-                                time  = np.linspace(0,365,366),
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon)
-            obs_amp  = Variable(name  = "amp", # mean amplitude over time period
-                                unit  = obs.unit,
-                                data  = obs_amp,
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon)
-            obs_maxp = Variable(name  = "maxp", # Julian day of the maximum of the annual cycle
-                                unit  = "d",
-                                data  = obs_maxp,
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon)
-            obs_minp = Variable(name  = "minp", # Julian day of the minimum of the annual cycle
-                                unit  = "d",
-                                data  = obs_minp,
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon)
+        with np.errstate(under="ignore"):
+            ocyc = Variable(
+                name="cycle",  # mean annual cycle
+                unit=obs.unit,
+                data=ocyc.mean(axis=0),
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+                time=ot,
+                time_bnds=otb,
+            )
+            oiav = Variable(
+                name="iav",  # deseasonalized interannual variability
+                unit=obs.unit,
+                data=obs.data - il.ExtendAnnualCycle(obs.time, ocyc.data, ocyc.time),
+                time=obs.time,
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+                time_bnds=obs.time_bnds,
+            )
+
+            ocycf = Variable(
+                name="cycle_fine",  # finely sampled cycle from cubic interpolation
+                unit=obs.unit,
+                data=obs_cyc,
+                time=np.linspace(0, 365, 366),
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+            )
+            obs_amp = Variable(
+                name="amp",  # mean amplitude over time period
+                unit=obs.unit,
+                data=obs_amp,
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+            )
+            obs_maxp = Variable(
+                name="maxp",  # Julian day of the maximum of the annual cycle
+                unit="d",
+                data=obs_maxp,
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+            )
+            obs_minp = Variable(
+                name="minp",  # Julian day of the minimum of the annual cycle
+                unit="d",
+                data=obs_minp,
+                ndata=obs.ndata,
+                lat=obs.lat,
+                lon=obs.lon,
+            )
 
             # Write out ILAMB variables for modeled quantities
-            mcyc     = Variable(name  = "cycle", # mean annual cycle
-                                unit  = mod.unit,
-                                data  = mcyc.mean(axis=0),
-                                ndata = mod.ndata,
-                                lat   = mod.lat,
-                                lon   = mod.lon,
-                                time  = mt,
-                                time_bnds = mtb)
-            miav     = Variable(name  = "iav", # deseasonalized interannual variability
-                                unit  = mod.unit,
-                                data  = mod.data-il.ExtendAnnualCycle(mod.time,mcyc.data,mcyc.time),
-                                time  = mod.time,
-                                ndata = mod.ndata,
-                                lat   = mod.lat,
-                                lon   = mod.lon,
-                                time_bnds = mod.time_bnds)
-
-            mcycf    = Variable(name  = "cycle_fine", # finely sampled cycle from cubic interpolation
-                                unit  = mod.unit,
-                                data  = mod_cyc,
-                                time  = np.linspace(0,365,366),
-                                ndata = mod.ndata,
-                                lat   = mod.lat,
-                                lon   = mod.lon)
-            mod_amp  = Variable(name  = "amp", # mean amplitude over time period
-                                unit  = mod.unit,
-                                data  = mod_amp,
-                                ndata = mod.ndata,
-                                lat   = mod.lat,
-                                lon   = mod.lon)
-            mod_maxp = Variable(name  = "maxp", # Julian day of the maximum of the annual cycle
-                                unit  = "d",
-                                data  = mod_maxp,
-                                ndata = mod.ndata,
-                                lat   = mod.lat,
-                                lon   = mod.lon)
-            mod_minp = Variable(name  = "minp", # Julian day of the minimum of the annual cycle
-                                unit  = "d",
-                                data  = mod_minp,
-                                ndata = mod.ndata,
-                                lat   = mod.lat,
-                                lon   = mod.lon)
-
+            mcyc = Variable(
+                name="cycle",  # mean annual cycle
+                unit=mod.unit,
+                data=mcyc.mean(axis=0),
+                ndata=mod.ndata,
+                lat=mod.lat,
+                lon=mod.lon,
+                time=mt,
+                time_bnds=mtb,
+            )
+            miav = Variable(
+                name="iav",  # deseasonalized interannual variability
+                unit=mod.unit,
+                data=mod.data - il.ExtendAnnualCycle(mod.time, mcyc.data, mcyc.time),
+                time=mod.time,
+                ndata=mod.ndata,
+                lat=mod.lat,
+                lon=mod.lon,
+                time_bnds=mod.time_bnds,
+            )
+
+            mcycf = Variable(
+                name="cycle_fine",  # finely sampled cycle from cubic interpolation
+                unit=mod.unit,
+                data=mod_cyc,
+                time=np.linspace(0, 365, 366),
+                ndata=mod.ndata,
+                lat=mod.lat,
+                lon=mod.lon,
+            )
+            mod_amp = Variable(
+                name="amp",  # mean amplitude over time period
+                unit=mod.unit,
+                data=mod_amp,
+                ndata=mod.ndata,
+                lat=mod.lat,
+                lon=mod.lon,
+            )
+            mod_maxp = Variable(
+                name="maxp",  # Julian day of the maximum of the annual cycle
+                unit="d",
+                data=mod_maxp,
+                ndata=mod.ndata,
+                lat=mod.lat,
+                lon=mod.lon,
+            )
+            mod_minp = Variable(
+                name="minp",  # Julian day of the minimum of the annual cycle
+                unit="d",
+                data=mod_minp,
+                ndata=mod.ndata,
+                lat=mod.lat,
+                lon=mod.lon,
+            )
 
-        #ccgcrv trend, longtime scale, seasonal (harmonics), smooth data for obs
-        #placeholder for ccgObsTrend, ccgObsPoly, ccgObsIav, ccgObsFun, ccgObsHarmo, ccgObsSmooth
+        # ccgcrv trend, longtime scale, seasonal (harmonics), smooth data for obs
+        # placeholder for ccgObsTrend, ccgObsPoly, ccgObsIav, ccgObsFun, ccgObsHarmo, ccgObsSmooth
         ccgObsTrend = copy.deepcopy(obs)
         ccgObsTrend.name = "ccgTrend"
 
         ccgObsPoly = copy.deepcopy(obs)
         ccgObsPoly.name = "ccgPoly"
 
         ccgObsIav = copy.deepcopy(obs)
@@ -653,60 +753,85 @@
 
         ccgObsHarmo = copy.deepcopy(obs)
         ccgObsHarmo.name = "ccgHarmo"
 
         ccgObsSmooth = copy.deepcopy(obs)
         ccgObsSmooth.name = "ccgSmooth"
 
-
-
         for ss in range(0, obs.ndata):
-            whrData = np.where(obs.data[:,ss].mask==False)
-            xp=(obs.time/365)[whrData]
-            yp=obs.data[:,ss][whrData]
+            whrData = np.where(obs.data[:, ss].mask == False)
+            xp = (obs.time / 365)[whrData]
+            yp = obs.data[:, ss][whrData]
 
-            #long term filter is 10 yrs
+            # long term filter is 10 yrs
             yrLongTerm = 10
-            filt = ccgfilt.ccgFilter(xp=xp,
-                                     yp=yp,
-                                     longterm = (yrLongTerm*365),
-                                     debug=False)
-
-            ccgObsTrend.data[whrData,ss] = filt.getTrendValue(xp)
-            ccgObsPoly.data[whrData,ss] = filt.getPolyValue(xp)
-            ccgObsFun.data[whrData,ss] = filt.getFunctionValue(xp)
-            ccgObsHarmo.data[whrData,ss] = filt.getHarmonicValue(xp)
-            ccgObsSmooth.data[whrData,ss] = filt.getSmoothValue(xp)
-            ccgObsIav.data[whrData,ss] = ccgObsSmooth.data[whrData,ss] - ccgObsTrend.data[whrData,ss] - ccgObsHarmo.data[whrData,ss]
+            filt = ccgfilt.ccgFilter(
+                xp=xp, yp=yp, longterm=(yrLongTerm * 365), debug=False
+            )
+
+            ccgObsTrend.data[whrData, ss] = filt.getTrendValue(xp)
+            ccgObsPoly.data[whrData, ss] = filt.getPolyValue(xp)
+            ccgObsFun.data[whrData, ss] = filt.getFunctionValue(xp)
+            ccgObsHarmo.data[whrData, ss] = filt.getHarmonicValue(xp)
+            ccgObsSmooth.data[whrData, ss] = filt.getSmoothValue(xp)
+            ccgObsIav.data[whrData, ss] = (
+                ccgObsSmooth.data[whrData, ss]
+                - ccgObsTrend.data[whrData, ss]
+                - ccgObsHarmo.data[whrData, ss]
+            )
             ccgCrossDates = filt.getTrendCrossingDates()
 
-            #write out crossingdates and BRW amplitude or not?
+            # write out crossingdates and BRW amplitude or not?
             flag_write = False
             if flag_write:
-                #write out Crossingdates
+                # write out Crossingdates
                 if not os.path.exists((os.path.join(self.output_path, "ccg/"))):
                     os.mkdir((os.path.join(self.output_path, "ccg/")))
 
-                if not os.path.exists((os.path.join(self.output_path, "ccg/CrossingDates/"))):
+                if not os.path.exists(
+                    (os.path.join(self.output_path, "ccg/CrossingDates/"))
+                ):
                     os.mkdir((os.path.join(self.output_path, "ccg/CrossingDates/")))
 
+                np.savetxt(
+                    os.path.join(
+                        self.output_path,
+                        "ccg/CrossingDates/ccgCrossDates_%s_benchmark_site_%s.csv"
+                        % (self.name, ss),
+                    ),
+                    ccgCrossDates,
+                    delimiter=",",
+                    fmt="%s",
+                )
 
-                np.savetxt(os.path.join(self.output_path,"ccg/CrossingDates/ccgCrossDates_%s_benchmark_site_%s.csv" % (self.name, ss)), ccgCrossDates, delimiter= ",", fmt= '%s')
-
-                #write out BRW amplitude:
+                # write out BRW amplitude:
                 if ss == 5:
                     ccgAmplitude = filt.getAmplitudes()
-                    if not os.path.exists((os.path.join(self.output_path, "ccg/seaCyleAmplitudeTrend/"))):
-                          os.mkdir((os.path.join(self.output_path, "ccg/seaCyleAmplitudeTrend/")))
-                    np.savetxt(os.path.join(self.output_path,"ccg/seaCyleAmplitudeTrend/ccgAmplitude_%s_benchmark_lat%s.csv" % (self.name, obs.lat[ss])), ccgAmplitude, delimiter= ",", header="year, total_amplitude, max_date, max_value, min_date, min_value")
-
-
-
+                    if not os.path.exists(
+                        (os.path.join(self.output_path, "ccg/seaCyleAmplitudeTrend/"))
+                    ):
+                        os.mkdir(
+                            (
+                                os.path.join(
+                                    self.output_path, "ccg/seaCyleAmplitudeTrend/"
+                                )
+                            )
+                        )
+                    np.savetxt(
+                        os.path.join(
+                            self.output_path,
+                            "ccg/seaCyleAmplitudeTrend/ccgAmplitude_%s_benchmark_lat%s.csv"
+                            % (self.name, obs.lat[ss]),
+                        ),
+                        ccgAmplitude,
+                        delimiter=",",
+                        header="year, total_amplitude, max_date, max_value, min_date, min_value",
+                    )
 
-        #ccgcrv trend, longtime scale, seasonal (harmonics), smooth data for mod
+        # ccgcrv trend, longtime scale, seasonal (harmonics), smooth data for mod
         ccgModTrend = copy.deepcopy(mod)
         ccgModTrend.name = "ccgTrend"
 
         ccgModPoly = copy.deepcopy(mod)
         ccgModPoly.name = "ccgPoly"
 
         ccgModIav = copy.deepcopy(mod)
@@ -717,437 +842,718 @@
 
         ccgModHarmo = copy.deepcopy(mod)
         ccgModHarmo.name = "ccgHarmo"
 
         ccgModSmooth = copy.deepcopy(mod)
         ccgModSmooth.name = "ccgSmooth"
 
-
-
         for ss in range(0, mod.ndata):
-
-            whrData = np.where(mod.data[:,ss].mask==False)
+            whrData = np.where(mod.data[:, ss].mask == False)
 
             if len(whrData) > 0:
-                xp=(mod.time/365)[whrData]
-                yp=mod.data[:,ss][whrData]
-                #long term filter is 10 yrs
+                xp = (mod.time / 365)[whrData]
+                yp = mod.data[:, ss][whrData]
+                # long term filter is 10 yrs
                 yrLongTerm = 10
-                filt = ccgfilt.ccgFilter(xp=xp,
-                                         yp=yp,
-                                         longterm = (yrLongTerm*365),
-                                         debug=False)
-
-                ccgModTrend.data[whrData,ss] = filt.getTrendValue(xp)
-                ccgModPoly.data[whrData,ss] = filt.getPolyValue(xp)
-
-                ccgModFun.data[whrData,ss] = filt.getFunctionValue(xp)
-                ccgModHarmo.data[whrData,ss] = filt.getHarmonicValue(xp)
-                ccgModSmooth.data[whrData,ss] = filt.getSmoothValue(xp)
-                ccgModIav.data[whrData,ss] = ccgModSmooth.data[whrData,ss] - ccgModTrend.data[whrData,ss] - ccgModHarmo.data[whrData,ss]
+                filt = ccgfilt.ccgFilter(
+                    xp=xp, yp=yp, longterm=(yrLongTerm * 365), debug=False
+                )
+
+                ccgModTrend.data[whrData, ss] = filt.getTrendValue(xp)
+                ccgModPoly.data[whrData, ss] = filt.getPolyValue(xp)
+
+                ccgModFun.data[whrData, ss] = filt.getFunctionValue(xp)
+                ccgModHarmo.data[whrData, ss] = filt.getHarmonicValue(xp)
+                ccgModSmooth.data[whrData, ss] = filt.getSmoothValue(xp)
+                ccgModIav.data[whrData, ss] = (
+                    ccgModSmooth.data[whrData, ss]
+                    - ccgModTrend.data[whrData, ss]
+                    - ccgModHarmo.data[whrData, ss]
+                )
 
-                #write out or not?
+                # write out or not?
                 if flag_write:
                     ccgCrossDates = filt.getTrendCrossingDates()
-                    np.savetxt(os.path.join(self.output_path,"ccg/CrossingDates/ccgCrossDates_%s_%s_site_%s.csv" % (self.name, m.name, ss)), ccgCrossDates, delimiter= ",", fmt= '%s')
-
+                    np.savetxt(
+                        os.path.join(
+                            self.output_path,
+                            "ccg/CrossingDates/ccgCrossDates_%s_%s_site_%s.csv"
+                            % (self.name, m.name, ss),
+                        ),
+                        ccgCrossDates,
+                        delimiter=",",
+                        fmt="%s",
+                    )
 
                     if ss == 5:
                         ccgAmplitude = filt.getAmplitudes()
-                        np.savetxt(os.path.join(self.output_path,"ccg/seaCyleAmplitudeTrend/ccgAmplitude_%s_%s_lat%s.csv" % (self.name,m.name, mod.lat[ss])), ccgAmplitude, delimiter= ",", header="year, total_amplitude, max_date, max_value, min_date, min_value")
-
+                        np.savetxt(
+                            os.path.join(
+                                self.output_path,
+                                "ccg/seaCyleAmplitudeTrend/ccgAmplitude_%s_%s_lat%s.csv"
+                                % (self.name, m.name, mod.lat[ss]),
+                            ),
+                            ccgAmplitude,
+                            delimiter=",",
+                            header="year, total_amplitude, max_date, max_value, min_date, min_value",
+                        )
 
         if flag_write:
-            results = Dataset(os.path.join(self.output_path,"ccg/ccg_%s_%s.nc" % (self.name,m.name)), mode="w")
-            for v in [ccgModTrend, ccgModPoly, ccgModIav, ccgModFun, ccgModHarmo, ccgModSmooth]:
+            results = Dataset(
+                os.path.join(
+                    self.output_path, "ccg/ccg_%s_%s.nc" % (self.name, m.name)
+                ),
+                mode="w",
+            )
+            for v in [
+                ccgModTrend,
+                ccgModPoly,
+                ccgModIav,
+                ccgModFun,
+                ccgModHarmo,
+                ccgModSmooth,
+            ]:
                 v.toNetCDF4(results)
             results.close()
 
-
-            results = Dataset(os.path.join(self.output_path,"ccg/ccg_%s_Benchmark.nc" % self.name), mode="w")
-            for v in [ccgObsTrend, ccgObsPoly, ccgObsIav, ccgObsFun, ccgObsHarmo, ccgObsSmooth]:
+            results = Dataset(
+                os.path.join(self.output_path, "ccg/ccg_%s_Benchmark.nc" % self.name),
+                mode="w",
+            )
+            for v in [
+                ccgObsTrend,
+                ccgObsPoly,
+                ccgObsIav,
+                ccgObsFun,
+                ccgObsHarmo,
+                ccgObsSmooth,
+            ]:
                 v.toNetCDF4(results)
             results.close()
 
-
-        #replace ILAMB iav with ccg IAV:
+        # replace ILAMB iav with ccg IAV:
         oiav.data = ccgObsIav.data
         miav.data = ccgModIav.data
 
-
-
-        #calculate score:
+        # calculate score:
         # Amplitude score: for each site we compute the relative error
         # in amplitude and then score each site using the
         # exponential. The score for the model is then the arithmetic
         # mean across sites.
-        #SampTmp = -np.clip(np.abs(mod_amp.data-obs_amp.data)/obs_amp.data, 0, 6)
-
-        #avoid underflow error
-        with np.errstate(under='ignore'):
-            Samp = Variable(name  = "Amplitude Score global",
-                            unit  = "1",
-                            data  = np.exp(-np.abs(mod_amp.data-obs_amp.data)/obs_amp.data).mean())
-
+        # SampTmp = -np.clip(np.abs(mod_amp.data-obs_amp.data)/obs_amp.data, 0, 6)
 
+        # avoid underflow error
+        with np.errstate(under="ignore"):
+            Samp = Variable(
+                name="Amplitude Score global",
+                unit="1",
+                data=np.exp(-np.abs(mod_amp.data - obs_amp.data) / obs_amp.data).mean(),
+            )
 
             # Interannual variability score: similar to the amplitude
             # score, we also score the relative error in the stdev(iav)
             # and report a mean across sites.
             ostd = oiav.data.std(axis=0)
             mstd = miav.data.std(axis=0)
-            Siav = Variable(name  = "Interannual Variability Score global",
-                            unit  = "1",
-                            data  = np.exp(-np.abs(mstd-ostd)/ostd).mean())
-
+            Siav = Variable(
+                name="Interannual Variability Score global",
+                unit="1",
+                data=np.exp(-np.abs(mstd - ostd) / ostd).mean(),
+            )
 
             # Min/Max Phase score: for each site we compute the phase
             # shift and normalize it linearly where a 0 day shift gets a
             # score of 1 and a 365/2 day shift is zero. We then compute a
             # weighted mean across sites where sites without a well
             # defined annual cycle are discarded.
-            Smax = Variable(name = "Max Phase Score global",
-                            unit = "1",
-                            data = np.average(_computeShift(obs_maxp,mod_maxp),weights=well_define))
-            Smin = Variable(name = "Min Phase Score global",
-                            unit = "1",
-                            data = np.average(_computeShift(obs_minp,mod_minp),weights=well_define))
-
-
+            Smax = Variable(
+                name="Max Phase Score global",
+                unit="1",
+                data=np.average(_computeShift(obs_maxp, mod_maxp), weights=well_define),
+            )
+            Smin = Variable(
+                name="Min Phase Score global",
+                unit="1",
+                data=np.average(_computeShift(obs_minp, mod_minp), weights=well_define),
+            )
 
         # Write out the intermediate variables
-        with Dataset(os.path.join(self.output_path,"%s_%s.nc" % (self.name,m.name)),mode="w") as results:
-            results.setncatts({"name" :m.name, "color":m.color, "weight":self.cweight})
-            for v in [mod,mcyc,miav,mcycf,mod_maxp,mod_minp,mod_amp,Samp,Siav,Smax,Smin]:
-                v.toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
-        if not self.master: return
-        with Dataset(os.path.join(self.output_path,"%s_Benchmark.nc" % self.name),mode="w") as results:
-            results.setncatts({"name" :"Benchmark", "color":np.asarray([0.5,0.5,0.5]),"weight":self.cweight})
-            for v in [obs,ocyc,oiav,ocycf,obs_maxp,obs_minp,obs_amp]:
-                v.toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
-
-
-    def modelPlots(self,m):
+        with Dataset(
+            os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name)), mode="w"
+        ) as results:
+            results.setncatts(
+                {"name": m.name, "color": m.color, "weight": self.cweight}
+            )
+            for v in [
+                mod,
+                mcyc,
+                miav,
+                mcycf,
+                mod_maxp,
+                mod_minp,
+                mod_amp,
+                Samp,
+                Siav,
+                Smax,
+                Smin,
+            ]:
+                v.toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
+        if not self.master:
+            return
+        with Dataset(
+            os.path.join(self.output_path, "%s_Benchmark.nc" % self.name), mode="w"
+        ) as results:
+            results.setncatts(
+                {
+                    "name": "Benchmark",
+                    "color": np.asarray([0.5, 0.5, 0.5]),
+                    "weight": self.cweight,
+                }
+            )
+            for v in [obs, ocyc, oiav, ocycf, obs_maxp, obs_minp, obs_amp]:
+                v.toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
 
+    def modelPlots(self, m):
         # Check that the required intermediate files are present
-        bname  = "%s/%s_Benchmark.nc" % (self.output_path,self.name)
-        fname  = "%s/%s_%s.nc" % (self.output_path,self.name,m.name)
-        if not os.path.isfile(bname): return
-        if not os.path.isfile(fname): return
+        bname = "%s/%s_Benchmark.nc" % (self.output_path, self.name)
+        fname = "%s/%s_%s.nc" % (self.output_path, self.name, m.name)
+        if not os.path.isfile(bname):
+            return
+        if not os.path.isfile(fname):
+            return
 
         # Get the HTML page
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
 
         # Read variables from the datafiles
-        obs   = Variable(filename=bname,variable_name="co2"       ,groupname="MeanState")
-        mod   = Variable(filename=fname,variable_name="co2"       ,groupname="MeanState")
-        ocyc  = Variable(filename=bname,variable_name="cycle"     ,groupname="MeanState")
-        mcyc  = Variable(filename=fname,variable_name="cycle"     ,groupname="MeanState")
-        oiav  = Variable(filename=bname,variable_name="iav"       ,groupname="MeanState")
-        miav  = Variable(filename=fname,variable_name="iav"       ,groupname="MeanState")
-        ocycf = Variable(filename=bname,variable_name="cycle_fine",groupname="MeanState")
-        mcycf = Variable(filename=fname,variable_name="cycle_fine",groupname="MeanState")
-        omaxp = Variable(filename=bname,variable_name="maxp"      ,groupname="MeanState")
-        ominp = Variable(filename=bname,variable_name="minp"      ,groupname="MeanState")
-        oamp  = Variable(filename=bname,variable_name="amp"       ,groupname="MeanState")
-        mmaxp = Variable(filename=fname,variable_name="maxp"      ,groupname="MeanState")
-        mminp = Variable(filename=fname,variable_name="minp"      ,groupname="MeanState")
-        mamp  = Variable(filename=fname,variable_name="amp"       ,groupname="MeanState")
-        t     = np.linspace(0,365,366)
+        obs = Variable(filename=bname, variable_name="co2", groupname="MeanState")
+        mod = Variable(filename=fname, variable_name="co2", groupname="MeanState")
+        ocyc = Variable(filename=bname, variable_name="cycle", groupname="MeanState")
+        mcyc = Variable(filename=fname, variable_name="cycle", groupname="MeanState")
+        oiav = Variable(filename=bname, variable_name="iav", groupname="MeanState")
+        miav = Variable(filename=fname, variable_name="iav", groupname="MeanState")
+        ocycf = Variable(
+            filename=bname, variable_name="cycle_fine", groupname="MeanState"
+        )
+        mcycf = Variable(
+            filename=fname, variable_name="cycle_fine", groupname="MeanState"
+        )
+        omaxp = Variable(filename=bname, variable_name="maxp", groupname="MeanState")
+        ominp = Variable(filename=bname, variable_name="minp", groupname="MeanState")
+        oamp = Variable(filename=bname, variable_name="amp", groupname="MeanState")
+        mmaxp = Variable(filename=fname, variable_name="maxp", groupname="MeanState")
+        mminp = Variable(filename=fname, variable_name="minp", groupname="MeanState")
+        mamp = Variable(filename=fname, variable_name="amp", groupname="MeanState")
+        t = np.linspace(0, 365, 366)
 
         # Create an index for ordering sites by descending latitude
-        sord  = np.argsort(obs.lat)[::-1]
-        inds  = np.asarray(range(len(self.lbls)),dtype=int)[sord]
-        lbls  = np.asarray(self.lbls)[sord]
+        sord = np.argsort(obs.lat)[::-1]
+        inds = np.asarray(range(len(self.lbls)), dtype=int)[sord]
+        lbls = np.asarray(self.lbls)[sord]
 
         # Create sparkline plots of each site
-        fig_height     = 1.
-        width_per_year = 5./28
-        fig_dpi        = 300.
-        lw             = 1.
-        bndmonths      = np.asarray(bnd_months,dtype=float)/365.
-        for site_id,site in zip(inds,lbls):
-
+        fig_height = 1.0
+        width_per_year = 5.0 / 28
+        fig_dpi = 300.0
+        lw = 1.0
+        bndmonths = np.asarray(bnd_months, dtype=float) / 365.0
+        for site_id, site in zip(inds, lbls):
             # Initialize site info
-            band    = self.lat_bands.searchsorted(obs.lat[site_id])
-            section = "Latitude Band %d to %d [ppm]" % (self.lat_bands[band-1],self.lat_bands[band])
-            vmin   = min(obs.data[:,site_id].min(),mod.data[:,site_id].min())
-            vmax   = max(obs.data[:,site_id].max(),mod.data[:,site_id].max())
-            tick   = max(int(np.floor(min(vmax,abs(vmin)))),1)
-            yticks = [-tick,0,tick]
+            band = self.lat_bands.searchsorted(obs.lat[site_id])
+            section = "Latitude Band %d to %d [ppm]" % (
+                self.lat_bands[band - 1],
+                self.lat_bands[band],
+            )
+            vmin = min(obs.data[:, site_id].min(), mod.data[:, site_id].min())
+            vmax = max(obs.data[:, site_id].max(), mod.data[:, site_id].max())
+            tick = max(int(np.floor(min(vmax, abs(vmin)))), 1)
+            yticks = [-tick, 0, tick]
 
             # How many years of data do we have?
-            t0,tf = mod.time_bnds[(np.where((mod.data[:,site_id]*mod.time).mask==False)[0])[[0,-1]]]/365.+1850
-            t0    = np.floor(t0[ 0])
-            tf    = np.ceil (tf[-1])
-            xticks = [i for i in range(int(t0),int(tf)+1) if str(i)[-1]=="0"]
+            t0, tf = (
+                mod.time_bnds[
+                    (np.where((mod.data[:, site_id] * mod.time).mask == False)[0])[
+                        [0, -1]
+                    ]
+                ]
+                / 365.0
+                + 1850
+            )
+            t0 = np.floor(t0[0])
+            tf = np.ceil(tf[-1])
+            xticks = [i for i in range(int(t0), int(tf) + 1) if str(i)[-1] == "0"]
 
             # Plot setup
-            fig_width0 = (5.   )*width_per_year
-            fig_width1 = (tf-t0)*width_per_year
-            fig_width2 = (tf-t0)*width_per_year
-            fig_width3 = (10.)  *width_per_year
-            fig,ax = plt.subplots(ncols   = 4,
-                                  figsize = (fig_width0+
-                                             fig_width1+
-                                             fig_width2+
-                                             fig_width3,fig_height),
-                                  gridspec_kw  = {'width_ratios':[fig_width0,fig_width3,fig_width1,fig_width2]},
-                                  tight_layout = True,
-                                  dpi = fig_dpi)
+            fig_width0 = (5.0) * width_per_year
+            fig_width1 = (tf - t0) * width_per_year
+            fig_width2 = (tf - t0) * width_per_year
+            fig_width3 = (10.0) * width_per_year
+            fig, ax = plt.subplots(
+                ncols=4,
+                figsize=(fig_width0 + fig_width1 + fig_width2 + fig_width3, fig_height),
+                gridspec_kw={
+                    "width_ratios": [fig_width0, fig_width3, fig_width1, fig_width2]
+                },
+                tight_layout=True,
+                dpi=fig_dpi,
+            )
 
             # Text only plot with the name and location of the site
-            ax[0].text(0.5,0.5,"%s\n%d,%d" % (site,obs.lat[site_id],obs.lon[site_id]),
-                       horizontalalignment = 'center',
-                       verticalalignment   = 'center',
-                       transform=ax[0].transAxes)
+            ax[0].text(
+                0.5,
+                0.5,
+                "%s\n%d,%d" % (site, obs.lat[site_id], obs.lon[site_id]),
+                horizontalalignment="center",
+                verticalalignment="center",
+                transform=ax[0].transAxes,
+            )
             ax[0].set_xticks([])
             ax[0].set_yticks([])
             ax[0].axis(False)
 
             # Plot the finely interpolated annual cycle, shade JFM and JJA
-            ax[1].fill_between(bndmonths[[0,3]],[vmin,vmin],[vmax,vmax],color='k',alpha=0.05,lw=0)
-            ax[1].fill_between(bndmonths[[6,9]],[vmin,vmin],[vmax,vmax],color='k',alpha=0.05,lw=0)
-            ax[1].plot(ocyc.time/365,ocyc.data[:,site_id],lw=1.5*lw,color='k',alpha=0.35)
-            ax[1].plot(mcyc.time/365,mcyc.data[:,site_id],lw=lw,color=m.color)
-            ax[1].set_ylim(vmin,vmax)
-            ax[1].spines['top'  ].set_visible(False)
-            ax[1].spines['right'].set_visible(False)
-            ax[1].spines['bottom'].set_position('zero')
+            ax[1].fill_between(
+                bndmonths[[0, 3]],
+                [vmin, vmin],
+                [vmax, vmax],
+                color="k",
+                alpha=0.05,
+                lw=0,
+            )
+            ax[1].fill_between(
+                bndmonths[[6, 9]],
+                [vmin, vmin],
+                [vmax, vmax],
+                color="k",
+                alpha=0.05,
+                lw=0,
+            )
+            ax[1].plot(
+                ocyc.time / 365,
+                ocyc.data[:, site_id],
+                lw=1.5 * lw,
+                color="k",
+                alpha=0.35,
+            )
+            ax[1].plot(mcyc.time / 365, mcyc.data[:, site_id], lw=lw, color=m.color)
+            ax[1].set_ylim(vmin, vmax)
+            ax[1].spines["top"].set_visible(False)
+            ax[1].spines["right"].set_visible(False)
+            ax[1].spines["bottom"].set_position("zero")
             ax[1].set_xticks([])
             ax[1].set_yticks(yticks)
             ax[1].set_xticklabels([])
-            ax[1].set_ylabel('cycle')
+            ax[1].set_ylabel("cycle")
 
             # Plot the variability in co2, shade every other decade
-            shade = [t0,]+xticks+[tf,]
-            alf   = 0.15
-            bot   = vmin + 0.02*(vmax-vmin)
-            for i in range(1,len(shade)-1):
+            shade = (
+                [
+                    t0,
+                ]
+                + xticks
+                + [
+                    tf,
+                ]
+            )
+            alf = 0.15
+            bot = vmin + 0.02 * (vmax - vmin)
+            for i in range(1, len(shade) - 1):
                 if i % 2 == 0:
-                    ax[2].text(shade[i],bot,shade[i],color='k',alpha=alf,size=12)
-                    ax[3].text(shade[i],bot,shade[i],color='k',alpha=alf,size=12)
+                    ax[2].text(shade[i], bot, shade[i], color="k", alpha=alf, size=12)
+                    ax[3].text(shade[i], bot, shade[i], color="k", alpha=alf, size=12)
                 else:
-                    ax[2].fill_between(shade[i:(i+2)],[vmin,vmin],[vmax,vmax],color='k',alpha=0.05,lw=0)
-                    ax[2].text(shade[i],bot,shade[i],color='k',alpha=alf,size=12)
-                    ax[3].fill_between(shade[i:(i+2)],[vmin,vmin],[vmax,vmax],color='k',alpha=0.05,lw=0)
-                    ax[3].text(shade[i],bot,shade[i],color='k',alpha=alf,size=12)
-
-            ax[2].plot(obs.time/365+1850,obs.data[:,site_id],lw=1.5*lw,color='k',alpha=0.35)
-            ax[2].plot(mod.time/365+1850,mod.data[:,site_id],lw=lw,color=m.color)
-            ax[2].set_ylim(vmin,vmax)
-            ax[2].spines['top'  ].set_visible(False)
-            ax[2].spines['right'].set_visible(False)
-            ax[2].spines['bottom'].set_position('zero')
+                    ax[2].fill_between(
+                        shade[i : (i + 2)],
+                        [vmin, vmin],
+                        [vmax, vmax],
+                        color="k",
+                        alpha=0.05,
+                        lw=0,
+                    )
+                    ax[2].text(shade[i], bot, shade[i], color="k", alpha=alf, size=12)
+                    ax[3].fill_between(
+                        shade[i : (i + 2)],
+                        [vmin, vmin],
+                        [vmax, vmax],
+                        color="k",
+                        alpha=0.05,
+                        lw=0,
+                    )
+                    ax[3].text(shade[i], bot, shade[i], color="k", alpha=alf, size=12)
+
+            ax[2].plot(
+                obs.time / 365 + 1850,
+                obs.data[:, site_id],
+                lw=1.5 * lw,
+                color="k",
+                alpha=0.35,
+            )
+            ax[2].plot(
+                mod.time / 365 + 1850, mod.data[:, site_id], lw=lw, color=m.color
+            )
+            ax[2].set_ylim(vmin, vmax)
+            ax[2].spines["top"].set_visible(False)
+            ax[2].spines["right"].set_visible(False)
+            ax[2].spines["bottom"].set_position("zero")
             ax[2].set_yticks(yticks)
             ax[2].set_xticklabels([])
             ax[2].set_xticks([])
-            ax[2].set_ylabel('var')
+            ax[2].set_ylabel("var")
 
             # Plot the interannual variability in co2, shade every other decade
-            ax[3].plot(oiav.time/365+1850,oiav.data[:,site_id],lw=1.5*lw,color='k',alpha=0.35)
-            ax[3].plot(miav.time/365+1850,miav.data[:,site_id],lw=lw,color=m.color)
-            ax[3].set_ylim(vmin,vmax)
-            ax[3].spines['top'  ].set_visible(False)
-            ax[3].spines['right'].set_visible(False)
-            ax[3].spines['bottom'].set_position('zero')
+            ax[3].plot(
+                oiav.time / 365 + 1850,
+                oiav.data[:, site_id],
+                lw=1.5 * lw,
+                color="k",
+                alpha=0.35,
+            )
+            ax[3].plot(
+                miav.time / 365 + 1850, miav.data[:, site_id], lw=lw, color=m.color
+            )
+            ax[3].set_ylim(vmin, vmax)
+            ax[3].spines["top"].set_visible(False)
+            ax[3].spines["right"].set_visible(False)
+            ax[3].spines["bottom"].set_position("zero")
             ax[3].set_xticks([])
             ax[3].set_yticks(yticks)
-            ax[3].tick_params(axis='x',direction='inout',length=10)
-            ax[3].set_ylabel('iav')
+            ax[3].tick_params(axis="x", direction="inout", length=10)
+            ax[3].set_ylabel("iav")
 
             # Save the figure
-            fig.savefig(os.path.join(self.output_path,"%s_global_%s.png" % (m.name,site)))
-            page.addFigure(section,
-                           site,
-                           "MNAME_global_%s.png" % site,
-                           side   = "",
-                           legend = False,
-                           width  = fig.get_size_inches()[0]*fig.dpi*0.25,
-                           br     = True,
-                           longname = "Site %s" % site)
+            fig.savefig(
+                os.path.join(self.output_path, "%s_global_%s.png" % (m.name, site))
+            )
+            page.addFigure(
+                section,
+                site,
+                "MNAME_global_%s.png" % site,
+                side="",
+                legend=False,
+                width=fig.get_size_inches()[0] * fig.dpi * 0.25,
+                br=True,
+                longname="Site %s" % site,
+            )
             plt.close()
 
-
         # Compute mean amplitude, max and min phase over latitude bands
         lat_bnds = self.lat_bands
-        lat      = 0.5*(lat_bnds[:-1]+lat_bnds[1:])
-        nb       = lat_bnds.size-1
-        o_band_min = np.zeros(nb); o_band_max = np.zeros(nb); o_band_amp = np.zeros(nb); o_band_iav = np.zeros(nb)
-        m_band_min = np.zeros(nb); m_band_max = np.zeros(nb); m_band_amp = np.zeros(nb); m_band_iav = np.zeros(nb)
-        with np.errstate(under='ignore'):
+        lat = 0.5 * (lat_bnds[:-1] + lat_bnds[1:])
+        nb = lat_bnds.size - 1
+        o_band_min = np.zeros(nb)
+        o_band_max = np.zeros(nb)
+        o_band_amp = np.zeros(nb)
+        o_band_iav = np.zeros(nb)
+        m_band_min = np.zeros(nb)
+        m_band_max = np.zeros(nb)
+        m_band_amp = np.zeros(nb)
+        m_band_iav = np.zeros(nb)
+        with np.errstate(under="ignore"):
             for i in range(o_band_min.size):
-                ind  = np.where((obs.lat >  lat_bnds[i  ])*
-                                (obs.lat <= lat_bnds[i+1]))[0]
+                ind = np.where((obs.lat > lat_bnds[i]) * (obs.lat <= lat_bnds[i + 1]))[
+                    0
+                ]
                 o_band_min[i] = _meanDay(ominp.data[ind])
                 o_band_max[i] = _meanDay(omaxp.data[ind])
-                o_band_amp[i] =           oamp.data[ind].mean()
-                o_band_iav[i] =           oiav.data.std(axis=0)[ind].mean()
+                o_band_amp[i] = oamp.data[ind].mean()
+                o_band_iav[i] = oiav.data.std(axis=0)[ind].mean()
                 m_band_min[i] = _meanDay(mminp.data[ind])
                 m_band_max[i] = _meanDay(mmaxp.data[ind])
-                m_band_amp[i] =           mamp.data[ind].mean()
-                m_band_iav[i] =           miav.data.std(axis=0)[ind].mean()
-
-
+                m_band_amp[i] = mamp.data[ind].mean()
+                m_band_iav[i] = miav.data.std(axis=0)[ind].mean()
 
         # To plot the mean values over latitude bands superimposed on
         # the globe, we have to transform the phase and amplitude
         # values to [-180,180], as if they were longitudes.
-        o_band_min = o_band_min/365.*360-180
-        o_band_max = o_band_max/365.*360-180
-        m_band_min = m_band_min/365.*360-180
-        m_band_max = m_band_max/365.*360-180
-
-        max_amp    = o_band_amp.max()
-        min_amp    = o_band_amp.min()
-        amp_ticks = np.linspace(min_amp,max_amp,6)
+        o_band_min = o_band_min / 365.0 * 360 - 180
+        o_band_max = o_band_max / 365.0 * 360 - 180
+        m_band_min = m_band_min / 365.0 * 360 - 180
+        m_band_max = m_band_max / 365.0 * 360 - 180
+
+        max_amp = o_band_amp.max()
+        min_amp = o_band_amp.min()
+        amp_ticks = np.linspace(min_amp, max_amp, 6)
         amp_ticklabels = ["%.2f" % t for t in amp_ticks]
-        damp     = 0.1*(max_amp - min_amp)
+        damp = 0.1 * (max_amp - min_amp)
         max_amp += damp
         min_amp -= damp
-        o_band_amp = (o_band_amp-min_amp)/(max_amp-min_amp)*360-180
-        m_band_amp = (m_band_amp-min_amp)/(max_amp-min_amp)*360-180
-        amp_ticks  = (amp_ticks -min_amp)/(max_amp-min_amp)*360-180
-
-        max_iav    = max(o_band_iav.max(),m_band_iav.max())
-        min_iav    = 0.
-        iav_ticks = np.linspace(min_iav,max_iav,6)
+        o_band_amp = (o_band_amp - min_amp) / (max_amp - min_amp) * 360 - 180
+        m_band_amp = (m_band_amp - min_amp) / (max_amp - min_amp) * 360 - 180
+        amp_ticks = (amp_ticks - min_amp) / (max_amp - min_amp) * 360 - 180
+
+        max_iav = max(o_band_iav.max(), m_band_iav.max())
+        min_iav = 0.0
+        iav_ticks = np.linspace(min_iav, max_iav, 6)
         iav_ticklabels = ["%.2f" % t for t in iav_ticks]
-        diav     = 0.1*(max_iav - min_iav)
+        diav = 0.1 * (max_iav - min_iav)
         max_iav += diav
         min_iav -= diav
-        o_band_iav = (o_band_iav-min_iav)/(max_iav-min_iav)*360.-180.
-        m_band_iav = (m_band_iav-min_iav)/(max_iav-min_iav)*360.-180.
-        iav_ticks  = (iav_ticks -min_iav)/(max_iav-min_iav)*360.-180.
+        o_band_iav = (o_band_iav - min_iav) / (max_iav - min_iav) * 360.0 - 180.0
+        m_band_iav = (m_band_iav - min_iav) / (max_iav - min_iav) * 360.0 - 180.0
+        iav_ticks = (iav_ticks - min_iav) / (max_iav - min_iav) * 360.0 - 180.0
 
         # Plot mean latitude band amplitude where amplitude is on the longitude axis
-        fig,ax = plt.subplots(figsize=(8,4.5),tight_layout=True,subplot_kw={'projection':ccrs.PlateCarree()})
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.875'),zorder=-1)
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                    edgecolor='face',
-                                                    facecolor='1.000'),zorder=-1)
-        ax.set_extent([-180,+180,-90,+90],ccrs.PlateCarree())
+        fig, ax = plt.subplots(
+            figsize=(8, 4.5),
+            tight_layout=True,
+            subplot_kw={"projection": ccrs.PlateCarree()},
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+            ),
+            zorder=-1,
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "ocean", "110m", edgecolor="face", facecolor="1.000"
+            ),
+            zorder=-1,
+        )
+        ax.set_extent([-180, +180, -90, +90], ccrs.PlateCarree())
         ms = 8
-        ax.scatter(obs.lon,obs.lat,8,color="0.60",label="Sites")
-        ax.plot(o_band_amp,lat,'--o',color=np.asarray([0.5,0.5,0.5]),label="%s amplitude" % self.name,mew=0,markersize=ms)
-        ax.plot(m_band_amp,lat,'-o' ,color=m.color,label="%s amplitude" % m.name,mew=0,markersize=ms)
-        ax.yaxis.grid(color="0.875",linestyle="-")
-        ax.legend(bbox_to_anchor=(0,1.005,1,0.25),loc='lower left',mode='expand',ncol=5,borderaxespad=0,frameon=False)
-        ax.set_xlim(-180,180)
-        ax.set_ylim(-90,90)
+        ax.scatter(obs.lon, obs.lat, 8, color="0.60", label="Sites")
+        ax.plot(
+            o_band_amp,
+            lat,
+            "--o",
+            color=np.asarray([0.5, 0.5, 0.5]),
+            label="%s amplitude" % self.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.plot(
+            m_band_amp,
+            lat,
+            "-o",
+            color=m.color,
+            label="%s amplitude" % m.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.yaxis.grid(color="0.875", linestyle="-")
+        ax.legend(
+            bbox_to_anchor=(0, 1.005, 1, 0.25),
+            loc="lower left",
+            mode="expand",
+            ncol=5,
+            borderaxespad=0,
+            frameon=False,
+        )
+        ax.set_xlim(-180, 180)
+        ax.set_ylim(-90, 90)
         ax.set_xlabel(obs.unit)
         ax.set_xticks(amp_ticks)
         ax.set_xticklabels(amp_ticklabels)
         ax.set_yticks(lat_bnds)
-        fig.savefig(os.path.join(self.output_path,"%s_global_amp.png" % m.name))
+        fig.savefig(os.path.join(self.output_path, "%s_global_amp.png" % m.name))
         plt.close()
-        page.addFigure("Summary",
-                       "amp",
-                       "MNAME_RNAME_amp.png",
-                       side   = "AMPLITUDE",
-                       width  = fig.get_size_inches()[0]*fig.dpi*0.75,
-                       legend = False,
-                       longname = "Amplitude")
+        page.addFigure(
+            "Summary",
+            "amp",
+            "MNAME_RNAME_amp.png",
+            side="AMPLITUDE",
+            width=fig.get_size_inches()[0] * fig.dpi * 0.75,
+            legend=False,
+            longname="Amplitude",
+        )
 
         # Plot mean latitude band iav where iav is on the longitude axis
 
-        fig,ax = plt.subplots(figsize=(8,4.5),tight_layout=True,subplot_kw={'projection':ccrs.PlateCarree()})
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.875'),zorder=-1)
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                    edgecolor='face',
-                                                    facecolor='1.000'),zorder=-1)
-        ax.set_extent([-180,+180,-90,+90],ccrs.PlateCarree())
+        fig, ax = plt.subplots(
+            figsize=(8, 4.5),
+            tight_layout=True,
+            subplot_kw={"projection": ccrs.PlateCarree()},
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+            ),
+            zorder=-1,
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "ocean", "110m", edgecolor="face", facecolor="1.000"
+            ),
+            zorder=-1,
+        )
+        ax.set_extent([-180, +180, -90, +90], ccrs.PlateCarree())
         ms = 8
-        ax.scatter(obs.lon,obs.lat,8,color="0.60",label="Sites")
-        ax.plot(o_band_iav,lat,'--o',color=np.asarray([0.5,0.5,0.5]),label="%s variability" % self.name,mew=0,markersize=ms)
-        ax.plot(m_band_iav,lat,'-o' ,color=m.color,label="%s variability" % m.name,mew=0,markersize=ms)
-        ax.yaxis.grid(color="0.875",linestyle="-")
-        ax.legend(bbox_to_anchor=(0,1.005,1,0.25),loc='lower left',mode='expand',ncol=5,borderaxespad=0,frameon=False)
-        ax.set_xlim(-180,180)
-        ax.set_ylim(-90,90)
+        ax.scatter(obs.lon, obs.lat, 8, color="0.60", label="Sites")
+        ax.plot(
+            o_band_iav,
+            lat,
+            "--o",
+            color=np.asarray([0.5, 0.5, 0.5]),
+            label="%s variability" % self.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.plot(
+            m_band_iav,
+            lat,
+            "-o",
+            color=m.color,
+            label="%s variability" % m.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.yaxis.grid(color="0.875", linestyle="-")
+        ax.legend(
+            bbox_to_anchor=(0, 1.005, 1, 0.25),
+            loc="lower left",
+            mode="expand",
+            ncol=5,
+            borderaxespad=0,
+            frameon=False,
+        )
+        ax.set_xlim(-180, 180)
+        ax.set_ylim(-90, 90)
         ax.set_xlabel(obs.unit)
         ax.set_xticks(iav_ticks)
         ax.set_xticklabels(iav_ticklabels)
         ax.set_yticks(lat_bnds)
-        fig.savefig(os.path.join(self.output_path,"%s_global_iav.png" % m.name))
+        fig.savefig(os.path.join(self.output_path, "%s_global_iav.png" % m.name))
         plt.close()
-        page.addFigure("Summary",
-                       "iav",
-                       "MNAME_RNAME_iav.png",
-                       side   = "INTERANNUAL VARIABILITY",
-                       width  = fig.get_size_inches()[0]*fig.dpi*0.75,
-                       legend = False)
+        page.addFigure(
+            "Summary",
+            "iav",
+            "MNAME_RNAME_iav.png",
+            side="INTERANNUAL VARIABILITY",
+            width=fig.get_size_inches()[0] * fig.dpi * 0.75,
+            legend=False,
+        )
 
         # Plot mean latitude band max phase where the phase is on the longitude axis
-        fig,ax = plt.subplots(figsize=(8,4.5),tight_layout=True,subplot_kw={'projection':ccrs.PlateCarree()})
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.875'),zorder=-1)
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                    edgecolor='face',
-                                                    facecolor='1.000'),zorder=-1)
-        ax.set_extent([-180,+180,-90,+90],ccrs.PlateCarree())
+        fig, ax = plt.subplots(
+            figsize=(8, 4.5),
+            tight_layout=True,
+            subplot_kw={"projection": ccrs.PlateCarree()},
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+            ),
+            zorder=-1,
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "ocean", "110m", edgecolor="face", facecolor="1.000"
+            ),
+            zorder=-1,
+        )
+        ax.set_extent([-180, +180, -90, +90], ccrs.PlateCarree())
         ms = 8
-        ax.scatter(obs.lon,obs.lat,8,color="0.60",label="Sites")
-        ax.plot(o_band_max,lat,'--o',color=np.asarray([0.5,0.5,0.5]),label="%s maximum" % self.name,mew=0,markersize=ms)
-        ax.plot(m_band_max,lat,'-o' ,color=m.color,label="%s maximum" % m.name,mew=0,markersize=ms)
-        ax.yaxis.grid(color="0.875",linestyle="-")
-        ax.legend(bbox_to_anchor=(0,1.005,1,0.25),loc='lower left',mode='expand',ncol=3,borderaxespad=0,frameon=False)
-        ax.set_xlim(-180,180)
-        ax.set_ylim(-90,90)
-        ax.set_xticks(mid_months/365.*360.-180)
+        ax.scatter(obs.lon, obs.lat, 8, color="0.60", label="Sites")
+        ax.plot(
+            o_band_max,
+            lat,
+            "--o",
+            color=np.asarray([0.5, 0.5, 0.5]),
+            label="%s maximum" % self.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.plot(
+            m_band_max,
+            lat,
+            "-o",
+            color=m.color,
+            label="%s maximum" % m.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.yaxis.grid(color="0.875", linestyle="-")
+        ax.legend(
+            bbox_to_anchor=(0, 1.005, 1, 0.25),
+            loc="lower left",
+            mode="expand",
+            ncol=3,
+            borderaxespad=0,
+            frameon=False,
+        )
+        ax.set_xlim(-180, 180)
+        ax.set_ylim(-90, 90)
+        ax.set_xticks(mid_months / 365.0 * 360.0 - 180)
         ax.set_xticklabels(lbl_months)
         ax.set_yticks(lat_bnds)
-        fig.savefig(os.path.join(self.output_path,"%s_global_maxphase.png" % m.name))
+        fig.savefig(os.path.join(self.output_path, "%s_global_maxphase.png" % m.name))
         plt.close()
-        page.addFigure("Summary",
-                       "maxphase",
-                       "MNAME_RNAME_maxphase.png",
-                       side   = "TIMING OF MAXIMUM",
-                       width  = fig.get_size_inches()[0]*fig.dpi*0.75,
-                       legend = False,
-                       longname = "Timing of maximum phase")
+        page.addFigure(
+            "Summary",
+            "maxphase",
+            "MNAME_RNAME_maxphase.png",
+            side="TIMING OF MAXIMUM",
+            width=fig.get_size_inches()[0] * fig.dpi * 0.75,
+            legend=False,
+            longname="Timing of maximum phase",
+        )
 
         # Plot mean latitude band min phase where the phase is on the longitude axis
-        fig,ax = plt.subplots(figsize=(8,4.5),tight_layout=True,subplot_kw={'projection':ccrs.PlateCarree()})
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.875'),zorder=-1)
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                    edgecolor='face',
-                                                    facecolor='1.000'),zorder=-1)
-        ax.set_extent([-180,+180,-90,+90],ccrs.PlateCarree())
+        fig, ax = plt.subplots(
+            figsize=(8, 4.5),
+            tight_layout=True,
+            subplot_kw={"projection": ccrs.PlateCarree()},
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+            ),
+            zorder=-1,
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "ocean", "110m", edgecolor="face", facecolor="1.000"
+            ),
+            zorder=-1,
+        )
+        ax.set_extent([-180, +180, -90, +90], ccrs.PlateCarree())
         ms = 8
-        ax.scatter(obs.lon,obs.lat,8,color="0.60",label="Sites")
-        ax.plot(o_band_min,lat,'--o',color=np.asarray([0.5,0.5,0.5]),label="%s minimum" % self.name,mew=0,markersize=ms)
-        ax.plot(m_band_min,lat,'-o' ,color=m.color,label="%s minimum" % m.name,mew=0,markersize=ms)
-        ax.yaxis.grid(color="0.875",linestyle="-")
-        ax.legend(bbox_to_anchor=(0,1.005,1,0.25),loc='lower left',mode='expand',ncol=3,borderaxespad=0,frameon=False)
-        ax.set_xlim(-180,180)
-        ax.set_ylim(-90,90)
-        ax.set_xticks(mid_months/365.*360.-180)
+        ax.scatter(obs.lon, obs.lat, 8, color="0.60", label="Sites")
+        ax.plot(
+            o_band_min,
+            lat,
+            "--o",
+            color=np.asarray([0.5, 0.5, 0.5]),
+            label="%s minimum" % self.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.plot(
+            m_band_min,
+            lat,
+            "-o",
+            color=m.color,
+            label="%s minimum" % m.name,
+            mew=0,
+            markersize=ms,
+        )
+        ax.yaxis.grid(color="0.875", linestyle="-")
+        ax.legend(
+            bbox_to_anchor=(0, 1.005, 1, 0.25),
+            loc="lower left",
+            mode="expand",
+            ncol=3,
+            borderaxespad=0,
+            frameon=False,
+        )
+        ax.set_xlim(-180, 180)
+        ax.set_ylim(-90, 90)
+        ax.set_xticks(mid_months / 365.0 * 360.0 - 180)
         ax.set_xticklabels(lbl_months)
         ax.set_yticks(lat_bnds)
-        fig.savefig(os.path.join(self.output_path,"%s_global_minphase.png" % m.name))
+        fig.savefig(os.path.join(self.output_path, "%s_global_minphase.png" % m.name))
         plt.close()
-        page.addFigure("Summary",
-                       "minphase",
-                       "MNAME_RNAME_minphase.png",
-                       side   = "TIMING OF MINIMUM",
-                       width  = fig.get_size_inches()[0]*fig.dpi*0.75,
-                       legend = False,
-                       longname = "Timing of minimum phase")
+        page.addFigure(
+            "Summary",
+            "minphase",
+            "MNAME_RNAME_minphase.png",
+            side="TIMING OF MINIMUM",
+            width=fig.get_size_inches()[0] * fig.dpi * 0.75,
+            legend=False,
+            longname="Timing of minimum phase",
+        )
 
     def compositePlots(self):
         pass
-
-
-
-
-
-
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfDiurnal.py` & `ILAMB-2.7/src/ILAMB/ConfDiurnal.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,48 +1,52 @@
-from .Confrontation import Confrontation
-from .Confrontation import getVariableList
+import glob
+import os
+
+import cftime
 import matplotlib.pyplot as plt
-from . import Post as post
-from scipy.interpolate import CubicSpline
-from .Variable import Variable
-from netCDF4 import Dataset
-from . import ilamblib as il
 import numpy as np
-import os,glob
-from .constants import lbl_months,bnd_months
 from cf_units import Unit
-import cftime
+from netCDF4 import Dataset
 
-def _meanDiurnalCycle(var,n):
-    begin = np.argmin(var.time[:(n-1)]%n)
-    end   = begin+int(var.time[begin:].size/float(n))*n
-    vmean = var.data[begin:end].reshape((-1,n))
-    vmean = vmean[np.where(vmean.mask.any(axis=1)==False)]
-    per   = np.percentile(vmean,[10,90],axis=0)
-    per10 = per[0,:]
-    per90 = per[1,:]
-    np.seterr(under='ignore',over='ignore')
+from ILAMB import Post as post
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.constants import bnd_months, lbl_months
+from ILAMB.Variable import Variable
+
+
+def _meanDiurnalCycle(var, n):
+    begin = np.argmin(var.time[: (n - 1)] % n)
+    end = begin + int(var.time[begin:].size / float(n)) * n
+    vmean = var.data[begin:end].reshape((-1, n))
+    vmean = vmean[np.where(vmean.mask.any(axis=1) == False)]
+    per = np.percentile(vmean, [10, 90], axis=0)
+    per10 = per[0, :]
+    per90 = per[1, :]
+    np.seterr(under="ignore", over="ignore")
     vmean = vmean.mean(axis=0)
-    np.seterr(under='raise',over='raise')
-    t     = np.linspace(0,24,n+1)[:-1]
-    tmax  = t[vmean.argmax()]
-    return t,vmean,per10,per90,tmax
-
-def DiurnalReshape(time,time_bnds,data):
-    dt    = (time_bnds[:,1]-time_bnds[:,0])[:-1]
-    dt    = dt.mean()
-    spd   = int(round(1./dt))
-    begin = np.argmin(time[:(spd-1)]%spd)
-    end   = begin+int(time[begin:].size/float(spd))*spd
-    shp   = (-1,spd) + data.shape[1:]
+    np.seterr(under="raise", over="raise")
+    t = np.linspace(0, 24, n + 1)[:-1]
+    tmax = t[vmean.argmax()]
+    return t, vmean, per10, per90, tmax
+
+
+def DiurnalReshape(time, time_bnds, data):
+    dt = (time_bnds[:, 1] - time_bnds[:, 0])[:-1]
+    dt = dt.mean()
+    spd = int(round(1.0 / dt))
+    begin = np.argmin(time[: (spd - 1)] % spd)
+    end = begin + int(time[begin:].size / float(spd)) * spd
+    shp = (-1, spd) + data.shape[1:]
     cycle = data[begin:end].reshape(shp)
-    t     = time[begin:end].reshape(shp).mean(axis=1)
-    return cycle,t
+    t = time[begin:end].reshape(shp).mean(axis=1)
+    return cycle, t
+
 
-def _findSeasonalTiming(t,x):
+def _findSeasonalTiming(t, x):
     """Return the beginning and ending time of the season of x.
 
     The data x is assumed to start out relatively small, pass through
     a seasonal period of high values, and then return to small values,
     similar to a bell curve. This routine returns the times where
     these seasonal changes occur. To do this, we accumulate the
     variable x and then try to fit 3 linear segments to each seasonal
@@ -57,345 +61,415 @@
         a single cycle of data to extract the season from
 
     Returns
     -------
     tbnds: numpy.ndarray
         the beginning and ending time in an array of size 2
     """
-    def cost(t,y):
+
+    def cost(t, y):
         from scipy.stats import linregress
-        out = linregress(t,y)
-        return np.sqrt((((out.slope*t+out.intercept)-y)**2).sum())
+
+        out = linregress(t, y)
+        return np.sqrt((((out.slope * t + out.intercept) - y) ** 2).sum())
+
     y = x.cumsum()
-    b = int(y.size/2-1)
-    e = int(y.size/2+1)
-    I = np.asarray(range(2,b))
+    b = int(y.size / 2 - 1)
+    e = int(y.size / 2 + 1)
+    I = np.asarray(range(2, b))
     C = np.zeros(I.shape)
-    for a,i in enumerate(I): C[a] = cost(t[:i],y[:i]) + cost(t[i:e],y[i:e])
+    for a, i in enumerate(I):
+        C[a] = cost(t[:i], y[:i]) + cost(t[i:e], y[i:e])
     b = I[C.argmin()]
-    I = np.asarray(range(e,y.size-2))
+    I = np.asarray(range(e, y.size - 2))
     C = np.zeros(I.shape)
-    for a,i in enumerate(I): C[a] = cost(t[b:i],y[b:i]) + cost(t[i:],y[i:])
+    for a, i in enumerate(I):
+        C[a] = cost(t[b:i], y[b:i]) + cost(t[i:], y[i:])
     e = I[C.argmin()]
-    return t[[b,e]]
+    return t[[b, e]]
+
 
-def _findSeasonalCentroid(t,x):
+def _findSeasonalCentroid(t, x):
     """Return the centroid of the season in polar and cartesian coordinates.
 
     Parameters
     ----------
     time: numpy.ndarray
         time array but scaled [0,2 pi]
     x: numpy.ndarray
         a single cycle of data to extract the season from
 
     Returns
     -------
     centroid: numpy.ndarray
         array of size 4, [r,theta,x,y]
     """
-    x0 = (x*np.cos(t/365.*2*np.pi)).mean()
-    y0 = (x*np.sin(t/365.*2*np.pi)).mean()
-    r0 = np.sqrt(x0*x0+y0*y0)
-    a0 = np.arctan2(y0,x0)
-    return r0,a0,x0,y0
+    x0 = (x * np.cos(t / 365.0 * 2 * np.pi)).mean()
+    y0 = (x * np.sin(t / 365.0 * 2 * np.pi)).mean()
+    r0 = np.sqrt(x0 * x0 + y0 * y0)
+    a0 = np.arctan2(y0, x0)
+    return r0, a0, x0, y0
+
 
 class ConfDiurnal(Confrontation):
-    """A confrontation for examining the diurnal
-    """
-    def __init__(self,**keywords):
+    """A confrontation for examining the diurnal"""
 
+    def __init__(self, **keywords):
         # Calls the regular constructor
-        super(ConfDiurnal,self).__init__(**keywords)
+        super(ConfDiurnal, self).__init__(**keywords)
 
         # Setup a html layout for generating web views of the results
         pages = []
 
         # Mean State page
-        pages.append(post.HtmlPage("MeanState","Mean State"))
+        pages.append(post.HtmlPage("MeanState", "Mean State"))
         pages[-1].setHeader("CNAME / RNAME / MNAME")
-        pages[-1].setSections(["Seasonal Diurnal Cycle","Diurnal magnitude"])
-        pages.append(post.HtmlAllModelsPage("AllModels","All Models"))
+        pages[-1].setSections(["Seasonal Diurnal Cycle", "Diurnal magnitude"])
+        pages.append(post.HtmlAllModelsPage("AllModels", "All Models"))
         pages[-1].setHeader("CNAME / RNAME")
         pages[-1].setSections([])
         pages[-1].setRegions(self.regions)
-        pages.append(post.HtmlPage("DataInformation","Data Information"))
+        pages.append(post.HtmlPage("DataInformation", "Data Information"))
         pages[-1].setSections([])
         pages[-1].text = "\n"
         with Dataset(self.source) as dset:
             for attr in dset.ncattrs():
                 a = dset.getncattr(attr)
-                if 'astype' in dir(a): a = a.astype('str')
-                if 'encode' in dir(a): a = a.encode('ascii','ignore')                
-                pages[-1].text += "<p><b>&nbsp;&nbsp;%s:&nbsp;</b>%s</p>\n" % (attr,a)
-        self.layout = post.HtmlLayout(pages,self.longname)
-
-    def stageData(self,m):
-
-        obs = Variable(filename       = self.source,
-                       variable_name  = self.variable,
-                       alternate_vars = self.alternate_vars,
-                       convert_calendar = False)
-        if obs.time is None: raise il.NotTemporalVariable()
+                if "astype" in dir(a):
+                    a = a.astype("str")
+                if "encode" in dir(a):
+                    a = a.encode("ascii", "ignore")
+                pages[-1].text += "<p><b>&nbsp;&nbsp;%s:&nbsp;</b>%s</p>\n" % (attr, a)
+        self.layout = post.HtmlLayout(pages, self.longname)
+
+    def stageData(self, m):
+        obs = Variable(
+            filename=self.source,
+            variable_name=self.variable,
+            alternate_vars=self.alternate_vars,
+            convert_calendar=False,
+        )
+        if obs.time is None:
+            raise il.NotTemporalVariable()
         self.pruneRegions(obs)
 
         # Try to extract a commensurate quantity from the model
-        mod = m.extractTimeSeries(self.variable,
-                                  alt_vars     = self.alternate_vars,
-                                  expression   = self.derived,
-                                  convert_calendar = False,
-                                  lats         = None if obs.spatial else obs.lat,
-                                  lons         = None if obs.spatial else obs.lon)
-        
+        mod = m.extractTimeSeries(
+            self.variable,
+            alt_vars=self.alternate_vars,
+            expression=self.derived,
+            convert_calendar=False,
+            lats=None if obs.spatial else obs.lat,
+            lons=None if obs.spatial else obs.lon,
+        )
+
         # Handle molar mass, migrate to ILAMB.Variable.convert()
-        if (np.any([Unit(u).is_convertible("g")   for u in mod.unit.split()]) and
-            np.any([Unit(u).is_convertible("mol") for u in obs.unit.split()])):
-            if self.variable in ["gpp","nee","reco"]:
+        if np.any([Unit(u).is_convertible("g") for u in mod.unit.split()]) and np.any(
+            [Unit(u).is_convertible("mol") for u in obs.unit.split()]
+        ):
+            if self.variable in ["gpp", "nee", "reco"]:
                 mod.unit = str(Unit(mod.unit) / Unit("12.0107 g mol-1"))
-            
+
         # When we make things comparable, sites can get pruned, we
         # also need to prune the site labels
-        lat = np.copy(obs.lat); lon = np.copy(obs.lon)
-        obs,mod = il.MakeComparable(obs,mod,clip_ref=True,prune_sites=True,allow_diff_times=True)
-        
-        return obs,mod
+        lat = np.copy(obs.lat)
+        lon = np.copy(obs.lon)
+        obs, mod = il.MakeComparable(
+            obs, mod, clip_ref=True, prune_sites=True, allow_diff_times=True
+        )
 
-    def confront(self,m):
+        return obs, mod
 
+    def confront(self, m):
         # get the HTML page
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
 
         # Grab the data
-        obs,mod = self.stageData(m)
-                 
+        obs, mod = self.stageData(m)
+
         # Number of data points per day
-        nobs = int(np.round(1./np.diff(obs.time).mean()))
-        nmod = int(np.round(1./np.diff(mod.time).mean()))
-        
+        nobs = int(np.round(1.0 / np.diff(obs.time).mean()))
+        nmod = int(np.round(1.0 / np.diff(mod.time).mean()))
+
         # Analysis on a per year basis
-        Tobs = cftime.num2date(obs.time,"days since 1850-1-1")
-        Tmod = cftime.num2date(mod.time,"days since 1850-1-1")
-        Yobs = np.asarray([t.year for t in Tobs],dtype=int)
-        Ymod = np.asarray([t.year for t in Tmod],dtype=int)
-        Y    = np.unique(Yobs)
-        S1 = []; S2 = []; S3 = []; Lobs = []; Lmod = []
-        Sobs  = {}; Smod  = {}
-        omask = np.zeros(obs.time.size,dtype=int)
+        Tobs = cftime.num2date(obs.time, "days since 1850-1-1")
+        Tmod = cftime.num2date(mod.time, "days since 1850-1-1")
+        Yobs = np.asarray([t.year for t in Tobs], dtype=int)
+        Ymod = np.asarray([t.year for t in Tmod], dtype=int)
+        Y = np.unique(Yobs)
+        S1 = []
+        S2 = []
+        S3 = []
+        Lobs = []
+        Lmod = []
+        Sobs = {}
+        Smod = {}
+        omask = np.zeros(obs.time.size, dtype=int)
         for y in Y:
-
             # datum for this year
-            datum = cftime.date2num(cftime.datetime(y,1,1),"days since 1850-1-1")
-            
+            datum = cftime.date2num(cftime.datetime(y, 1, 1), "days since 1850-1-1")
+
             # Reshape the year's worth of data
-            iobs = np.where(y==Yobs)[0]
-            imod = np.where(y==Ymod)[0]
-            if (iobs.size < 0.9*nobs*365): continue
-            if (imod.size < 0.9*nmod*365): continue
-            vobs,tobs = DiurnalReshape(obs.time     [iobs] - datum,
-                                       obs.time_bnds[iobs] - datum,
-                                       obs.data     [iobs,0])
-            vmod,tmod = DiurnalReshape(mod.time     [imod] - datum,
-                                       mod.time_bnds[imod] - datum,
-                                       mod.data     [imod,0])
+            iobs = np.where(y == Yobs)[0]
+            imod = np.where(y == Ymod)[0]
+            if iobs.size < 0.9 * nobs * 365:
+                continue
+            if imod.size < 0.9 * nmod * 365:
+                continue
+            vobs, tobs = DiurnalReshape(
+                obs.time[iobs] - datum, obs.time_bnds[iobs] - datum, obs.data[iobs, 0]
+            )
+            vmod, tmod = DiurnalReshape(
+                mod.time[imod] - datum, mod.time_bnds[imod] - datum, mod.data[imod, 0]
+            )
 
             # Compute the diurnal magnitude
-            vobs = vobs.max(axis=1)-vobs.min(axis=1)
-            vmod = vmod.max(axis=1)-vmod.min(axis=1)
-            Sobs[y] = Variable(name = "season_%d" % y,
-                               unit = obs.unit,
-                               time = tobs,
-                               data = vobs)
-            Smod[y] = Variable(name = "season_%d" % y,
-                               unit = mod.unit,
-                               time = tmod,
-                               data = vmod)
+            vobs = vobs.max(axis=1) - vobs.min(axis=1)
+            vmod = vmod.max(axis=1) - vmod.min(axis=1)
+            Sobs[y] = Variable(
+                name="season_%d" % y, unit=obs.unit, time=tobs, data=vobs
+            )
+            Smod[y] = Variable(
+                name="season_%d" % y, unit=mod.unit, time=tmod, data=vmod
+            )
 
             # Compute metrics
-            To  = _findSeasonalTiming  (tobs,vobs)
-            Ro  = _findSeasonalCentroid(tobs,vobs)
-            Tm  = _findSeasonalTiming  (tmod,vmod)
-            Rm  = _findSeasonalCentroid(tmod,vmod)
-            dTo = To[1]-To[0]       # season length of the observation
-            a   = np.log(0.1) / 0.5 # 50% relative error equals a score of 1/10
-            s1  = np.exp(a* np.abs(To[0]-Tm[0])/dTo)
-            s2  = np.exp(a* np.abs(To[1]-Tm[1])/dTo)
-            s3  = np.linalg.norm(np.asarray([Ro[2]-Rm[2],Ro[3]-Rm[3]])) #  |Ro - Rm|
-            den = np.linalg.norm(np.asarray([      Ro[2],      Ro[3]])) # /|Ro|
+            To = _findSeasonalTiming(tobs, vobs)
+            Ro = _findSeasonalCentroid(tobs, vobs)
+            Tm = _findSeasonalTiming(tmod, vmod)
+            Rm = _findSeasonalCentroid(tmod, vmod)
+            dTo = To[1] - To[0]  # season length of the observation
+            a = np.log(0.1) / 0.5  # 50% relative error equals a score of 1/10
+            s1 = np.exp(a * np.abs(To[0] - Tm[0]) / dTo)
+            s2 = np.exp(a * np.abs(To[1] - Tm[1]) / dTo)
+            s3 = np.linalg.norm(
+                np.asarray([Ro[2] - Rm[2], Ro[3] - Rm[3]])
+            )  #  |Ro - Rm|
+            den = np.linalg.norm(np.asarray([Ro[2], Ro[3]]))  # /|Ro|
             if den < 1e-12 or np.isnan(den):
-                s3  = 0.
+                s3 = 0.0
             else:
                 s3 /= den
-                s3  = np.exp(-s3)
-            S1.append(s1); S2.append(s2); S3.append(s3)
-            Lobs.append(To[1]-To[0])
-            Lmod.append(Tm[1]-Tm[0])
+                s3 = np.exp(-s3)
+            S1.append(s1)
+            S2.append(s2)
+            S3.append(s3)
+            Lobs.append(To[1] - To[0])
+            Lmod.append(Tm[1] - Tm[0])
 
             # mask away the off season
-            obs.data.mask[:,0] += (y == Yobs)*(((obs.time-datum) < To[0]) + ((obs.time-datum) > To[1]))
-            mod.data.mask[:,0] += (y == Ymod)*(((mod.time-datum) < Tm[0]) + ((mod.time-datum) > Tm[1]))
+            obs.data.mask[:, 0] += (y == Yobs) * (
+                ((obs.time - datum) < To[0]) + ((obs.time - datum) > To[1])
+            )
+            mod.data.mask[:, 0] += (y == Ymod) * (
+                ((mod.time - datum) < Tm[0]) + ((mod.time - datum) > Tm[1])
+            )
 
         # Seasonal Mean Diurnal Cycle
-        ot,omean,o10,o90,opeak = _meanDiurnalCycle(obs,nobs)
-        mt,mmean,m10,m90,mpeak = _meanDiurnalCycle(mod,nmod)
+        ot, omean, o10, o90, opeak = _meanDiurnalCycle(obs, nobs)
+        mt, mmean, m10, m90, mpeak = _meanDiurnalCycle(mod, nmod)
 
         # Seasonal Mean Daily Uptake
         ouptake = obs.integrateInTime(mean=True)
         muptake = mod.integrateInTime(mean=True)
 
         # Score by mean values across years
-        S1   = np.asarray(S1  ).mean()
-        S2   = np.asarray(S2  ).mean()
-        S3   = np.asarray(S3  ).mean()
-        S4   = abs(opeak-mpeak)/12.
-        S4   = 1 - ((S4>1)*(1-S4) + (S4<=1)*S4)
-        S5   = np.exp(-np.abs(ouptake.data-muptake.data)/ouptake.data)
+        S1 = np.asarray(S1).mean()
+        S2 = np.asarray(S2).mean()
+        S3 = np.asarray(S3).mean()
+        S4 = abs(opeak - mpeak) / 12.0
+        S4 = 1 - ((S4 > 1) * (1 - S4) + (S4 <= 1) * S4)
+        S5 = np.exp(-np.abs(ouptake.data - muptake.data) / ouptake.data)
         Lobs = np.asarray(Lobs).mean()
         Lmod = np.asarray(Lmod).mean()
 
-        with Dataset(os.path.join(self.output_path,"%s_%s.nc" % (self.name,m.name)),mode="w") as results:
-            results.setncatts({"name" :m.name, "color":m.color,"weight":self.cweight})
-            Variable(name = "Season Length global",
-                     unit = "d",
-                     data = Lmod).toNetCDF4(results,group="MeanState")
-            Variable(name = "Season Beginning Score global",
-                     unit = "1",
-                     data = S1).toNetCDF4(results,group="MeanState")
-            Variable(name = "Season Ending Score global",
-                     unit = "1",
-                     data = S2).toNetCDF4(results,group="MeanState")
-            Variable(name = "Season Strength Score global",
-                     unit = "1",
-                     data = S3).toNetCDF4(results,group="MeanState")
-            Variable(name = "Diurnal Max Timing Score global",
-                     unit = "1",
-                     data = S4).toNetCDF4(results,group="MeanState")
-            Variable(name = "Daily Uptake Score global",
-                     unit = "1",
-                     data = S5).toNetCDF4(results,group="MeanState")
-            Variable(name = "cycle_mean",
-                     unit = mod.unit,
-                     time = mt,
-                     data = mmean).toNetCDF4(results,group="MeanState")
-            Variable(name = "cycle_lower",
-                     unit = mod.unit,
-                     time = mt,
-                     data = m10).toNetCDF4(results,group="MeanState")
-            Variable(name = "cycle_upper",
-                     unit = mod.unit,
-                     time = mt,
-                     data = m90).toNetCDF4(results,group="MeanState")
-            Variable(name = "Season Mean Daily Uptake",
-                     unit = muptake.unit,
-                     data = muptake.data).toNetCDF4(results,group="MeanState")
-            Variable(name = "Season Time of Maximum",
-                     unit = "h",
-                     data = mpeak).toNetCDF4(results,group="MeanState")
-            for key in Smod.keys(): Smod[key].toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
-        if not self.master: return
-        with Dataset(os.path.join(self.output_path,"%s_Benchmark.nc" % self.name),mode="w") as results:
-            results.setncatts({"name" :"Benchmark", "color":np.asarray([0.5,0.5,0.5]), "weight":self.cweight})
-            Variable(name = "Season Length global",
-                     unit = "d",
-                     data = Lobs).toNetCDF4(results,group="MeanState")
-            Variable(name = "cycle_mean",
-                     unit = obs.unit,
-                     time = ot,
-                     data = omean).toNetCDF4(results,group="MeanState")
-            Variable(name = "cycle_lower",
-                     unit = obs.unit,
-                     time = ot,
-                     data = o10).toNetCDF4(results,group="MeanState")
-            Variable(name = "cycle_upper",
-                     unit = obs.unit,
-                     time = ot,
-                     data = o90).toNetCDF4(results,group="MeanState")
-            Variable(name = "Season Mean Daily Uptake",
-                     unit = ouptake.unit,
-                     data = ouptake.data).toNetCDF4(results,group="MeanState")
-            Variable(name = "Season Time of Maximum",
-                     unit = "h",
-                     data = opeak).toNetCDF4(results,group="MeanState")
-            for key in Sobs.keys(): Sobs[key].toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
+        with Dataset(
+            os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name)), mode="w"
+        ) as results:
+            results.setncatts(
+                {"name": m.name, "color": m.color, "weight": self.cweight}
+            )
+            Variable(name="Season Length global", unit="d", data=Lmod).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="Season Beginning Score global", unit="1", data=S1).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="Season Ending Score global", unit="1", data=S2).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="Season Strength Score global", unit="1", data=S3).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(
+                name="Diurnal Max Timing Score global", unit="1", data=S4
+            ).toNetCDF4(results, group="MeanState")
+            Variable(name="Daily Uptake Score global", unit="1", data=S5).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="cycle_mean", unit=mod.unit, time=mt, data=mmean).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="cycle_lower", unit=mod.unit, time=mt, data=m10).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="cycle_upper", unit=mod.unit, time=mt, data=m90).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(
+                name="Season Mean Daily Uptake", unit=muptake.unit, data=muptake.data
+            ).toNetCDF4(results, group="MeanState")
+            Variable(name="Season Time of Maximum", unit="h", data=mpeak).toNetCDF4(
+                results, group="MeanState"
+            )
+            for key in Smod.keys():
+                Smod[key].toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
+        if not self.master:
+            return
+        with Dataset(
+            os.path.join(self.output_path, "%s_Benchmark.nc" % self.name), mode="w"
+        ) as results:
+            results.setncatts(
+                {
+                    "name": "Benchmark",
+                    "color": np.asarray([0.5, 0.5, 0.5]),
+                    "weight": self.cweight,
+                }
+            )
+            Variable(name="Season Length global", unit="d", data=Lobs).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="cycle_mean", unit=obs.unit, time=ot, data=omean).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="cycle_lower", unit=obs.unit, time=ot, data=o10).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(name="cycle_upper", unit=obs.unit, time=ot, data=o90).toNetCDF4(
+                results, group="MeanState"
+            )
+            Variable(
+                name="Season Mean Daily Uptake", unit=ouptake.unit, data=ouptake.data
+            ).toNetCDF4(results, group="MeanState")
+            Variable(name="Season Time of Maximum", unit="h", data=opeak).toNetCDF4(
+                results, group="MeanState"
+            )
+            for key in Sobs.keys():
+                Sobs[key].toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
 
     def determinePlotLimits(self):
-
         self.limits = {}
-        self.limits["season"] = 0.
-        for fname in glob.glob(os.path.join(self.output_path,"*.nc")):
+        self.limits["season"] = 0.0
+        for fname in glob.glob(os.path.join(self.output_path, "*.nc")):
             with Dataset(fname) as dataset:
-                if "MeanState" not in dataset.groups: continue
-                group     = dataset.groups["MeanState"]
-                variables = [v for v in group.variables.keys() if v not in group.dimensions.keys()]
+                if "MeanState" not in dataset.groups:
+                    continue
+                group = dataset.groups["MeanState"]
+                variables = [
+                    v
+                    for v in group.variables.keys()
+                    if v not in group.dimensions.keys()
+                ]
                 for vname in variables:
                     if "season" in vname:
-                        self.limits["season"] = max(self.limits["season"],group.variables[vname].up99)
-
-    def modelPlots(self,m):
-
-        bname  = "%s/%s_Benchmark.nc" % (self.output_path,self.name)
-        fname  = "%s/%s_%s.nc" % (self.output_path,self.name,m.name)
-        if not os.path.isfile(bname): return
-        if not os.path.isfile(fname): return
+                        self.limits["season"] = max(
+                            self.limits["season"], group.variables[vname].up99
+                        )
+
+    def modelPlots(self, m):
+        bname = "%s/%s_Benchmark.nc" % (self.output_path, self.name)
+        fname = "%s/%s_%s.nc" % (self.output_path, self.name, m.name)
+        if not os.path.isfile(bname):
+            return
+        if not os.path.isfile(fname):
+            return
 
         # get the HTML page
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
-        page.priority = ["Beginning","Ending","Strength","Score","Overall"]
+        page.priority = ["Beginning", "Ending", "Strength", "Score", "Overall"]
 
         # list pf plots must be in both the benchmark and the model
         with Dataset(bname) as dset:
-            bplts = [key for key in dset.groups["MeanState"].variables.keys() if "season" in key]
+            bplts = [
+                key
+                for key in dset.groups["MeanState"].variables.keys()
+                if "season" in key
+            ]
         with Dataset(fname) as dset:
-            fplts = [key for key in dset.groups["MeanState"].variables.keys() if "season" in key]
+            fplts = [
+                key
+                for key in dset.groups["MeanState"].variables.keys()
+                if "season" in key
+            ]
         plots = [v for v in bplts if v in fplts]
         plots.sort()
 
         for plot in plots:
+            obs = Variable(filename=bname, variable_name=plot, groupname="MeanState")
+            mod = Variable(filename=fname, variable_name=plot, groupname="MeanState")
 
-            obs = Variable(filename = bname, variable_name = plot, groupname = "MeanState")
-            mod = Variable(filename = fname, variable_name = plot, groupname = "MeanState")
-
-            page.addFigure("Diurnal magnitude",
-                           plot,
-                           "MNAME_%s.png" % plot,
-                           side   = plot.split("_")[-1],
-                           legend = False)
-            plt.figure(figsize=(5,5),tight_layout=True)
-            plt.polar(obs.time/365.*2*np.pi,obs.data,'-k',alpha=0.6,lw=2)
-            plt.polar(mod.time/365.*2*np.pi,mod.data,'-',color=m.color)
-            plt.xticks(bnd_months[:-1]/365.*2*np.pi,lbl_months)
-            plt.ylim(0,self.limits["season"])
-            plt.savefig("%s/%s_%s.png" % (self.output_path,m.name,plot))
+            page.addFigure(
+                "Diurnal magnitude",
+                plot,
+                "MNAME_%s.png" % plot,
+                side=plot.split("_")[-1],
+                legend=False,
+            )
+            plt.figure(figsize=(5, 5), tight_layout=True)
+            plt.polar(obs.time / 365.0 * 2 * np.pi, obs.data, "-k", alpha=0.6, lw=2)
+            plt.polar(mod.time / 365.0 * 2 * np.pi, mod.data, "-", color=m.color)
+            plt.xticks(bnd_months[:-1] / 365.0 * 2 * np.pi, lbl_months)
+            plt.ylim(0, self.limits["season"])
+            plt.savefig("%s/%s_%s.png" % (self.output_path, m.name, plot))
             plt.close()
 
         # mean Diurnal Cycle
-        obs = Variable(filename = bname, variable_name = "cycle_mean" , groupname = "MeanState")
-        olo = Variable(filename = bname, variable_name = "cycle_lower", groupname = "MeanState")
-        ohi = Variable(filename = bname, variable_name = "cycle_upper", groupname = "MeanState")
-        mod = Variable(filename = fname, variable_name = "cycle_mean" , groupname = "MeanState")
-        mlo = Variable(filename = fname, variable_name = "cycle_lower", groupname = "MeanState")
-        mhi = Variable(filename = fname, variable_name = "cycle_upper", groupname = "MeanState")
-        fig,ax = plt.subplots(figsize=(8,4.5),tight_layout=True)
+        obs = Variable(
+            filename=bname, variable_name="cycle_mean", groupname="MeanState"
+        )
+        olo = Variable(
+            filename=bname, variable_name="cycle_lower", groupname="MeanState"
+        )
+        ohi = Variable(
+            filename=bname, variable_name="cycle_upper", groupname="MeanState"
+        )
+        mod = Variable(
+            filename=fname, variable_name="cycle_mean", groupname="MeanState"
+        )
+        mlo = Variable(
+            filename=fname, variable_name="cycle_lower", groupname="MeanState"
+        )
+        mhi = Variable(
+            filename=fname, variable_name="cycle_upper", groupname="MeanState"
+        )
+        fig, ax = plt.subplots(figsize=(8, 4.5), tight_layout=True)
         dt = np.diff(obs.time).mean()
-        ax.plot        (obs.time+0.5*dt,obs.data,color='k',alpha=0.5,lw=2)
-        ax.fill_between(obs.time+0.5*dt,olo.data,ohi.data,color='k',alpha=0.09,lw=0)
+        ax.plot(obs.time + 0.5 * dt, obs.data, color="k", alpha=0.5, lw=2)
+        ax.fill_between(
+            obs.time + 0.5 * dt, olo.data, ohi.data, color="k", alpha=0.09, lw=0
+        )
         dt = np.diff(mod.time).mean()
-        ax.plot        (mod.time+0.5*dt,mod.data,color=m.color,lw=2)
-        ax.fill_between(mod.time+0.5*dt,mlo.data,mhi.data,color=m.color,alpha=0.15,lw=0)
-        xticks      = np.linspace(0,24,9)
+        ax.plot(mod.time + 0.5 * dt, mod.data, color=m.color, lw=2)
+        ax.fill_between(
+            mod.time + 0.5 * dt, mlo.data, mhi.data, color=m.color, alpha=0.15, lw=0
+        )
+        xticks = np.linspace(0, 24, 9)
         xticklabels = ["%2d:00" % t for t in xticks]
         ax.set_xticks(xticks)
         ax.set_xticklabels(xticklabels)
         ax.grid(True)
         ax.set_ylabel(post.UnitStringToMatplotlib(obs.unit))
         ax.set_xlabel("local time")
-        plt.savefig("%s/%s_cycle.png" % (self.output_path,m.name))
+        plt.savefig("%s/%s_cycle.png" % (self.output_path, m.name))
         plt.close()
-        page.addFigure("Seasonal Diurnal Cycle",
-                       "cycle",
-                       "MNAME_cycle.png",
-                       side   = "CYCLE",
-                       legend = False)
+        page.addFigure(
+            "Seasonal Diurnal Cycle",
+            "cycle",
+            "MNAME_cycle.png",
+            side="CYCLE",
+            legend=False,
+        )
 
     def compositePlots(self):
         pass
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfEvapFraction.py` & `ILAMB-2.7/src/ILAMB/ConfEvapFraction.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,185 +1,232 @@
-from .Confrontation import Confrontation
-from .Variable import Variable
-from netCDF4 import Dataset
-from . import ilamblib as il
-import numpy as np
+import logging
 import os
+
+import numpy as np
 from mpi4py import MPI
 
-import logging
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Variable import Variable
+
 logger = logging.getLogger("%i" % MPI.COMM_WORLD.rank)
 
-def _evapfrac(sh,le,vname,energy_threshold):
-    mask = ((le.data<0)+
-            (sh.data<0)+
-            ((le.data+sh.data)<energy_threshold))
-    sh.data = np.ma.masked_array(sh.data,mask=mask)
-    le.data = np.ma.masked_array(le.data,mask=mask)
-    np.seterr(over='ignore',under='ignore')
-    ef      = np.ma.masked_array(le.data/(le.data+sh.data),mask=mask)
-    np.seterr(over='warn',under='warn')
-    ef      = Variable(name      = vname,
-                       unit      = "1",
-                       data      = ef,
-                       lat       = sh.lat,
-                       lat_bnds  = sh.lat_bnds,
-                       lon       = sh.lon,
-                       lon_bnds  = sh.lon_bnds,
-                       time      = sh.time,
-                       time_bnds = sh.time_bnds)
-    return sh,le,ef
 
-class ConfEvapFraction(Confrontation):
+def _evapfrac(sh, le, vname, energy_threshold):
+    mask = (le.data < 0) + (sh.data < 0) + ((le.data + sh.data) < energy_threshold)
+    sh.data = np.ma.masked_array(sh.data, mask=mask)
+    le.data = np.ma.masked_array(le.data, mask=mask)
+    np.seterr(over="ignore", under="ignore")
+    ef = np.ma.masked_array(le.data / (le.data + sh.data), mask=mask)
+    np.seterr(over="warn", under="warn")
+    ef = Variable(
+        name=vname,
+        unit="1",
+        data=ef,
+        lat=sh.lat,
+        lat_bnds=sh.lat_bnds,
+        lon=sh.lon,
+        lon_bnds=sh.lon_bnds,
+        time=sh.time,
+        time_bnds=sh.time_bnds,
+    )
+    return sh, le, ef
+
 
-    def __init__(self,**keywords):
-        if not ('hfss_source' in keywords and 'hfls_source' in keywords):
+class ConfEvapFraction(Confrontation):
+    def __init__(self, **keywords):
+        if not ("hfss_source" in keywords and "hfls_source" in keywords):
             msg = "This confrontation requires two data sources 'hfss_source' and 'hfls_source'"
             raise il.MisplacedData(msg)
-        for key in ['hfss_source','hfls_source']:
-            keywords[key] = os.path.join(os.environ["ILAMB_ROOT"],keywords[key])
-        keywords['source'] = keywords['hfss_source']
-        self.source = keywords['source']
-        super(ConfEvapFraction,self).__init__(**keywords)
-        self.derived = "hfss + hfls" # just for ilamb-doctor to detect required symbols
-        
-    def stageData(self,m):
+        for key in ["hfss_source", "hfls_source"]:
+            keywords[key] = os.path.join(os.environ["ILAMB_ROOT"], keywords[key])
+        keywords["source"] = keywords["hfss_source"]
+        self.source = keywords["source"]
+        super(ConfEvapFraction, self).__init__(**keywords)
+        self.derived = "hfss + hfls"  # just for ilamb-doctor to detect required symbols
+
+    def stageData(self, m):
+        energy_threshold = float(self.keywords.get("energy_threshold", 20.0))  # W m-2
 
-        energy_threshold = float(self.keywords.get("energy_threshold",20.)) # W m-2
-        
         # Handle obs data
-        sh_obs = Variable(filename = self.keywords["hfss_source"],
-                          variable_name = "hfss",alternate_vars=['sh'],
-                          t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                          tf = None if len(self.study_limits) != 2 else self.study_limits[1]).convert("W m-2")
-        le_obs = Variable(filename = self.keywords["hfls_source"],
-                          variable_name = "hfls",alternate_vars=['le'],
-                          t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                          tf = None if len(self.study_limits) != 2 else self.study_limits[1]).convert("W m-2")
-        sh_obs,le_obs,obs = _evapfrac(sh_obs,le_obs,self.variable,energy_threshold)
+        sh_obs = Variable(
+            filename=self.keywords["hfss_source"],
+            variable_name="hfss",
+            alternate_vars=["sh"],
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        ).convert("W m-2")
+        le_obs = Variable(
+            filename=self.keywords["hfls_source"],
+            variable_name="hfls",
+            alternate_vars=["le"],
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        ).convert("W m-2")
+        sh_obs, le_obs, obs = _evapfrac(sh_obs, le_obs, self.variable, energy_threshold)
 
         # Prune out uncovered regions
-        if obs.time is None: raise il.NotTemporalVariable()
+        if obs.time is None:
+            raise il.NotTemporalVariable()
         self.pruneRegions(obs)
 
         # Handle model data
-        sh_mod = m.extractTimeSeries("hfss",
-                                     alt_vars = ["FSH"],
-                                     initial_time = obs.time_bnds[ 0,0],
-                                     final_time   = obs.time_bnds[-1,1],
-                                     lats         = None if obs.spatial else obs.lat,
-                                     lons         = None if obs.spatial else obs.lon)
-        le_mod = m.extractTimeSeries("hfls",
-                                     alt_vars = ["EFLX_LH_TOT"],
-                                     initial_time = obs.time_bnds[ 0,0],
-                                     final_time   = obs.time_bnds[-1,1],
-                                     lats         = None if obs.spatial else obs.lat,
-                                     lons         = None if obs.spatial else obs.lon)
-        sh_mod,le_mod,mod = _evapfrac(sh_mod,le_mod,self.variable,energy_threshold)
+        sh_mod = m.extractTimeSeries(
+            "hfss",
+            alt_vars=["FSH"],
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+            lats=None if obs.spatial else obs.lat,
+            lons=None if obs.spatial else obs.lon,
+        )
+        le_mod = m.extractTimeSeries(
+            "hfls",
+            alt_vars=["EFLX_LH_TOT"],
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+            lats=None if obs.spatial else obs.lat,
+            lons=None if obs.spatial else obs.lon,
+        )
+        sh_mod, le_mod, mod = _evapfrac(sh_mod, le_mod, self.variable, energy_threshold)
 
         # Make variables comparable
-        obs,mod = il.MakeComparable(obs,mod,
-                                    mask_ref  = True,
-                                    clip_ref  = True,
-                                    logstring = "[%s][%s]" % (self.longname,m.name))
-        sh_obs,sh_mod = il.MakeComparable(sh_obs,sh_mod,
-                                          mask_ref  = True,
-                                          clip_ref  = True,
-                                          logstring = "[%s][%s]" % (self.longname,m.name))
-        le_obs,le_mod = il.MakeComparable(le_obs,le_mod,
-                                          mask_ref  = True,
-                                          clip_ref  = True,
-                                          logstring = "[%s][%s]" % (self.longname,m.name))
+        obs, mod = il.MakeComparable(
+            obs,
+            mod,
+            mask_ref=True,
+            clip_ref=True,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
+        sh_obs, sh_mod = il.MakeComparable(
+            sh_obs,
+            sh_mod,
+            mask_ref=True,
+            clip_ref=True,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
+        le_obs, le_mod = il.MakeComparable(
+            le_obs,
+            le_mod,
+            mask_ref=True,
+            clip_ref=True,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
 
         # Compute the mean ef
         sh_obs = sh_obs.integrateInTime(mean=True)
         le_obs = le_obs.integrateInTime(mean=True)
-        np.seterr(over='ignore',under='ignore')
-        obs_timeint = np.ma.masked_array(le_obs.data/(le_obs.data+sh_obs.data),mask=(sh_obs.data.mask+le_obs.data.mask))
-        np.seterr(over='warn',under='warn')
-        obs_timeint = Variable(name      = self.variable,
-                               unit      = "1",
-                               data      = obs_timeint,
-                               lat       = sh_obs.lat,
-                               lat_bnds  = sh_obs.lat_bnds,
-                               lon       = sh_obs.lon,
-                               lon_bnds  = sh_obs.lon_bnds)
+        np.seterr(over="ignore", under="ignore")
+        obs_timeint = np.ma.masked_array(
+            le_obs.data / (le_obs.data + sh_obs.data),
+            mask=(sh_obs.data.mask + le_obs.data.mask),
+        )
+        np.seterr(over="warn", under="warn")
+        obs_timeint = Variable(
+            name=self.variable,
+            unit="1",
+            data=obs_timeint,
+            lat=sh_obs.lat,
+            lat_bnds=sh_obs.lat_bnds,
+            lon=sh_obs.lon,
+            lon_bnds=sh_obs.lon_bnds,
+        )
         sh_mod = sh_mod.integrateInTime(mean=True)
         le_mod = le_mod.integrateInTime(mean=True)
-        np.seterr(over='ignore',under='ignore')
-        mod_timeint = np.ma.masked_array(le_mod.data/(le_mod.data+sh_mod.data),mask=(sh_mod.data.mask+le_mod.data.mask))
-        np.seterr(over='warn',under='warn')
-        mod_timeint = Variable(name      = self.variable,
-                               unit      = "1",
-                               data      = mod_timeint,
-                               lat       = sh_mod.lat,
-                               lat_bnds  = sh_mod.lat_bnds,
-                               lon       = sh_mod.lon,
-                               lon_bnds  = sh_mod.lon_bnds)
+        np.seterr(over="ignore", under="ignore")
+        mod_timeint = np.ma.masked_array(
+            le_mod.data / (le_mod.data + sh_mod.data),
+            mask=(sh_mod.data.mask + le_mod.data.mask),
+        )
+        np.seterr(over="warn", under="warn")
+        mod_timeint = Variable(
+            name=self.variable,
+            unit="1",
+            data=mod_timeint,
+            lat=sh_mod.lat,
+            lat_bnds=sh_mod.lat_bnds,
+            lon=sh_mod.lon,
+            lon_bnds=sh_mod.lon_bnds,
+        )
 
-        return obs,mod,obs_timeint,mod_timeint
+        return obs, mod, obs_timeint, mod_timeint
 
     def requires(self):
-        return ['hfss','hfls'],[]
+        return ["hfss", "hfls"], []
 
-    def confront(self,m):
+    def confront(self, m):
         r"""Confronts the input model with the observational data.
 
         This routine is exactly the same as Confrontation except that
         user-provided period means are passed as options to the analysis.
 
         Parameters
         ----------
         m : ILAMB.ModelResult.ModelResult
             the model results
 
         """
         # Grab the data
-        obs,mod,obs_timeint,mod_timeint = self.stageData(m)
-
-        mod_file = os.path.join(self.output_path,"%s_%s.nc"        % (self.name,m.name))
-        obs_file = os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name,      ))
-        with il.FileContextManager(self.master,mod_file,obs_file) as fcm:
+        obs, mod, obs_timeint, mod_timeint = self.stageData(m)
 
+        mod_file = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
+        obs_file = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name,))
+        with il.FileContextManager(self.master, mod_file, obs_file) as fcm:
             # Encode some names and colors
-            fcm.mod_dset.setncatts({"name" :m.name,
-                                    "color":m.color,
-                                    "weight":self.cweight,                                    
-                                    "complete":0})
+            fcm.mod_dset.setncatts(
+                {
+                    "name": m.name,
+                    "color": m.color,
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
             if self.master:
-                fcm.obs_dset.setncatts({"name" :"Benchmark",
-                                        "color":np.asarray([0.5,0.5,0.5]),
-                                        "weight":self.cweight,
-                                        "complete":0})
+                fcm.obs_dset.setncatts(
+                    {
+                        "name": "Benchmark",
+                        "color": np.asarray([0.5, 0.5, 0.5]),
+                        "weight": self.cweight,
+                        "complete": 0,
+                    }
+                )
 
             # Read in some options and run the mean state analysis
-            mass_weighting = self.keywords.get("mass_weighting",False)
-            skip_rmse      = self.keywords.get("skip_rmse"     ,False)
-            skip_iav       = self.keywords.get("skip_iav"      ,True )
-            skip_cycle     = self.keywords.get("skip_cycle"    ,False)
+            mass_weighting = self.keywords.get("mass_weighting", False)
+            skip_rmse = self.keywords.get("skip_rmse", False)
+            skip_iav = self.keywords.get("skip_iav", True)
+            skip_cycle = self.keywords.get("skip_cycle", False)
             if obs.spatial:
-                il.AnalysisMeanStateSpace(obs,mod,dataset   = fcm.mod_dset,
-                                          regions           = self.regions,
-                                          benchmark_dataset = fcm.obs_dset,
-                                          table_unit        = self.table_unit,
-                                          plot_unit         = self.plot_unit,
-                                          space_mean        = self.space_mean,
-                                          skip_rmse         = skip_rmse,
-                                          skip_iav          = skip_iav,
-                                          skip_cycle        = skip_cycle,
-                                          mass_weighting    = mass_weighting,
-                                          ref_timeint       = obs_timeint,
-                                          com_timeint       = mod_timeint)
+                il.AnalysisMeanStateSpace(
+                    obs,
+                    mod,
+                    dataset=fcm.mod_dset,
+                    regions=self.regions,
+                    benchmark_dataset=fcm.obs_dset,
+                    table_unit=self.table_unit,
+                    plot_unit=self.plot_unit,
+                    space_mean=self.space_mean,
+                    skip_rmse=skip_rmse,
+                    skip_iav=skip_iav,
+                    skip_cycle=skip_cycle,
+                    mass_weighting=mass_weighting,
+                    ref_timeint=obs_timeint,
+                    com_timeint=mod_timeint,
+                )
             else:
-                il.AnalysisMeanStateSites(obs,mod,dataset   = fcm.mod_dset,
-                                          regions           = self.regions,
-                                          benchmark_dataset = fcm.obs_dset,
-                                          table_unit        = self.table_unit,
-                                          plot_unit         = self.plot_unit,
-                                          space_mean        = self.space_mean,
-                                          skip_rmse         = skip_rmse,
-                                          skip_iav          = skip_iav,
-                                          skip_cycle        = skip_cycle,
-                                          mass_weighting    = mass_weighting)
-            fcm.mod_dset.setncattr("complete",1)
-            if self.master: fcm.obs_dset.setncattr("complete",1)
-        logger.info("[%s][%s] Success" % (self.longname,m.name))
+                il.AnalysisMeanStateSites(
+                    obs,
+                    mod,
+                    dataset=fcm.mod_dset,
+                    regions=self.regions,
+                    benchmark_dataset=fcm.obs_dset,
+                    table_unit=self.table_unit,
+                    plot_unit=self.plot_unit,
+                    space_mean=self.space_mean,
+                    skip_rmse=skip_rmse,
+                    skip_iav=skip_iav,
+                    skip_cycle=skip_cycle,
+                    mass_weighting=mass_weighting,
+                )
+            fcm.mod_dset.setncattr("complete", 1)
+            if self.master:
+                fcm.obs_dset.setncattr("complete", 1)
+        logger.info("[%s][%s] Success" % (self.longname, m.name))
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfIOMB.py` & `ILAMB-2.7/src/ILAMB/Confrontation.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,884 +1,1342 @@
-from .Confrontation import getVariableList
-from .Confrontation import Confrontation
-from .constants import earth_rad,mid_months,lbl_months,bnd_months
-from .Variable import Variable
-from .Regions import Regions
-from . import ilamblib as il
-from . import Post as post
-from netCDF4 import Dataset
-from copy import deepcopy
-import pylab as plt
-import numpy as np
-import os,glob,re
-from sympy import sympify
-
-from mpi4py import MPI
+import glob
 import logging
-logger = logging.getLogger("%i" % MPI.COMM_WORLD.rank)
+import os
+import re
 
-def VariableReduce(var,region="global",time=None,depth=None,lat=None,lon=None):
-    ILAMBregions = Regions()
-    out = deepcopy(var)
-    out.data.mask += ILAMBregions.getMask(region,out)
-    if time  is not None:
-        out = out.integrateInTime (t0=time[0] ,tf=time[1] ,mean=True)
-    if depth is not None and var.layered:
-        out = out.integrateInDepth(z0=depth[0],zf=depth[1],mean=True)
-    if lat   is not None:
-        lat0        = np.argmin(np.abs(var.lat-lat[0]))
-        latf        = np.argmin(np.abs(var.lat-lat[1]))+1
-        wgt         = earth_rad*(np.sin(var.lat_bnds[:,1])-np.sin(var.lat_bnds[:,0]))[lat0:latf]
-        np.seterr(over='ignore',under='ignore')
-        out.data    = np.ma.average(out.data[...,lat0:latf,:],axis=-2,weights=wgt/wgt.sum())
-        np.seterr(over='raise',under='raise')
-        out.lat     = None
-        out.lat_bnd = None
-        out.spatial = False
-    if lon   is not None:
-        lon0        = np.argmin(np.abs(var.lon-lon[0]))
-        lonf        = np.argmin(np.abs(var.lon-lon[1]))+1
-        wgt         = earth_rad*(var.lon_bnds[:,1]-var.lon_bnds[:,0])[lon0:lonf]
-        np.seterr(over='ignore',under='ignore')
-        out.data    = np.ma.average(out.data[...,lon0:lonf],axis=-1,weights=wgt/wgt.sum())
-        np.seterr(over='raise',under='raise')
-        out.lon     = None
-        out.lon_bnd = None
-        out.spatial = False
-    return out
-
-def TimeLatBias(ref,com):
-    # composite depth axis
-    d0 = max(ref.depth_bnds.min(),com.depth_bnds.min())
-    df = min(ref.depth_bnds.max(),com.depth_bnds.max())
-    d  = np.unique(np.hstack([ref.depth_bnds.flatten(),com.depth_bnds.flatten()]))
-    d  = d[(d>=d0)*(d<=df)]
-    db = np.asarray([d[:-1],d[1:]]).T
-    d  = db.mean(axis=1)
-    # composite lat axis
-    l0 = max(ref.lat_bnds.min(),com.lat_bnds.min())
-    lf = min(ref.lat_bnds.max(),com.lat_bnds.max())
-    l  = np.unique(np.hstack([ref.lat_bnds.flatten(),com.lat_bnds.flatten()]))
-    l  = l[(l>=l0)*(l<=lf)]
-    lb = np.asarray([l[:-1],l[1:]]).T
-    l  = lb.mean(axis=1)
-    # interpolation / difference
-    data  = il.NearestNeighborInterpolation(com.depth,com.lat,com.data,d,l)
-    data -= il.NearestNeighborInterpolation(ref.depth,ref.lat,ref.data,d,l)
-    area  = np.diff(db)[:,np.newaxis] * (earth_rad*(np.sin(lb[:,1])-np.sin(lb[:,0])))
-    return Variable(name  = ref.name.replace("timelonint","timelonbias"),
-                    unit  = ref.unit,
-                    data  = data,
-                    area  = area,
-                    lat   = l,
-                    depth = d,
-                    lat_bnds   = lb,
-                    depth_bnds = db)
-
-def CycleBias(ref,com):
-    # composite depth axis
-    d0 = max(ref.depth_bnds.min(),com.depth_bnds.min())
-    df = min(ref.depth_bnds.max(),com.depth_bnds.max())
-    d  = np.unique(np.hstack([ref.depth_bnds.flatten(),com.depth_bnds.flatten()]))
-    d  = d[(d>=d0)*(d<=df)]
-    db = np.asarray([d[:-1],d[1:]]).T
-    d  = db.mean(axis=1)
-    # interpolation / difference
-    data  = il.NearestNeighborInterpolation(com.time,com.depth,com.data,com.time,d)
-    data -= il.NearestNeighborInterpolation(ref.time,ref.depth,ref.data,ref.time,d)
-    return Variable(name  = ref.name.replace("cycle","cyclebias"),
-                    unit  = ref.unit,
-                    data  = data,
-                    time  = mid_months,
-                    time_bnds = np.asarray([bnd_months[:-1],bnd_months[1:]]).T,
-                    depth      = d,
-                    depth_bnds = db)
-
-class ConfIOMB(Confrontation):
+import cftime as cf
+import numpy as np
+import pylab as plt
+from mpi4py import MPI
+from netCDF4 import Dataset
+from sympy import sympify
 
-    def __init__(self,**keywords):
+from ILAMB import Post as post
+from ILAMB import ilamblib as il
+from ILAMB.constants import space_opts, time_opts
+from ILAMB.Regions import Regions
+from ILAMB.Relationship import Relationship
+from ILAMB.Variable import Variable
 
-        # Calls the regular constructor
-        super(ConfIOMB,self).__init__(**keywords)
+logger = logging.getLogger("%i" % MPI.COMM_WORLD.rank)
 
-        # Get/modify depths
-        self.depths = np.asarray(self.keywords.get("depths",[0,100,250]),dtype=float)
-        with Dataset(self.source) as dset:
-            v = dset.variables[self.variable]
-            depth_name = [d for d in v.dimensions if d in ["layer","depth"]]
 
-            if len(depth_name) == 0:
-                # if there is no depth dimension, we assume the data is surface
-                self.depths = np.asarray([0],dtype=float)
-            else:
-                # if there are depths, then make sure that the depths
-                # at which we will compute are in the range of depths
-                # of the data
-                depth_name = depth_name[0]
-                data = dset.variables[dset.variables[depth_name].bounds][...] if "bounds" in dset.variables[depth_name].ncattrs() else dset.variables[depth_name][...]
-                self.depths = self.depths[(self.depths>=data.min())*(self.depths<=data.max())]
+def getVariableList(dataset):
+    """Extracts the list of variables in the dataset that aren't
+    dimensions or the bounds of dimensions.
+
+    """
+    variables = [
+        v for v in dataset.variables.keys() if v not in dataset.dimensions.keys()
+    ]
+    for d in dataset.dimensions.keys():
+        try:
+            variables.pop(variables.index(dataset.variables[d].getncattr("bounds")))
+        except:
+            pass
+    return variables
+
+
+def replace_url(string):
+    url = re.findall(
+        "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+~]|[!*\(\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
+        string,
+    )
+    for u in url:
+        u_text = u
+        u_text = u_text.replace("https://doi.org/doi:", "doi:")
+        u_text = u_text.replace("https://doi.org/", "doi:")
+        u_text = u_text.replace("http://doi.org/doi:", "doi:")
+        u_text = u_text.replace("http://doi.org/", "doi:")
+        string = string.replace(u, "<a href='%s'>%s</a>" % (u, u_text))
+    # if no https link was found, then it may be a doi link which we
+    # want to hyperlink appropriately
+    if len(url) == 0:
+        url = re.findall(
+            "doi:(?:[a-zA-Z]|[0-9]|[$-_@.&+~]|[!*\(\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
+            string,
+        )
+        for u in url:
+            # if doi is in a text reference, it ends with a period that we do not want
+            u = u.strip(".")
+            u_link = u.replace("doi:", "https://doi.org/")
+            string = string.replace(u, "<a href='%s'>%s</a>" % (u_link, u))
+    return string
+
+
+def parse_bibtex(string):
+    citation = ""
+    entry_begins = []
+    for m in re.finditer("@", string):
+        entry_begins.append(m.start())
+    entry_begins.append(len(string))
+    for i in range(len(entry_begins) - 1):
+        entry = string[entry_begins[i] : entry_begins[i + 1]]
+        e = dict(re.findall(r"\s+(\w+)\s+=\s+\{(.*)\}", entry))
+        if i > 0:
+            citation += "<br>"
+        citation += "<dd>"
+        if "author" in e:
+            citation += e["author"]
+        if "year" in e:
+            citation += " (%s)" % e["year"]
+        if "title" in e:
+            citation += ", %s" % e["title"]
+        if "journal" in e:
+            citation += ", <i>%s</i>" % e["journal"]
+        if "number" in e:
+            citation += ", <i>%s</i>" % e["number"]
+        if "page" in e:
+            citation += ", %s" % e["page"]
+        if "doi" in e:
+            citation += ", %s" % replace_url(e["doi"])
+        citation += "</dd>"
+    return citation
+
+
+def create_data_header(attr, val):
+    vals = val.split(";")
+    html = "<p><dl><dt><b>&nbsp;&nbsp;%s:</dt></b>" % (attr.capitalize())
+    for v in vals:
+        v = v.strip()
+        if v.startswith("@"):
+            html += parse_bibtex(v)
+        else:
+            html += "<dd>%s</dd>" % (replace_url(v))
+    html += "</dl></p>"
+    return html
+
+
+class Confrontation(object):
+    """A generic class for confronting model results with observational data.
+
+    This class is meant to provide the user with a simple way to
+    specify observational datasets and compare them to model
+    results. A generic analysis routine is called which checks mean
+    states of the variables, afterwhich the results are tabulated and
+    plotted automatically. A HTML page is built dynamically as plots
+    are created based on available information and successful
+    analysis.
+
+    Parameters
+    ----------
+    name : str
+        a name for the confrontation
+    source : str
+        full path to the observational dataset
+    variable_name : str
+        name of the variable to extract from the source dataset
+    output_path : str, optional
+        path into which all output from this confrontation will be generated
+    alternate_vars : list of str, optional
+        other accepted variable names when extracting from models
+    derived : str, optional
+        an algebraic expression which captures how the confrontation variable may be generated
+    regions : list of str, optional
+        a list of regions over which the spatial analysis will be performed (default is global)
+    table_unit : str, optional
+        the unit to use in the output HTML table
+    plot_unit : str, optional
+        the unit to use in the output images
+    space_mean : bool, optional
+        enable to take spatial means (as opposed to spatial integrals) in the analysis (enabled by default)
+    relationships : list of ILAMB.Confrontation.Confrontation, optional
+        a list of confrontations with whose data we use to study relationships
+    cmap : str, optional
+        the colormap to use in rendering plots (default is 'jet')
+    land : str, bool
+        enable to force the masking of areas with no land (default is False)
+    limit_type : str
+        change the types of plot limits, one of ['minmax', '99per' (default)]
+    """
+
+    def __init__(self, **keywords):
+        # Initialize
+        self.master = True
+        self.name = keywords.get("name", None)
+        self.source = keywords.get("source", None)
+        self.variable = keywords.get("variable", None)
+        self.output_path = keywords.get("output_path", "./")
+        self.alternate_vars = keywords.get("alternate_vars", [])
+        self.derived = keywords.get("derived", None)
+        self.regions = list(keywords.get("regions", ["global"]))
+        self.data = None
+        self.cmap = keywords.get("cmap", "jet")
+        self.land = keywords.get("land", False)
+        self.limits = None
+        self.longname = self.output_path
+        self.longname = "/".join(
+            self.longname.replace("//", "/").rstrip("/").split("/")[-2:]
+        )
+        self.table_unit = keywords.get("table_unit", None)
+        self.plot_unit = keywords.get("plot_unit", None)
+        self.space_mean = keywords.get("space_mean", True)
+        self.relationships = keywords.get("relationships", None)
+        self.df_errs = keywords.get("df_errs", None)
+        self.keywords = keywords
+        self.extents = np.asarray([[-90.0, +90.0], [-180.0, +180.0]])
+        self.study_limits = []
+        self.cweight = 1
+        self.scale_factor = float(keywords.get("scale_factor", 1.0))
+
+        # Make sure the source data exists
+
+        if not os.path.isfile(self.source):
+            msg = (
+                "\n\nI am looking for data for the %s confrontation here\n\n"
+                % self.name
+            )
+            msg += "%s\n\nbut I cannot find it. " % self.source
+            msg += "Did you download the data? Have you set the ILAMB_ROOT envronment variable?\n"
+            raise il.MisplacedData(msg)
 
         # Setup a html layout for generating web views of the results
-        pages       = []
-        sections    = ["Period Mean at %d [m]" % d for d in self.depths]
-        sections   += ["Mean regional depth profiles"]
-        sections   += ["Overlapping mean regional depth profiles"]
-        sections   += ["Mean regional annual cycle"]
-        sections   += ["Overlapping mean regional annual cycle"]
-        pages.append(post.HtmlPage("MeanState","Mean State"))
+        pages = []
+
+        # Mean State page
+        pages.append(post.HtmlPage("MeanState", "Mean State"))
+        pages[-1].setHeader("CNAME / RNAME / MNAME")
+        pages[-1].setSections(
+            ["Temporally integrated period mean", "Spatially integrated regional mean"]
+        )
+
+        # Datasites page
+        self.hasSites = False
+        self.lbls = None
+        y0 = None
+        yf = None
+        with Dataset(self.source) as dataset:
+            if "data" in dataset.dimensions:
+                # self.hasSites = True
+                if "site_name" in dataset.ncattrs():
+                    self.lbls = dataset.site_name.split(",")
+                else:
+                    self.lbls = [
+                        "site%d" % s for s in range(len(dataset.dimensions["data"]))
+                    ]
+            if "time" in dataset.dimensions:
+                t = dataset.variables["time"]
+                tdata = t[[0, -1]]
+                if "bounds" in t.ncattrs():
+                    tdata = dataset.variables[t.bounds]
+                    tdata = [tdata[0, 0], tdata[-1, 1]]
+                tdata = cf.num2date(tdata, units=t.units, calendar=t.calendar)
+                y0 = tdata[0].year
+                yf = tdata[1].year
+
+        if self.hasSites:
+            pages.append(post.HtmlSitePlotsPage("SitePlots", "Site Plots"))
+            pages[-1].setHeader("CNAME / RNAME / MNAME")
+            pages[-1].setSections([])
+            var = Variable(
+                filename=self.source,
+                variable_name=self.variable,
+                alternate_vars=self.alternate_vars,
+            ).integrateInTime(mean=True)
+            if self.plot_unit is not None:
+                var.convert(self.plot_unit)
+            pages[-1].lat = var.lat
+            pages[-1].lon = var.lon
+            pages[-1].vname = self.variable
+            pages[-1].unit = var.unit
+            pages[-1].vals = var.data
+            pages[-1].sites = self.lbls
+
+        # Relationships page
+        if self.relationships is not None:
+            pages.append(post.HtmlPage("Relationships", "Relationships"))
+            pages[-1].setHeader("CNAME / RNAME / MNAME")
+            pages[-1].setSections(list(self.relationships))
+        pages.append(post.HtmlAllModelsPage("AllModels", "All Models"))
         pages[-1].setHeader("CNAME / RNAME / MNAME")
-        pages[-1].setSections(sections)
-        pages.append(post.HtmlAllModelsPage("AllModels","All Models"))
-        pages[-1].setHeader("CNAME / RNAME")
         pages[-1].setSections([])
         pages[-1].setRegions(self.regions)
-        pages.append(post.HtmlPage("DataInformation","Data Information"))
+        pages.append(post.HtmlPage("DataInformation", "Data Information"))
         pages[-1].setSections([])
         pages[-1].text = "\n"
         with Dataset(self.source) as dset:
-            for attr in dset.ncattrs():
-                pages[-1].text += "<p><b>&nbsp;&nbsp;%s:&nbsp;</b>%s</p>\n" % (attr,str(dset.getncattr(attr)).encode('ascii','ignore'))
-        self.layout = post.HtmlLayout(pages,self.longname)
 
-    def stageData(self,m):
+            def _attribute_sort(attr):
+                # If the attribute begins with one of the ones we
+                # specifically order, return the index into order. If
+                # it does not, return the number of entries in the
+                # list and the file's order will be preserved.
+                order = [
+                    "title",
+                    "version",
+                    "institution",
+                    "source",
+                    "history",
+                    "references",
+                    "comments",
+                    "convention",
+                ]
+                for i, a in enumerate(order):
+                    if attr.lower().startswith(a):
+                        return i
+                return len(order)
+
+            attrs = dset.ncattrs()
+            attrs = sorted(attrs, key=_attribute_sort)
+
+            for attr in attrs:
+                try:
+                    val = dset.getncattr(attr)
+                    if type(val) != str:
+                        val = str(val)
+                    pages[-1].text += create_data_header(attr, val)
+                except:
+                    pass
+        self.layout = post.HtmlLayout(pages, self.longname, years=(y0, yf))
+
+        # Define relative weights of each score in the overall score
+        # (FIX: need some way for the user to modify this)
+        self.weight = {
+            "Bias Score": 1.0,
+            "RMSE Score": 2.0,
+            "Seasonal Cycle Score": 1.0,
+            "Interannual Variability Score": 1.0,
+            "Spatial Distribution Score": 1.0,
+        }
+
+    def requires(self):
+        if self.derived is not None:
+            ands = [arg.name for arg in sympify(self.derived).free_symbols]
+            ors = []
+        else:
+            ands = []
+            ors = [self.variable] + self.alternate_vars
+        return ands, ors
+
+    def stageData(self, m):
+        r"""Extracts model data which matches the observational dataset.
+
+        The datafile associated with this confrontation defines what
+        is to be extracted from the model results. If the
+        observational data represents sites, as opposed to spatially
+        defined over a latitude/longitude grid, then the model results
+        will be sampled at the site locations to match. The spatial
+        grids need not align, the analysis will handle the
+        interpolations when necesary.
+
+        If both datasets are defined on the same temporal scale, then
+        the maximum overlap time is computed and the datasets are
+        clipped to match. If there is some disparity in the temporal
+        scale (e.g. annual mean observational data and monthly mean
+        model results) then we coarsen the model results automatically
+        to match the observational data.
+
+        Parameters
+        ----------
+        m : ILAMB.ModelResult.ModelResult
+            the model result context
+
+        Returns
+        -------
+        obs : ILAMB.Variable.Variable
+            the variable context associated with the observational dataset
+        mod : ILAMB.Variable.Variable
+            the variable context associated with the model result
+        """
+        obs = Variable(
+            filename=self.source,
+            variable_name=self.variable,
+            alternate_vars=self.alternate_vars,
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        )
+        obs.data *= self.scale_factor
+        if obs.time is None:
+            raise il.NotTemporalVariable()
+        self.pruneRegions(obs)
+
+        # The reference might be layered and we want to extract a
+        # slice to compare against models
+        if "depth" in self.keywords and obs.layered:
+            obs.trim(d=[self.keywords["depth"] - 0.01, self.keywords["depth"] + 0.01])
+            if obs.depth.size > 1:
+                obs = obs.integrateInDepth(mean=True)
+                shp = list(obs.data.shape)
+                shp.insert(1, 1)
+                obs.data.shape = shp
+                obs.depth = np.asarray([self.keywords["depth"]])
+                obs.depth_bnds = np.asarray(
+                    [[self.keywords["depth"] - 0.01, self.keywords["depth"] + 0.01]]
+                )
+                obs.layered = True
+                obs.name = self.variable
+
+        # Try to extract a commensurate quantity from the model
+        mod = m.extractTimeSeries(
+            self.variable,
+            alt_vars=self.alternate_vars,
+            expression=self.derived,
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+            lats=None if obs.spatial else obs.lat,
+            lons=None if obs.spatial else obs.lon,
+        )
+        obs, mod = il.MakeComparable(
+            obs,
+            mod,
+            mask_ref=True,
+            clip_ref=True,
+            extents=self.extents,
+            logstring="[%s][%s]" % (self.longname, m.name),
+        )
+
+        # Check the order of magnitude of the data and convert to help avoid roundoff errors
+        def _reduceRoundoffErrors(var):
+            if "s-1" in var.unit:
+                return var.convert(var.unit.replace("s-1", "d-1"))
+            if "kg" in var.unit:
+                return var.convert(var.unit.replace("kg", "g"))
+            return var
+
+        def _getOrder(var):
+            return np.log10(np.abs(var.data).clip(1e-16)).mean()
+
+        order = _getOrder(obs)
+        count = 0
+        while order < -2 and count < 2:
+            obs = _reduceRoundoffErrors(obs)
+            order = _getOrder(obs)
+            count += 1
+
+        # convert the model data to the same unit
+        mod = mod.convert(obs.unit)
+
+        return obs, mod
+
+    def pruneRegions(self, var):
+        # remove regions if there is no data from the input variable
+        r = Regions()
+        self.regions = [region for region in self.regions if r.hasData(region, var)]
+
+    def confront(self, m):
+        r"""Confronts the input model with the observational data.
+
+        This routine performs a mean-state analysis the details of
+        which may be found in the documentation of
+        ILAMB.ilamblib.AnalysisMeanState. If relationship information
+        was provided, it will also perform the analysis documented in
+        ILAMB.ilamblib.AnalysisRelationship. Output from the analysis
+        is stored in a netCDF4 file in the output path.
+
+        Parameters
+        ----------
+        m : ILAMB.ModelResult.ModelResult
+            the model results
+        """
+        # Grab the data
+        obs, mod = self.stageData(m)
+
+        mod_file = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
+        obs_file = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name,))
+        with il.FileContextManager(self.master, mod_file, obs_file) as fcm:
+            # Encode some names and colors
+            fcm.mod_dset.setncatts(
+                {
+                    "name": m.name,
+                    "color": m.color,
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
+            if self.master:
+                fcm.obs_dset.setncatts(
+                    {
+                        "name": "Benchmark",
+                        "color": np.asarray([0.5, 0.5, 0.5]),
+                        "weight": self.cweight,
+                        "complete": 0,
+                    }
+                )
+
+            # Read in some options and run the mean state analysis
+            mass_weighting = self.keywords.get("mass_weighting", False)
+            skip_rmse = self.keywords.get("skip_rmse", False)
+            skip_iav = self.keywords.get("skip_iav", True)
+            skip_cycle = self.keywords.get("skip_cycle", False)
+            rmse_score_basis = self.keywords.get("rmse_score_basis", "cycle")
+            if obs.spatial:
+                il.AnalysisMeanStateSpace(
+                    obs,
+                    mod,
+                    dataset=fcm.mod_dset,
+                    regions=self.regions,
+                    benchmark_dataset=fcm.obs_dset,
+                    table_unit=self.table_unit,
+                    plot_unit=self.plot_unit,
+                    space_mean=self.space_mean,
+                    skip_rmse=skip_rmse,
+                    skip_iav=skip_iav,
+                    skip_cycle=skip_cycle,
+                    mass_weighting=mass_weighting,
+                    rmse_score_basis=rmse_score_basis,
+                    df_errs=self.df_errs,
+                )
+            else:
+                il.AnalysisMeanStateSites(
+                    obs,
+                    mod,
+                    dataset=fcm.mod_dset,
+                    regions=self.regions,
+                    benchmark_dataset=fcm.obs_dset,
+                    table_unit=self.table_unit,
+                    plot_unit=self.plot_unit,
+                    space_mean=self.space_mean,
+                    skip_rmse=skip_rmse,
+                    skip_iav=skip_iav,
+                    skip_cycle=skip_cycle,
+                    mass_weighting=mass_weighting,
+                    df_errs=self.df_errs,
+                )
+            fcm.mod_dset.setncattr("complete", 1)
+            if self.master:
+                fcm.obs_dset.setncattr("complete", 1)
+        logger.info("[%s][%s] Success" % (self.longname, m.name))
 
-        mem_slab = self.keywords.get("mem_slab",100000.) # Mb
+    def determinePlotLimits(self):
+        """Determine the limits of all plots which are inclusive of all ranges.
 
-        # peak at the reference dataset without reading much into memory
-        info = ""
-        unit = ""
-        with Dataset(self.source) as dset:
-            var = dset.variables[self.variable]
-            obs_t,obs_tb,obs_cb,obs_b,obs_e,cal = il.GetTime(var)
-            obs_nt = obs_t.size
-            obs_mem = var.size*8e-6
-            unit = var.units
-            climatology = False if obs_cb is None else True
-            if climatology:
-                info += "[climatology]"
-                obs_cb = (obs_cb-1850)*365.
-                t0 = obs_cb[0]; tf = obs_cb[1]
-            else:
-                t0 = obs_tb[0,0]; tf = obs_tb[-1,1]
-            info += " contents span years %.1f to %.1f, est memory %d [Mb]" % (t0/365.+1850,tf/365.+1850,obs_mem)
-        logger.info("[%s][%s]%s" % (self.name,self.variable,info))
-
-        # to peak at the model, we need any variable that could be
-        # part of the expression to look at the time
-        info = ""
-        possible = [self.variable,] + self.alternate_vars
-        if self.derived is not None: possible += [str(s) for s in sympify(self.derived).free_symbols]
-        vname = [v for v in possible if v in m.variables.keys()]
-        if len(vname) == 0:
-            logger.debug("[%s] Could not find [%s] in the model results" % (self.name,",".join(possible)))
-            raise il.VarNotInModel()
-        vname = vname[0]
-
-        # peak at the model dataset without reading much into memory
-        mod_nt  =  0
-        mod_mem =  0.
-        mod_t0  =  2147483647
-        mod_tf  = -2147483648
-        for fname in m.variables[vname]:
-            with Dataset(fname) as dset:
-                var = dset.variables[vname]
-                mod_t,mod_tb,mod_cb,mod_b,mod_e,cal = il.GetTime(var,t0=t0-m.shift,tf=tf-m.shift)
-                if mod_t is None:
-                    info += "\n      %s does not overlap the reference" % (fname)
-                    continue
-                mod_t += m.shift
-                mod_tb += m.shift
-                ind = np.where((mod_tb[:,0] >= t0)*(mod_tb[:,1] <= tf))[0]
-                if ind.size == 0:
-                    info += "\n      %s does not overlap the reference" % (fname)
-                    continue
-                mod_t  = mod_t [ind]
-                mod_tb = mod_tb[ind]
-                mod_t0 = min(mod_t0,mod_tb[ 0,0])
-                mod_tf = max(mod_tf,mod_tb[-1,1])
-                nt = mod_t.size
-                mod_nt += nt
-                mem = (var.size/var.shape[0]*nt)*8e-6
-                mod_mem += mem
-                info += "\n      %s spans years %.1f to %.1f, est memory in time bounds %d [Mb]" % (fname,mod_t.min()/365.+1850,mod_t.max()/365.+1850,mem)
-        info += "\n      total est memory = %d [Mb]" % mod_mem
-        logger.info("[%s][%s][%s] reading model data from possibly many files%s" % (self.name,m.name,vname,info))
-        if mod_t0 > mod_tf:
-            logger.debug("[%s] Could not find [%s] in the model results in the given time frame, tinput = [%.1f,%.1f]" % (self.name,",".join(possible),t0,tf))
-            raise il.VarNotInModel()
-
-        # if the reference is a climatology, then build a model climatology in slabs
-        info = ""
-        if climatology:
-
-            # how many slabs
-            ns   = int(np.floor(mod_mem/mem_slab))+1
-            ns   = min(max(1,ns),mod_nt)
-            logger.info("[%s][%s] building climatology in %d slabs" % (self.name,m.name,ns))
-
-            # across what times?
-            slab_t = (mod_tf-mod_t0)*np.linspace(0,1,ns+1)+mod_t0
-            slab_t = np.floor(slab_t / 365)*365 + bnd_months[(np.abs(bnd_months[:,np.newaxis] - (slab_t % 365))).argmin(axis=0)]
-
-            # ready to slab
-            tb_prev = None
-            data    = None
-            dnum    = None
-            for i in range(ns):
-
-                v = m.extractTimeSeries(self.variable,
-                                        alt_vars     = self.alternate_vars,
-                                        expression   = self.derived,
-                                        initial_time = slab_t[i],
-                                        final_time   = slab_t[i+1]).convert(unit)
-
-                # trim does not work properly so we will add a manual check ourselves
-                if tb_prev is None:
-                    tb_prev = v.time_bnds[...]
+        The routine will open all netCDF files in the output path and
+        add the maximum and minimum of all variables which are
+        designated to be plotted. If legends are desired for a given
+        plot, these are rendered here as well. This routine should be
+        called before calling any plotting routine.
+
+        """
+
+        filelist = glob.glob(os.path.join(self.output_path, "*.nc"))
+        benchmark_file = [f for f in filelist if "Benchmark" in f]
+
+        # There may be regions in which there is no benchmark data and
+        # these should be weeded out. If the plotting phase occurs in
+        # the same run as the analysis phase, this is not needed.
+        if benchmark_file:
+            with Dataset(benchmark_file[0]) as dset:
+                if "MeanState" in dset.groups:
+                    Vs = getVariableList(dset.groups["MeanState"])
                 else:
-                    if np.allclose(tb_prev[-1],v.time_bnds[0]):
-                        v.data = v.data[1:]
-                        v.time = v.time[1:]
-                        v.time_bnds = v.time_bnds[1:]
-                    tb_prev = v.time_bnds[...]
-                if v.time.size == 0: continue
-
-                mind = (np.abs(mid_months[:,np.newaxis]-(v.time % 365))).argmin(axis=0)
-                if data is None:
-                    data = np.ma.zeros((12,)+v.data.shape[1:])
-                    dnum = np.ma.zeros(data.shape,dtype=int)
-                data[mind,...] += v.data
-                dnum[mind,...] += 1
-            with np.errstate(over='ignore',under='ignore'):
-                data = data / dnum.clip(1)
-
-            # return variables
-            obs = Variable(filename       = self.source,
-                           variable_name  = self.variable,
-                           alternate_vars = self.alternate_vars)
-            mod = Variable(name  = obs.name,
-                           unit  = obs.unit,
-                           data  = data,
-                           time  = obs.time,
-                           lat   = v.lat,
-                           lon   = v.lon,
-                           depth = v.depth,
-                           time_bnds  = obs.time_bnds,
-                           lat_bnds   = v.lat_bnds,
-                           lon_bnds   = v.lon_bnds,
-                           depth_bnds = v.depth_bnds)
+                    Vs = []
+            Vs = [v for v in Vs if "timeint" in v]
+            if Vs:
+                self.pruneRegions(
+                    Variable(
+                        filename=benchmark_file[0],
+                        variable_name=Vs[0],
+                        groupname="MeanState",
+                    )
+                )
+
+        # Determine the min/max of variables over all models
+        limits = {}
+        for fname in filelist:
+            with Dataset(fname) as dataset:
+                if "MeanState" not in dataset.groups:
+                    continue
+                variables = getVariableList(dataset.groups["MeanState"])
+                for vname in variables:
+                    var = dataset.groups["MeanState"].variables[vname]
+                    if var[...].size <= 1:
+                        continue
+                    pname = vname.split("_")[0]
 
-            yield obs,mod
+                    """If the plot is a time series, it has been averaged over regions
+                    already and we need a separate dictionary for the
+                    region as well. These can be based on the
+                    percentiles from the attributes of the netCDF
+                    variables."""
+                    if pname in time_opts:
+                        region = vname.split("_")[-1]
+                        if pname not in limits:
+                            limits[pname] = {}
+                        if region not in limits[pname]:
+                            limits[pname][region] = {}
+                            limits[pname][region]["min"] = +1e20
+                            limits[pname][region]["max"] = -1e20
+                            limits[pname][region]["unit"] = post.UnitStringToMatplotlib(
+                                var.getncattr("units")
+                            )
+                        limits[pname][region]["min"] = min(
+                            limits[pname][region]["min"], var.getncattr("min")
+                        )
+                        limits[pname][region]["max"] = max(
+                            limits[pname][region]["max"], var.getncattr("max")
+                        )
 
-        # if obs is historical, then we yield slabs of both
-        else:
+                    else:
+                        """If the plot is spatial, we want to set the limits as a percentile
+                        of all data across models and the
+                        benchmark. So here we load the data up and in
+                        another pass will compute the percentiles."""
+                        if pname not in limits:
+                            limits[pname] = {}
+                            limits[pname]["min"] = +1e20
+                            limits[pname]["max"] = -1e20
+                            limits[pname]["unit"] = post.UnitStringToMatplotlib(
+                                var.getncattr("units")
+                            )
+                            limits[pname]["data"] = var[...].compressed()
+                        else:
+                            limits[pname]["data"] = np.hstack(
+                                [limits[pname]["data"], var[...].compressed()]
+                            )
 
-            obs_mem *= (mod_tf-mod_t0)/(tf-t0)
-            mod_t0 = max(mod_t0,t0)
-            mod_tf = min(mod_tf,tf)
-            ns   = int(np.floor(max(obs_mem,mod_mem)/mem_slab))+1
-            ns   = min(min(max(1,ns),mod_nt),obs_nt)
-            logger.info("[%s][%s] staging data in %d slabs" % (self.name,m.name,ns))
-
-            # across what times?
-            slab_t = (mod_tf-mod_t0)*np.linspace(0,1,ns+1)+mod_t0
-            slab_t = np.floor(slab_t / 365)*365 + bnd_months[(np.abs(bnd_months[:,np.newaxis] - (slab_t % 365))).argmin(axis=0)]
-
-            obs_tb = None; mod_tb = None
-            for i in range(ns):
-
-                # get reference variable
-                obs = Variable(filename       = self.source,
-                               variable_name  = self.variable,
-                               alternate_vars = self.alternate_vars,
-                               t0             = slab_t[i],
-                               tf             = slab_t[i+1]).trim(t=[slab_t[i],slab_t[i+1]])
-                if obs_tb is None:
-                    obs_tb = obs.time_bnds[...]
-                else:
-                    if np.allclose(obs_tb[-1],obs.time_bnds[0]):
-                        obs.data = obs.data[1:]
-                        obs.time = obs.time[1:]
-                        obs.time_bnds = obs.time_bnds[1:]
-                    assert np.allclose(obs.time_bnds[0,0],obs_tb[-1,1])
-                    obs_tb = obs.time_bnds[...]
-
-                # get model variable
-                mod = m.extractTimeSeries(self.variable,
-                                          alt_vars     = self.alternate_vars,
-                                          expression   = self.derived,
-                                          initial_time = slab_t[i],
-                                          final_time   = slab_t[i+1]).trim(t=[slab_t[i],slab_t[i+1]]).convert(obs.unit)
-                if mod_tb is None:
-                    mod_tb = mod.time_bnds[...]
-                else:
-                    if np.allclose(mod_tb[-1],mod.time_bnds[0]):
-                        mod.data = mod.data[1:]
-                        mod.time = mod.time[1:]
-                        mod.time_bnds = mod.time_bnds[1:]
-                    assert np.allclose(mod.time_bnds[0,0],mod_tb[-1,1])
-                    mod_tb = mod.time_bnds[...]
-                assert obs.time.size == mod.time.size
-                yield obs,mod
-
-    def confront(self,m):
-
-        def _addDepth(v):
-            v.depth = np.asarray([5.])
-            v.depth_bnds = np.asarray([[0.,10.]])
-            shp = list(v.data.shape)
-            shp.insert(1,1)
-            v.data.shape = shp
-            v.layered = True
-            return v
-
-        mod_file = os.path.join(self.output_path,"%s_%s.nc"        % (self.name,m.name))
-        obs_file = os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name,      ))
-        with il.FileContextManager(self.master,mod_file,obs_file) as fcm:
+        # For those limits which we built up data across all models, compute the percentiles
+        for pname in limits.keys():
+            if "data" in limits[pname] and len(limits[pname]["data"]) > 0:
+                limits[pname]["min"], limits[pname]["max"] = np.percentile(
+                    limits[pname]["data"], [1, 99]
+                )
 
-            # Encode some names and colors
-            fcm.mod_dset.setncatts({"name" :m.name,
-                                    "color":m.color,
-                                    "weight":self.cweight,
-                                    "complete":0})
-            if self.master:
-                fcm.obs_dset.setncatts({"name" :"Benchmark",
-                                        "color":np.asarray([0.5,0.5,0.5]),
-                                        "weight":self.cweight,
-                                        "complete":0})
-
-            obs_timeint = {}; mod_timeint = {}
-            obs_depth   = {}; mod_depth   = {}
-            ocyc        = {}; oN          = {}
-            mcyc        = {}; mN          = {}
-            for depth in self.depths:
-                dlbl = "%d" % depth
-                obs_timeint[dlbl] = []
-                mod_timeint[dlbl] = []
-            for region in self.regions:
-                obs_depth[region] = []
-                mod_depth[region] = []
-            unit = None
-            max_obs = -1e20
-            for obs,mod in self.stageData(m):
-
-                # if the data has no depth, we assume it is surface
-                if not obs.layered: obs = _addDepth(obs)
-                if not mod.layered: mod = _addDepth(mod)
-                max_obs = max(max_obs,obs.data.max())
-
-                # time bounds for this slab
-                tb = obs.time_bnds[[0,-1],[0,1]].reshape((1,2))
-                t  = np.asarray([tb.mean()])
-
-                # mean lat/lon slices at various depths
-                for depth in self.depths:
-
-                    dlbl = "%d" % depth
-                    z  = obs.integrateInDepth(z0=depth-1.,zf=depth+1,mean=True).integrateInTime(mean=True)
-                    unit = z.unit
-                    obs_timeint[dlbl].append(Variable(name = "timeint%s" % dlbl,
-                                                      unit = z.unit,
-                                                      data = z.data.reshape((1,)+z.data.shape),
-                                                      time = t,     time_bnds = tb,
-                                                      lat  = z.lat, lat_bnds  = z.lat_bnds,
-                                                      lon  = z.lon, lon_bnds  = z.lon_bnds))
-                    z = mod
-                    if mod.layered: z = z.integrateInDepth(z0=depth-1.,zf=depth+1,mean=True)
-                    z = z.integrateInTime(mean=True)
-                    mod_timeint[dlbl].append(Variable(name = "timeint%s" % dlbl,
-                                                      unit = z.unit,
-                                                      data = z.data.reshape((1,)+z.data.shape),
-                                                      time = t,     time_bnds = tb,
-                                                      lat  = z.lat, lat_bnds  = z.lat_bnds,
-                                                      lon  = z.lon, lon_bnds  = z.lon_bnds))
+        # Second pass to plot legends (FIX: only for master?)
+        for pname in limits.keys():
+            try:
+                opts = space_opts[pname]
+            except:
+                continue
+
+            # Determine plot limits and colormap
+            if opts["sym"]:
+                vabs = max(abs(limits[pname]["min"]), abs(limits[pname]["min"]))
+                limits[pname]["min"] = -vabs
+                limits[pname]["max"] = vabs
+            if "shift" in pname:
+                limits[pname]["min"] = -6
+                limits[pname]["max"] = +6
+            if "phase" in pname:
+                limits[pname]["min"] = 0
+                limits[pname]["max"] = 365
 
-                # mean
-                for region in self.regions:
-                    z = VariableReduce(obs,region=region,time=tb[0],lon=[-180.,+180.])
-                    z.time = t; z.time_bnds  = tb; z.temporal = True; z.data.shape = (1,)+z.data.shape
-                    obs_depth[region].append(z)
-
-                    z = VariableReduce(mod,region=region,time=tb[0],lon=[-180.,+180.])
-                    z.time = t; z.time_bnds  = tb; z.temporal = True; z.data.shape = (1,)+z.data.shape
-                    mod_depth[region].append(z)
+            # if a score, force to be [0,1]
+            if "score" in pname:
+                limits[pname]["min"] = 0
+                limits[pname]["max"] = 1
 
-                # annual cycle in slabs
-                for region in self.regions:
-                    z = obs.integrateInSpace(region=region,mean=True)
-                    if region not in ocyc:
-                        ocyc[region] = np.ma.zeros((12,)+z.data.shape[1:])
-                        oN  [region] = np.ma.zeros((12,)+z.data.shape[1:],dtype=int)
-                    i = (np.abs(mid_months[:,np.newaxis]-(z.time % 365))).argmin(axis=0)
-                    (ocyc[region])[i,...] += z.data
-                    (oN  [region])[i,...] += 1
-
-                    z = mod.integrateInSpace(region=region,mean=True)
-                    if region not in mcyc:
-                        mcyc[region] = np.ma.zeros((12,)+z.data.shape[1:])
-                        mN  [region] = np.ma.zeros((12,)+z.data.shape[1:],dtype=int)
-                    i = (np.abs(mid_months[:,np.newaxis]-(z.time % 365))).argmin(axis=0)
-                    (mcyc[region])[i,...] += z.data
-                    (mN  [region])[i,...] += 1
-
-            # combine time slabs from the different depths
-            large_bias = float(self.keywords.get("large_bias",0.1*max_obs))
-
-            for dlbl in obs_timeint.keys():
-
-                # period means and bias
-                obs_tmp = il.CombineVariables(obs_timeint[dlbl]).integrateInTime(mean=True)
-                mod_tmp = il.CombineVariables(mod_timeint[dlbl]).integrateInTime(mean=True)
-                obs_tmp.name = obs_tmp.name.split("_")[0]
-                mod_tmp.name = mod_tmp.name.split("_")[0]
-                bias = obs_tmp.spatialDifference(mod_tmp)
-                bias.name = mod_tmp.name.replace("timeint","bias")
-                mod_tmp.toNetCDF4(fcm.mod_dset,group="MeanState")
-                bias.toNetCDF4(fcm.mod_dset,group="MeanState")
-                bias_score = None
-                if dlbl == "0":
-                    with np.errstate(all="ignore"):
-                        bias_score = Variable(name  = bias.name.replace("bias","biasscore"),
-                                              data  = np.exp(-np.abs(bias.data)/large_bias),
-                                              unit  = "1",
-                                              ndata = bias.ndata,
-                                              lat   = bias.lat, lat_bnds = bias.lat_bnds,
-                                              lon   = bias.lon, lon_bnds = bias.lon_bnds,
-                                              area  = bias.area)
-                        bias_score.toNetCDF4(fcm.mod_dset,group="MeanState")
+            limits[pname]["cmap"] = opts["cmap"]
+            if limits[pname]["cmap"] == "choose":
+                limits[pname]["cmap"] = self.cmap
+            if "score" in pname:
+                limits[pname]["cmap"] = plt.cm.get_cmap(limits[pname]["cmap"])
 
-                for region in self.regions:
+            # Plot a legend for each key
+            if opts["haslegend"]:
+                fig, ax = plt.subplots(figsize=(6.8, 1.0), tight_layout=True)
+                label = opts["label"]
+                if label == "unit":
+                    label = limits[pname]["unit"]
+                post.ColorBar(
+                    ax,
+                    vmin=limits[pname]["min"],
+                    vmax=limits[pname]["max"],
+                    cmap=limits[pname]["cmap"],
+                    ticks=opts["ticks"],
+                    ticklabels=opts["ticklabels"],
+                    label=label,
+                )
+                fig.savefig(os.path.join(self.output_path, "legend_%s.png" % (pname)))
+                plt.close()
 
-                    sval = mod_tmp.integrateInSpace(region=region,mean=True)
-                    sval.name = "Period Mean at %s %s" % (dlbl,region)
-                    sval.toNetCDF4(fcm.mod_dset,group="MeanState")
-
-                    sval = bias.integrateInSpace(region=region,mean=True)
-                    sval.name = "Bias at %s %s" % (dlbl,region)
-                    sval.toNetCDF4(fcm.mod_dset,group="MeanState")
-
-                    if bias_score is not None:
-                        sval = bias_score.integrateInSpace(region=region,mean=True)
-                        sval.name = "Bias Score at %s %s" % (dlbl,region)
-                        sval.toNetCDF4(fcm.mod_dset,group="MeanState")
+        # Determine min/max of relationship variables
+        for fname in glob.glob(os.path.join(self.output_path, "*.nc")):
+            with Dataset(fname) as dataset:
+                for g in dataset.groups.keys():
+                    if "relationship" not in g:
+                        continue
+                    grp = dataset.groups[g]
+                    if g not in limits:
+                        limits[g] = {}
+                        limits[g]["xmin"] = +1e20
+                        limits[g]["xmax"] = -1e20
+                        limits[g]["ymin"] = +1e20
+                        limits[g]["ymax"] = -1e20
+                    limits[g]["xmin"] = min(
+                        limits[g]["xmin"], grp.variables["ind_bnd"][0, 0]
+                    )
+                    limits[g]["xmax"] = max(
+                        limits[g]["xmax"], grp.variables["ind_bnd"][-1, -1]
+                    )
+                    limits[g]["ymin"] = min(
+                        limits[g]["ymin"], grp.variables["dep_bnd"][0, 0]
+                    )
+                    limits[g]["ymax"] = max(
+                        limits[g]["ymax"], grp.variables["dep_bnd"][-1, -1]
+                    )
 
-                if self.master:
-                    obs_tmp.toNetCDF4(fcm.obs_dset,group="MeanState")
-                    for region in self.regions:
-                        sval = obs_tmp.integrateInSpace(region=region,mean=True)
-                        sval.name = "Period Mean at %s %s" % (dlbl,region)
-                        sval.toNetCDF4(fcm.obs_dset,group="MeanState")
+        self.limits = limits
+
+    def computeOverallScore(self, m):
+        """Computes the overall composite score for a given model.
+
+        This routine opens the netCDF results file associated with
+        this confrontation-model pair, and then looks for a "scalars"
+        group in the dataset as well as any subgroups that may be
+        present. For each grouping of scalars, it will blend any value
+        with the word "Score" in the name to render an overall score,
+        overwriting the existing value if present.
+
+        Parameters
+        ----------
+        m : ILAMB.ModelResult.ModelResult
+            the model results
+
+        """
+
+        def _computeOverallScore(scalars):
+            """Given a netCDF4 group of scalars, blend them into an overall score"""
+            scores = {}
+            variables = [
+                v
+                for v in scalars.variables.keys()
+                if "Score" in v and "Overall" not in v
+            ]
+            for region in self.regions:
+                overall_score = 0.0
+                sum_of_weights = 0.0
+                for v in variables:
+                    if region not in v:
+                        continue
+                    score = v.replace(region, "").strip()
+                    weight = 1.0
+                    if score in self.weight:
+                        weight = self.weight[score]
+                    overall_score += weight * scalars.variables[v][...]
+                    sum_of_weights += weight
+                overall_score /= max(sum_of_weights, 1e-12)
+                scores["Overall Score %s" % region] = overall_score
+            return scores
+
+        fname = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
+        if not os.path.isfile(fname):
+            return
+        with Dataset(fname, mode="r+") as dataset:
+            datasets = [
+                dataset.groups[grp] for grp in dataset.groups if "scalars" not in grp
+            ]
+            groups = [grp for grp in dataset.groups if "scalars" not in grp]
+            datasets.append(dataset)
+            groups.append(None)
+            for dset, grp in zip(datasets, groups):
+                if "scalars" in dset.groups:
+                    scalars = dset.groups["scalars"]
+                    score = _computeOverallScore(scalars)
+                    for key in score.keys():
+                        if key in scalars.variables:
+                            scalars.variables[key][0] = score[key]
+                        else:
+                            Variable(data=score[key], name=key, unit="1").toNetCDF4(
+                                dataset, group=grp
+                            )
+
+    def compositePlots(self):
+        """Renders plots which display information of all models.
+
+        This routine renders plots which contain information from all
+        models. Thus only the master process will run this routine,
+        and only after all analysis has finished.
 
-            # combine depth/lat slabs for different regions
+        """
+        if not self.master:
+            return
+
+        # get the HTML page
+        page = [page for page in self.layout.pages if "MeanState" in page.name][0]
+
+        models = []
+        colors = []
+        corr = {}
+        std = {}
+        cycle = {}
+        has_cycle = False
+        has_std = False
+        for fname in glob.glob(os.path.join(self.output_path, "*.nc")):
+            dataset = Dataset(fname)
+            if "MeanState" not in dataset.groups:
+                continue
+            dset = dataset.groups["MeanState"]
+            models.append(dataset.getncattr("name"))
+            colors.append(dataset.getncattr("color"))
             for region in self.regions:
-                mod_tmp = il.CombineVariables(mod_depth[region]).integrateInTime(mean=True)
-                mod_tmp.name = "timelonint_of_%s_over_%s" % (self.variable,region)
-                mod_tmp.toNetCDF4(fcm.mod_dset,group="MeanState")
-                obs_tmp = il.CombineVariables(obs_depth[region]).integrateInTime(mean=True)
-                obs_tmp.name = "timelonint_of_%s_over_%s" % (self.variable,region)
-                mod_bias = TimeLatBias(obs_tmp,mod_tmp)
-                mod_bias.toNetCDF4(fcm.mod_dset,group="MeanState")
-                np.seterr(over='ignore',under='ignore')
-                ocyc[region] = ocyc[region]/(oN[region].clip(1))
-                mcyc[region] = mcyc[region]/(mN[region].clip(1))
-
-                np.seterr(over='raise',under='raise')
-                mcyc[region] = Variable(name = "cycle_of_%s_over_%s" % (self.variable,region),
-                                        unit = mod.unit,
-                                        data = mcyc[region],
-                                        depth = mod.depth,
-                                        depth_bnds = mod.depth_bnds,
-                                        time = mid_months)
-                ocyc[region] = Variable(name = "cycle_of_%s_over_%s" % (self.variable,region),
-                                        unit = obs.unit,
-                                        data = ocyc[region],
-                                        depth = obs.depth,
-                                        depth_bnds = obs.depth_bnds,
-                                        time = mid_months)
-                cyc_bias = CycleBias(ocyc[region],mcyc[region])
-                cyc_bias    .toNetCDF4(fcm.mod_dset,group="MeanState")
-                mcyc[region].toNetCDF4(fcm.mod_dset,group="MeanState")
-                if self.master:
-                    obs_tmp     .toNetCDF4(fcm.obs_dset,group="MeanState")
-                    ocyc[region].toNetCDF4(fcm.obs_dset,group="MeanState")
-            fcm.mod_dset.setncattr("complete",1)
-            if self.master: fcm.obs_dset.setncattr("complete",1)
-
-    def modelPlots(self,m):
-
-        def _fheight(region):
-            if region in ["arctic","southern"]: return 6.8
-            return 2.8
-
-        bname  = "%s/%s_Benchmark.nc" % (self.output_path,self.name)
-        fname  = "%s/%s_%s.nc" % (self.output_path,self.name,m.name)
-        if not os.path.isfile(bname): return
-        if not os.path.isfile(fname): return
+                if region not in cycle:
+                    cycle[region] = []
+                key = [
+                    v for v in dset.variables.keys() if ("cycle_" in v and region in v)
+                ]
+                if len(key) > 0:
+                    has_cycle = True
+                    cycle[region].append(
+                        Variable(
+                            filename=fname, groupname="MeanState", variable_name=key[0]
+                        )
+                    )
+
+                if region not in std:
+                    std[region] = []
+                if region not in corr:
+                    corr[region] = []
+
+                key = []
+                if "scalars" in dset.groups:
+                    key = [
+                        v
+                        for v in dset.groups["scalars"].variables.keys()
+                        if ("Spatial Distribution Score" in v and region in v)
+                    ]
+                if len(key) > 0:
+                    has_std = True
+                    sds = dset.groups["scalars"].variables[key[0]]
+                    corr[region].append(sds.getncattr("R"))
+                    std[region].append(sds.getncattr("std"))
+
+        # composite annual cycle plot
+        if has_cycle and len(models) > 2:
+            page.addFigure(
+                "Spatially integrated regional mean",
+                "compcycle",
+                "RNAME_compcycle.png",
+                side="ANNUAL CYCLE",
+                legend=False,
+            )
+
+        for region in self.regions:
+            if region not in cycle:
+                continue
+            fig, ax = plt.subplots(figsize=(6.8, 2.8), tight_layout=True)
+            for name, color, var in zip(models, colors, cycle[region]):
+                dy = 0.05 * (
+                    self.limits["cycle"][region]["max"]
+                    - self.limits["cycle"][region]["min"]
+                )
+                var.plot(
+                    ax,
+                    lw=2,
+                    color=color,
+                    label=name,
+                    ticks=time_opts["cycle"]["ticks"],
+                    ticklabels=time_opts["cycle"]["ticklabels"],
+                    vmin=self.limits["cycle"][region]["min"] - dy,
+                    vmax=self.limits["cycle"][region]["max"] + dy,
+                )
+                ylbl = time_opts["cycle"]["ylabel"]
+                if ylbl == "unit":
+                    ylbl = post.UnitStringToMatplotlib(var.unit)
+                ax.set_ylabel(ylbl)
+            fig.savefig(os.path.join(self.output_path, "%s_compcycle.png" % (region)))
+            plt.close()
+
+        # plot legends with model colors (sorted with Benchmark data on top)
+        page.addFigure(
+            "Spatially integrated regional mean",
+            "legend_compcycle",
+            "legend_compcycle.png",
+            side="MODEL COLORS",
+            legend=False,
+        )
+
+        def _alphabeticalBenchmarkFirst(key):
+            key = key[0].lower()
+            if key == "BENCHMARK":
+                return "A"
+            return key
+
+        tmp = sorted(zip(models, colors), key=_alphabeticalBenchmarkFirst)
+        fig, ax = plt.subplots()
+        for model, color in tmp:
+            ax.plot(0, 0, "o", mew=0, ms=8, color=color, label=model)
+        handles, labels = ax.get_legend_handles_labels()
+        plt.close()
+
+        ncol = np.ceil(float(len(models)) / 11.0).astype(int)
+        if ncol > 0:
+            fig, ax = plt.subplots(figsize=(3.0 * ncol, 2.8), tight_layout=True)
+            ax.legend(
+                handles, labels, loc="upper right", ncol=ncol, fontsize=10, numpoints=1
+            )
+            ax.axis(False)
+            fig.savefig(os.path.join(self.output_path, "legend_compcycle.png"))
+            fig.savefig(os.path.join(self.output_path, "legend_spatial_variance.png"))
+            fig.savefig(os.path.join(self.output_path, "legend_temporal_variance.png"))
+            plt.close()
+
+        # spatial distribution Taylor plot
+        if has_std:
+            page.addFigure(
+                "Temporally integrated period mean",
+                "spatial_variance",
+                "RNAME_spatial_variance.png",
+                side="SPATIAL TAYLOR DIAGRAM",
+                legend=False,
+            )
+            page.addFigure(
+                "Temporally integrated period mean",
+                "legend_spatial_variance",
+                "legend_spatial_variance.png",
+                side="MODEL COLORS",
+                legend=False,
+            )
+        if "Benchmark" in models:
+            colors.pop(models.index("Benchmark"))
+        for region in self.regions:
+            if not (region in std and region in corr):
+                continue
+            if len(std[region]) != len(corr[region]):
+                continue
+            if len(std[region]) == 0:
+                continue
+            fig = plt.figure(figsize=(6.0, 6.0))
+            post.TaylorDiagram(
+                np.asarray(std[region]), np.asarray(corr[region]), 1.0, fig, colors
+            )
+            fig.savefig(
+                os.path.join(self.output_path, "%s_spatial_variance.png" % region)
+            )
+            plt.close()
+
+    def modelPlots(self, m):
+        """For a given model, create the plots of the analysis results.
+
+        This routine will extract plotting information out of the
+        netCDF file which results from the analysis and create
+        plots. Note that determinePlotLimits should be called before
+        this routine.
+
+        """
+        self._relationship(m)
+        bname = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name))
+        fname = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
+        if not os.path.isfile(bname):
+            return
+        if not os.path.isfile(fname):
+            return
 
-        # get the HTML page and set table priorities
+        # get the HTML page
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
-        page.priority  = [" %d " % d for d in self.depths]
-        page.priority += ["Period","Bias"]
-        page.priority += ["Score","Overall"]
-
-        # model plots
-        cmap = { "timeint"    : self.cmap,
-                 "bias"       : "seismic",
-                 "biasscore"  : "score" }
-        plbl = { "timeint"    : "MEAN",
-                 "bias"       : "BIAS",
-                 "biasscore"  : "BIAS SCORE" }
+
         with Dataset(fname) as dataset:
-            group     = dataset.groups["MeanState"]
+            group = dataset.groups["MeanState"]
             variables = getVariableList(group)
-            color     = dataset.getncattr("color")
-            for ptype in ["timeint","bias","biasscore"]:
-                for vname in [v for v in variables if ptype in v]:
-                    var = Variable(filename=fname,variable_name=vname,groupname="MeanState")
-                    try:
-                        z = int(vname.replace(ptype,""))
-                    except:
+            color = dataset.getncattr("color")
+            for vname in variables:
+                # is this a variable we need to plot?
+                pname = vname.split("_")[0]
+                if group.variables[vname][...].size <= 1:
+                    continue
+                var = Variable(
+                    filename=fname, groupname="MeanState", variable_name=vname
+                )
+
+                if (var.spatial or (var.ndata is not None)) and not var.temporal:
+                    # grab plotting options
+                    if pname not in self.limits.keys():
                         continue
-                    page.addFigure("Period Mean at %d [m]" % z,
-                                   vname,
-                                   "MNAME_RNAME_%s.png" % vname,
-                                   side   = "MODEL %s AT %d [m]" % (plbl[ptype],z),
-                                   legend = True)
+                    if pname not in space_opts:
+                        continue
+                    opts = space_opts[pname]
+
+                    # add to html layout
+                    page.addFigure(
+                        opts["section"],
+                        pname,
+                        opts["pattern"],
+                        side=opts["sidelbl"],
+                        legend=opts["haslegend"],
+                    )
+
+                    # plot variable
                     for region in self.regions:
-                        ax = var.plot(None,
-                                      region = region,
-                                      vmin   = self.limits[vname]["min"],
-                                      vmax   = self.limits[vname]["max"],
-                                      cmap   = cmap[ptype],
-                                      land   = 0.750,
-                                      water  = 0.875)
+                        ax = var.plot(
+                            None,
+                            region=region,
+                            vmin=self.limits[pname]["min"],
+                            vmax=self.limits[pname]["max"],
+                            cmap=self.limits[pname]["cmap"],
+                        )
                         fig = ax.get_figure()
-                        fig.savefig("%s/%s_%s_%s.png" % (self.output_path,m.name,region,vname))
+                        fig.savefig(
+                            os.path.join(
+                                self.output_path,
+                                "%s_%s_%s.png" % (m.name, region, pname),
+                            )
+                        )
                         plt.close()
 
-        for region in self.regions:
-
-            vname = "timelonint_of_%s_over_%s" % (self.variable,region)
-            if vname in variables:
-                var0 = Variable(filename=bname,variable_name=vname,groupname="MeanState")
-                var  = Variable(filename=fname,variable_name=vname,groupname="MeanState")
-                bias = Variable(filename=fname,variable_name=vname.replace("timelonint","timelonbias"),groupname="MeanState")
-                if region == "global":
-                    page.addFigure("Mean regional depth profiles",
-                                   "timelonint",
-                                   "MNAME_RNAME_timelonint.png",
-                                   side     = "MODEL DEPTH PROFILE",
-                                   legend   = True,
-                                   longname = "Time/longitude averaged profile")
-                    page.addFigure("Overlapping mean regional depth profiles",
-                                   "timelonints",
-                                   "MNAME_RNAME_timelonints.png",
-                                   side     = "MODEL DEPTH PROFILE",
-                                   legend   = True,
-                                   longname = "Overlapping Time/longitude averaged profile")
-                    page.addFigure("Overlapping mean regional depth profiles",
-                                   "timelonbias",
-                                   "MNAME_RNAME_timelonbias.png",
-                                   side     = "MODEL DEPTH PROFILE BIAS",
-                                   legend   = True,
-                                   longname = "Overlapping Time/longitude averaged profile bias")
-                fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-                l   = np.hstack([var .lat_bnds  [:,0],var .lat_bnds  [-1,1]])
-                d0  = np.hstack([var0.depth_bnds[:,0],var0.depth_bnds[-1,1]])
-                d   = np.hstack([var .depth_bnds[:,0],var .depth_bnds[-1,1]])
-                ind = np.all(var.data.mask,axis=0)
-                ind = np.ma.masked_array(range(ind.size),mask=ind,dtype=int)
-                b   = ind.min()
-                e   = ind.max()+1
-                ax.pcolormesh(l[b:(e+1)],d,var.data[:,b:e],
-                              vmin = self.limits["timelonint"]["global"]["min"],
-                              vmax = self.limits["timelonint"]["global"]["max"],
-                              cmap = self.cmap)
-                ax.set_xlabel("latitude")
-                ax.set_ylim((d.max(),d.min()))
-                ax.set_ylabel("depth [m]")
-                fig.savefig("%s/%s_%s_timelonint.png" % (self.output_path,m.name,region))
-                ax.set_ylim((min(d0.max(),d.max()),max(d0.min(),d.min())))
-                fig.savefig("%s/%s_%s_timelonints.png" % (self.output_path,m.name,region))
-                plt.close()
-                fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-                l   = np.hstack([bias.lat_bnds  [:,0],bias.lat_bnds  [-1,1]])
-                d0  = np.hstack([var0.depth_bnds[:,0],var0.depth_bnds[-1,1]])
-                d   = np.hstack([bias.depth_bnds[:,0],bias.depth_bnds[-1,1]])
-                ind = np.all(bias.data.mask,axis=0)
-                ind = np.ma.masked_array(range(ind.size),mask=ind,dtype=int)
-                b   = ind.min()
-                e   = ind.max()+1
-                ax.pcolormesh(l[b:(e+1)],d,bias.data[:,b:e],
-                              vmin = self.limits["timelonbias"]["global"]["min"],
-                              vmax = self.limits["timelonbias"]["global"]["max"],
-                              cmap = "seismic")
-                ax.set_xlabel("latitude")
-                ax.set_ylim((d.max(),d.min()))
-                ax.set_ylabel("depth [m]")
-                ax.set_ylim((min(d0.max(),d.max()),max(d0.min(),d.min())))
-                fig.savefig("%s/%s_%s_timelonbias.png" % (self.output_path,m.name,region))
-                plt.close()
+                    # Jumping through hoops to get the benchmark plotted and in the html output
+                    if self.master and (
+                        pname == "timeint" or pname == "phase" or pname == "iav"
+                    ):
+                        opts = space_opts[pname]
+
+                        # add to html layout
+                        page.addFigure(
+                            opts["section"],
+                            "benchmark_%s" % pname,
+                            opts["pattern"].replace("MNAME", "Benchmark"),
+                            side=opts["sidelbl"].replace("MODEL", "BENCHMARK"),
+                            legend=True,
+                        )
+
+                        # plot variable
+                        obs = Variable(
+                            filename=bname, groupname="MeanState", variable_name=vname
+                        )
+                        for region in self.regions:
+                            ax = obs.plot(
+                                None,
+                                region=region,
+                                vmin=self.limits[pname]["min"],
+                                vmax=self.limits[pname]["max"],
+                                cmap=self.limits[pname]["cmap"],
+                            )
+                            fig = ax.get_figure()
+                            fig.savefig(
+                                os.path.join(
+                                    self.output_path,
+                                    "Benchmark_%s_%s.png" % (region, pname),
+                                )
+                            )
+                            plt.close()
 
+                if not (var.spatial or (var.ndata is not None)) and var.temporal:
+                    # grab the benchmark dataset to plot along with
+                    try:
+                        obs = Variable(
+                            filename=bname, groupname="MeanState", variable_name=vname
+                        ).convert(var.unit)
+                    except:
+                        continue
 
-            vname = "cycle_of_%s_over_%s" % (self.variable,region)
-            if vname in variables:
-                var0 = Variable(filename=bname,variable_name=vname,groupname="MeanState")
-                var  = Variable(filename=fname,variable_name=vname,groupname="MeanState")
-                bias = Variable(filename=fname,variable_name=vname.replace("cycle","cyclebias"),groupname="MeanState")
-                if region == "global":
-                    page.addFigure("Mean regional annual cycle",
-                                   "cycle",
-                                   "MNAME_RNAME_cycle.png",
-                                   side     = "MODEL ANNUAL CYCLE",
-                                   legend   = True,
-                                   longname = "Annual cycle")
-                    page.addFigure("Overlapping mean regional annual cycle",
-                                   "cycles",
-                                   "MNAME_RNAME_cycles.png",
-                                   side     = "MODEL ANNUAL CYCLE",
-                                   legend   = True,
-                                   longname = "Overlapping annual cycle")
-                    page.addFigure("Overlapping mean regional annual cycle",
-                                   "cyclebias",
-                                   "MNAME_RNAME_cyclebias.png",
-                                   side     = "MODEL ANNUAL CYCLE BIAS",
-                                   legend   = True,
-                                   longname = "Overlapping annual cycle bias")
-                fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-                d0  = np.hstack([var0.depth_bnds[:,0],var0.depth_bnds[-1,1]])
-                d   = np.hstack([var .depth_bnds[:,0],var .depth_bnds[-1,1]])
-                ax.pcolormesh(bnd_months,d,var.data.T,
-                              vmin = self.limits["cycle"]["global"]["min"],
-                              vmax = self.limits["cycle"]["global"]["max"],
-                              cmap = self.cmap)
-                ax.set_xticks     (mid_months)
-                ax.set_xticklabels(lbl_months)
-                ax.set_ylim((d.max(),d.min()))
-                ax.set_ylabel("depth [m]")
-                fig.savefig("%s/%s_%s_cycle.png" % (self.output_path,m.name,region))
-                ax.set_ylim((min(d0.max(),d.max()),max(d0.min(),d.min())))
-                fig.savefig("%s/%s_%s_cycles.png" % (self.output_path,m.name,region))
-                plt.close()
-                fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-                ax.pcolormesh(bnd_months,
-                              np.hstack([bias.depth_bnds[:,0],bias.depth_bnds[-1,1]]),
-                              bias.data.T,
-                              vmin = self.limits["cyclebias"]["global"]["min"],
-                              vmax = self.limits["cyclebias"]["global"]["max"],
-                              cmap = "seismic")
-                ax.set_xticks     (mid_months)
-                ax.set_xticklabels(lbl_months)
-                ax.set_ylim((d.max(),d.min()))
-                ax.set_ylabel("depth [m]")
-                ax.set_ylim((min(d0.max(),d.max()),max(d0.min(),d.min())))
-                fig.savefig("%s/%s_%s_cyclebias.png" % (self.output_path,m.name,region))
-                plt.close()
+                    # grab plotting options
+                    if pname not in time_opts:
+                        continue
+                    opts = time_opts[pname]
 
+                    # add to html layout
+                    page.addFigure(
+                        opts["section"],
+                        pname,
+                        opts["pattern"],
+                        side=opts["sidelbl"],
+                        legend=opts["haslegend"],
+                    )
 
-        # benchmark plots
-        if not self.master: return
-        with Dataset(bname) as dataset:
-            group     = dataset.groups["MeanState"]
-            variables = getVariableList(group)
-            color     = dataset.getncattr("color")
-            for ptype in ["timeint"]:
-                for vname in [v for v in variables if ptype in v]:
-                    var = Variable(filename=bname,variable_name=vname,groupname="MeanState")
-                    z   = int(vname.replace(ptype,""))
-                    page.addFigure("Period Mean at %d [m]" % z,
-                                   "benchmark_%s" % vname,
-                                   "Benchmark_RNAME_%s.png" % vname,
-                                   side   = "BENCHMARK %s AT %d [m]" % (plbl[ptype],z),
-                                   legend = True)
+                    # plot variable
                     for region in self.regions:
-                        ax = var.plot(None,
-                                      region = region,
-                                      vmin   = self.limits[vname]["min"],
-                                      vmax   = self.limits[vname]["max"],
-                                      cmap   = cmap[ptype],
-                                      land   = 0.750,
-                                      water  = 0.875)
-                        fig = ax.get_figure()
-                        fig.savefig("%s/Benchmark_%s_%s.png" % (self.output_path,region,vname))
+                        if region not in vname:
+                            continue
+                        fig, ax = plt.subplots(figsize=(6.8, 2.8), tight_layout=True)
+                        obs.plot(ax, lw=2, color="k", alpha=0.5)
+                        var.plot(
+                            ax,
+                            lw=2,
+                            color=color,
+                            label=m.name,
+                            ticks=opts["ticks"],
+                            ticklabels=opts["ticklabels"],
+                        )
+
+                        dy = 0.05 * (
+                            self.limits[pname][region]["max"]
+                            - self.limits[pname][region]["min"]
+                        )
+                        ax.set_ylim(
+                            self.limits[pname][region]["min"] - dy,
+                            self.limits[pname][region]["max"] + dy,
+                        )
+                        ylbl = opts["ylabel"]
+                        if ylbl == "unit":
+                            ylbl = post.UnitStringToMatplotlib(var.unit)
+                        ax.set_ylabel(ylbl)
+                        fig.savefig(
+                            os.path.join(
+                                self.output_path,
+                                "%s_%s_%s.png" % (m.name, region, pname),
+                            )
+                        )
                         plt.close()
 
-        for region in self.regions:
+        logger.info("[%s][%s] Success" % (self.longname, m.name))
 
-            vname = "timelonint_of_%s_over_%s" % (self.variable,region)
-            if vname in variables:
-                var0 = Variable(filename=fname,variable_name=vname,groupname="MeanState")
-                var  = Variable(filename=bname,variable_name=vname,groupname="MeanState")
-                if region == "global":
-                    page.addFigure("Mean regional depth profiles",
-                                   "benchmark_timelonint",
-                                   "Benchmark_RNAME_timelonint.png",
-                                   side   = "BENCHMARK DEPTH PROFILE",
-                                   legend = True,
-                                   longname = "Time/longitude averaged profile")
-                    page.addFigure("Overlapping mean regional depth profiles",
-                                   "benchmark_timelonints",
-                                   "Benchmark_RNAME_timelonints.png",
-                                   side   = "BENCHMARK DEPTH PROFILE",
-                                   legend = True,
-                                   longname = "Overlapping Time/longitude averaged profile")
-                fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-                l   = np.hstack([var .lat_bnds  [:,0],var .lat_bnds  [-1,1]])
-                d0  = np.hstack([var0.depth_bnds[:,0],var0.depth_bnds[-1,1]])
-                d   = np.hstack([var .depth_bnds[:,0],var .depth_bnds[-1,1]])
-                ind = np.all(var.data.mask,axis=0)
-                ind = np.ma.masked_array(range(ind.size),mask=ind,dtype=int)
-                b   = ind.min()
-                e   = ind.max()+1
-                ax.pcolormesh(l[b:(e+1)],d,var.data[:,b:e],
-                              vmin = self.limits["timelonint"]["global"]["min"],
-                              vmax = self.limits["timelonint"]["global"]["max"],
-                              cmap = self.cmap)
-                ax.set_xlabel("latitude")
-                ax.set_ylim((d.max(),d.min()))
-                ax.set_ylabel("depth [m]")
-                fig.savefig("%s/Benchmark_%s_timelonint.png" % (self.output_path,region))
-                ax.set_ylim((min(d0.max(),d.max()),max(d0.min(),d.min())))
-                fig.savefig("%s/Benchmark_%s_timelonints.png" % (self.output_path,region))
-                plt.close()
+    def sitePlots(self, m):
+        """ """
+        if not self.hasSites:
+            return
+
+        obs, mod = self.stageData(m)
+        for i in range(obs.ndata):
+            fig, ax = plt.subplots(figsize=(6.8, 2.8), tight_layout=True)
+            tmask = np.where(mod.data.mask[:, i] == False)[0]
+            if tmask.size > 0:
+                tmin, tmax = tmask[[0, -1]]
+            else:
+                tmin = 0
+                tmax = mod.time.size - 1
 
-            vname = "cycle_of_%s_over_%s" % (self.variable,region)
-            if vname in variables:
-                var0 = Variable(filename=bname,variable_name=vname,groupname="MeanState")
-                var  = Variable(filename=fname,variable_name=vname,groupname="MeanState")
-                if region == "global":
-                    page.addFigure("Mean regional annual cycle",
-                                   "benchmark_cycle",
-                                   "Benchmark_RNAME_cycle.png",
-                                   side     = "BENCHMARK ANNUAL CYCLE",
-                                   legend   = True,
-                                   longname = "Annual cycle")
-                    page.addFigure("Overlapping mean regional annual cycle",
-                                   "benchmark_cycles",
-                                   "Benchmark_RNAME_cycles.png",
-                                   side     = "BENCHMARK ANNUAL CYCLE",
-                                   legend   = True,
-                                   longname = "Overlapping annual cycle")
-                fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-                d  = np.hstack([var0.depth_bnds[:,0],var0.depth_bnds[-1,1]])
-                d0 = np.hstack([var .depth_bnds[:,0],var .depth_bnds[-1,1]])
-                ax.pcolormesh(bnd_months,d,var0.data.T,
-                              vmin = self.limits["cycle"]["global"]["min"],
-                              vmax = self.limits["cycle"]["global"]["max"],
-                              cmap = self.cmap)
-                ax.set_xticks     (mid_months)
-                ax.set_xticklabels(lbl_months)
-                ax.set_ylim((d.max(),d.min()))
-                ax.set_ylabel("depth [m]")
-                fig.savefig("%s/%s_%s_cycle.png" % (self.output_path,"Benchmark",region))
-                ax.set_ylim((min(d0.max(),d.max()),max(d0.min(),d.min())))
-                fig.savefig("%s/%s_%s_cycles.png" % (self.output_path,"Benchmark",region))
-                plt.close()
+            t = mod.time[tmin : (tmax + 1)]
+            x = mod.data[tmin : (tmax + 1), i]
+            y = obs.data[tmin : (tmax + 1), i]
+            ax.plot(t, y, "-k", lw=2, alpha=0.5)
+            ax.plot(t, x, "-", color=m.color)
+
+            ind = np.where(t % 365 < 30.0)[0]
+            ticks = t[ind] - (t[ind] % 365)
+            ticklabels = (ticks / 365.0 + 1850.0).astype(int)
+            ax.set_xticks(ticks)
+            ax.set_xticklabels(ticklabels)
+            ax.set_ylabel(post.UnitStringToMatplotlib(mod.unit))
+            fig.savefig(
+                os.path.join(
+                    self.output_path, "%s_%s_%s.png" % (m.name, self.lbls[i], "time")
+                )
+            )
+            plt.close()
+
+    def generateHtml(self):
+        """Generate the HTML for the results of this confrontation.
+
+        This routine opens all netCDF files and builds a table of
+        metrics. Then it passes the results to the HTML generator and
+        saves the result in the output directory. This only occurs on
+        the confrontation flagged as master.
+
+        """
+        # only the master processor needs to do this
+        if not self.master:
+            return
+
+        for page in self.layout.pages:
+            # build the metric dictionary
+            metrics = {}
+            page.models = []
+            for fname in glob.glob(os.path.join(self.output_path, "*.nc")):
+                with Dataset(fname) as dataset:
+                    mname = dataset.getncattr("name")
+                    if mname != "Benchmark":
+                        page.models.append(mname)
+                    if page.name not in dataset.groups:
+                        continue
+                    group = dataset.groups[page.name]
 
-    def determinePlotLimits(self):
+                    # if the dataset opens, we need to add the model (table row)
+                    metrics[mname] = {}
 
-        # Pick limit type
-        max_str = "up99"; min_str = "dn99"
-        if self.keywords.get("limit_type","99per") == "minmax":
-            max_str = "max"; min_str = "min"
+                    # each model will need to have all regions
+                    for region in self.regions:
+                        metrics[mname][region] = {}
 
-        # Determine the min/max of variables over all models
-        limits = {}
-        for fname in glob.glob("%s/*.nc" % self.output_path):
-            with Dataset(fname) as dataset:
-                if "MeanState" not in dataset.groups: continue
-                group     = dataset.groups["MeanState"]
-                variables = [v for v in group.variables.keys() if (v not in group.dimensions.keys() and
-                                                                   "_bnds" not in v                 and
-                                                                   group.variables[v][...].size > 1)]
-                for vname in variables:
-                    var    = group.variables[vname]
-                    pname  = vname.split("_")[ 0]
-                    if "_score" in vname:
-                        pname = "_".join(vname.split("_")[:2])
-                    if "_over_" in vname:
-                        region = vname.split("_over_")[-1]
-                        if pname not in limits: limits[pname] = {}
-                        if region not in limits[pname]:
-                            limits[pname][region] = {}
-                            limits[pname][region]["min"]  = +1e20
-                            limits[pname][region]["max"]  = -1e20
-                            limits[pname][region]["unit"] = post.UnitStringToMatplotlib(var.getncattr("units"))
-                        limits[pname][region]["min"] = min(limits[pname][region]["min"],var.getncattr("min"))
-                        limits[pname][region]["max"] = max(limits[pname][region]["max"],var.getncattr("max"))
-                    else:
-                        if pname not in limits:
-                            limits[pname] = {}
-                            limits[pname]["min"]  = +1e20
-                            limits[pname]["max"]  = -1e20
-                            limits[pname]["unit"] = post.UnitStringToMatplotlib(var.getncattr("units"))
-                        limits[pname]["min"] = min(limits[pname]["min"],var.getncattr(min_str))
-                        limits[pname]["max"] = max(limits[pname]["max"],var.getncattr(max_str))
+                    # columns in the table will be in the scalars group
+                    if "scalars" not in group.groups:
+                        continue
 
-        # Another pass to fix score limits
-        for pname in limits.keys():
-            if "score" in pname:
-                if "min" in limits[pname].keys():
-                    limits[pname]["min"] = 0.
-                    limits[pname]["max"] = 1.
-                else:
-                    for region in limits[pname].keys():
-                        limits[pname][region]["min"] = 0.
-                        limits[pname][region]["max"] = 1.
-        self.limits = limits
+                    # we add scalars to the model/region based on the region
+                    # name being in the variable name. If no region is found,
+                    # we assume it is the global region.
+                    grp = group.groups["scalars"]
+                    for vname in grp.variables.keys():
+                        found = False
+                        for region in self.regions:
+                            if vname.endswith(" %s" % region):
+                                found = True
+                                var = grp.variables[vname]
+                                name = "".join(vname.rsplit(" %s" % region, 1))
+                                metrics[mname][region][name] = Variable(
+                                    name=name, unit=var.units, data=var[...]
+                                )
+                        if not found:
+                            var = grp.variables[vname]
+                            if "global" not in metrics[mname]:
+                                logger.debug(
+                                    "[%s][%s] 'global' not in region list = [%s]"
+                                    % (self.longname, mname, ",".join(self.regions))
+                                )
+                                raise ValueError()
+                            metrics[mname]["global"][vname] = Variable(
+                                name=vname, unit=var.units, data=var[...]
+                            )
+            page.setMetrics(metrics)
+
+        # write the HTML page
+        with open(
+            os.path.join(self.output_path, "%s.html" % (self.name)),
+            "w",
+            encoding="utf-8",
+        ) as f:
+            f.write(str(self.layout))
+
+    def _relationship(self, m, nbin=25):
+        """ """
+
+        def _retrieveData(filename):
+            key = None
+            with Dataset(filename, mode="r") as dset:
+                key = [
+                    v
+                    for v in dset.groups["MeanState"].variables.keys()
+                    if "timeint_" in v
+                ]
+            return Variable(
+                filename=filename, groupname="MeanState", variable_name=key[0]
+            )
+
+        # If there are no relationships to analyze, get out of here
+        if self.relationships is None:
+            return
+
+        # Get the HTML page
+        page = [page for page in self.layout.pages if "Relationships" in page.name]
+        if len(page) == 0:
+            return
+        page = page[0]
+
+        # Try to get the dependent data from the model and obs
+        try:
+            ref_dep = _retrieveData(
+                os.path.join(self.output_path, "%s_%s.nc" % (self.name, "Benchmark"))
+            )
+            com_dep = _retrieveData(
+                os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
+            )
+            dep_name = self.longname.split("/")[0]
+            ref_dep.name = "%s/%s" % (dep_name, self.name)
+            com_dep.name = "%s/%s" % (dep_name, m.name)
+        except:
+            return
+
+        with Dataset(
+            os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name)), mode="r+"
+        ) as results:
+            # Grab/create a relationship and scalars group
+            group = None
+            if "Relationships" not in results.groups:
+                group = results.createGroup("Relationships")
+            else:
+                group = results.groups["Relationships"]
+            if "scalars" not in group.groups:
+                scalars = group.createGroup("scalars")
+            else:
+                scalars = group.groups["scalars"]
 
-        # Second pass to plot legends
-        cmaps = {"bias"       :"seismic",
-                 "timelonbias":"seismic",
-                 "cyclebias"  :"seismic",
-                 "rmse"       :"YlOrRd"}
-        for pname in limits.keys():
+            # for each relationship...
+            for c in self.relationships:
+                # try to get the independent data from the model and obs
+                try:
+                    ref_ind = _retrieveData(
+                        os.path.join(c.output_path, "%s_%s.nc" % (c.name, "Benchmark"))
+                    )
+                    com_ind = _retrieveData(
+                        os.path.join(c.output_path, "%s_%s.nc" % (c.name, m.name))
+                    )
+                    ind_name = c.longname.split("/")[0]
+                    ref_ind.name = "%s/%s" % (ind_name, c.name)
+                    com_ind.name = "%s/%s" % (ind_name, m.name)
+                except:
+                    continue
 
-            base_pname = pname
-            m = re.search("(\D+)\d+",pname)
-            if m: base_pname = m.group(1)
-
-            # Pick colormap
-            cmap = self.cmap
-            if base_pname in cmaps:
-                cmap = cmaps[base_pname]
-            elif "score" in pname:
-                cmap = "score"
-
-            # Need to symetrize?
-            if base_pname in ["bias","timelonbias","cyclebias"]:
-                if "min" in limits[pname]:
-                    vabs =  max(abs(limits[pname]["max"]),abs(limits[pname]["min"]))
-                    limits[pname]["min"] = -vabs
-                    limits[pname]["max"] =  vabs
-                else:
-                    vabs =  max(abs(limits[pname]["global"]["max"]),abs(limits[pname]["global"]["min"]))
-                    limits[pname]["global"]["min"] = -vabs
-                    limits[pname]["global"]["max"] =  vabs
-
-            # Some plots need legends
-            if base_pname in ["timeint","bias","biasscore","rmse","rmsescore","timelonint","timelonbias","cycle","cyclebias"]:
-                if "min" in limits[pname]:
-                    fig,ax = plt.subplots(figsize=(6.8,1.0),tight_layout=True)
-                    post.ColorBar(ax,
-                                  vmin  = limits[pname]["min" ],
-                                  vmax  = limits[pname]["max" ],
-                                  label = limits[pname]["unit"],
-                                  cmap  = cmap)
-                    fig.savefig("%s/legend_%s.png" % (self.output_path,pname))
-                    if base_pname == "timelonint" or base_pname == "cycle":
-                        fig.savefig("%s/legend_%ss.png" % (self.output_path,pname))
+                # if any one of the data sources are sites, they all
+                # should be, also check that the lat/lons are the same
+                src = [ref_ind, ref_dep, com_ind, com_dep]
+                for i, a in enumerate(src):
+                    for j, b in enumerate(src):
+                        if i == j:
+                            continue
+                        if a.ndata and b.ndata:
+                            assert np.allclose(a.lat, b.lat) * np.allclose(a.lon, b.lon)
+                        if a.ndata and not b.ndata:
+                            src[j] = b.extractDatasites(a.lat, a.lon)
+                ref_ind, ref_dep, com_ind, com_dep = src
+
+                # create relationships
+                ref = Relationship(ref_ind, ref_dep, order=1)
+                com = Relationship(com_ind, com_dep, order=1)
+
+                # set limits to global across models
+                ref.limits = (
+                    [self.limits["timeint"]["min"], self.limits["timeint"]["max"]],
+                    [c.limits["timeint"]["min"], c.limits["timeint"]["max"]],
+                )
+                com.limits = (
+                    [self.limits["timeint"]["min"], self.limits["timeint"]["max"]],
+                    [c.limits["timeint"]["min"], c.limits["timeint"]["max"]],
+                )
+
+                # Add figures to the html page
+                page.addFigure(
+                    c.longname,
+                    "benchmark_rel_%s" % ind_name,
+                    "Benchmark_RNAME_rel_%s.png" % ind_name,
+                    legend=False,
+                    benchmark=False,
+                )
+                page.addFigure(
+                    c.longname,
+                    "rel_%s" % ind_name,
+                    "MNAME_RNAME_rel_%s.png" % ind_name,
+                    legend=False,
+                    benchmark=False,
+                )
+                page.addFigure(
+                    c.longname,
+                    "rel_func_%s" % ind_name,
+                    "MNAME_RNAME_rel_func_%s.png" % ind_name,
+                    legend=False,
+                    benchmark=False,
+                )
+
+                # Analysis over regions
+                longname = c.longname.replace(
+                    "/", "|"
+                )  # we want the source too, but netCDF doesn't like the '/'
+                for region in self.regions:
+                    ref.makeComparable(com, region=region)
+
+                    # Make the plots
+                    fig, ax = plt.subplots(figsize=(6, 5.25), tight_layout=True)
+                    ref.plotDistribution(ax, region=region)
+                    fig.savefig(
+                        os.path.join(
+                            self.output_path,
+                            "%s_%s_rel_%s.png" % ("Benchmark", region, ind_name),
+                        )
+                    )
                     plt.close()
-                else:
-                    fig,ax = plt.subplots(figsize=(6.8,1.0),tight_layout=True)
-                    post.ColorBar(ax,
-                                  vmin  = limits[pname]["global"]["min" ],
-                                  vmax  = limits[pname]["global"]["max" ],
-                                  label = limits[pname]["global"]["unit"],
-                                  cmap  = cmap)
-                    fig.savefig("%s/legend_%s.png" % (self.output_path,pname))
-                    if base_pname == "timelonint" or base_pname == "cycle":
-                        fig.savefig("%s/legend_%ss.png" % (self.output_path,pname))
+
+                    fig, ax = plt.subplots(figsize=(6, 5.25), tight_layout=True)
+                    com.plotDistribution(ax, region=region)
+                    fig.savefig(
+                        os.path.join(
+                            self.output_path,
+                            "%s_%s_rel_%s.png" % (m.name, region, ind_name),
+                        )
+                    )
                     plt.close()
 
-    def compositePlots(self):
-        pass
+                    fig, ax = plt.subplots(figsize=(6, 5.25), tight_layout=True)
+                    com.plotFunction(ax, region=region, shift=-0.1, color=m.color)
+                    ref.plotFunction(ax, region=region, shift=+0.1)
+                    fig.savefig(
+                        os.path.join(
+                            self.output_path,
+                            "%s_%s_rel_func_%s.png" % (m.name, region, ind_name),
+                        )
+                    )
+                    plt.close()
+
+                    # Score the distribution
+                    score = ref.scoreHellinger(com, region=region)
+                    sname = "%s Hellinger Distance %s" % (longname, region)
+                    if sname in scalars.variables:
+                        scalars.variables[sname][0] = score
+                    else:
+                        Variable(name=sname, unit="1", data=score).toNetCDF4(
+                            results, group="Relationships"
+                        )
+
+                    # Score the functional response
+                    score = ref.scoreRMSE(com, region=region)
+                    sname = "%s Score %s" % (longname, region)
+                    if sname in scalars.variables:
+                        scalars.variables[sname][0] = score
+                    else:
+                        Variable(name=sname, unit="1", data=score).toNetCDF4(
+                            results, group="Relationships"
+                        )
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfPermafrost.py` & `ILAMB-2.7/src/ILAMB/ConfPermafrost.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,313 +1,443 @@
-from .Confrontation import Confrontation
 import cartopy.crs as ccrs
 import cartopy.feature as cfeature
-from .Variable import Variable
-from .Post import ColorBar
 import matplotlib.pyplot as plt
-from netCDF4 import Dataset
-from . import ilamblib as il
 import numpy as np
+from netCDF4 import Dataset
+
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Post import ColorBar
+from ILAMB.Variable import Variable
 
-def _ALTFromTSL(tsl,dmax=3.5,dres=0.01,Teps=273.15):
-    """
 
-    """
+def _ALTFromTSL(tsl, dmax=3.5, dres=0.01, Teps=273.15):
+    """ """
     from scipy.interpolate import interp1d
 
     # find the annual cycle
-    mask = tsl.data.mask.all(axis=(0,1))
+    mask = tsl.data.mask.all(axis=(0, 1))
     area = tsl.area
-    tsl  = tsl.annualCycle()
+    tsl = tsl.annualCycle()
 
     # which month is the mean soil temperature maximum?
     T = tsl.data.mean(axis=1).argmax(axis=0)
-    T = T[np.newaxis,np.newaxis,...]*np.ones((1,)+tsl.data.shape[-3:],dtype=int)
-    D,X,Y = np.ix_(np.arange(tsl.data.shape[1]),
-                   np.arange(tsl.data.shape[2]),
-                   np.arange(tsl.data.shape[3]))
-    d   = tsl.depth
-    tsl = tsl.data[T,D,X,Y]
+    T = T[np.newaxis, np.newaxis, ...] * np.ones((1,) + tsl.data.shape[-3:], dtype=int)
+    D, X, Y = np.ix_(
+        np.arange(tsl.data.shape[1]),
+        np.arange(tsl.data.shape[2]),
+        np.arange(tsl.data.shape[3]),
+    )
+    d = tsl.depth
+    tsl = tsl.data[T, D, X, Y]
     tsl = tsl.reshape(tsl.shape[-3:])
 
     # now we interpolate to find a more precise active layer thickness
-    D   = np.arange(0,dmax+0.1*dres,dres)
-    TSL = interp1d(d,tsl,axis=0,fill_value="extrapolate")
+    D = np.arange(0, dmax + 0.1 * dres, dres)
+    TSL = interp1d(d, tsl, axis=0, fill_value="extrapolate")
     TSL = TSL(D)
 
     # the active layer thickness is then the sum of all level
     # thicknesses whose temperature is greater than the threshold.
-    ALT = np.ma.masked_array((TSL>Teps).sum(axis=0)*dres,mask=mask+(TSL>Teps).all(axis=0))
+    ALT = np.ma.masked_array(
+        (TSL > Teps).sum(axis=0) * dres, mask=mask + (TSL > Teps).all(axis=0)
+    )
 
     return ALT
 
-class ConfPermafrost(Confrontation):
-
-    def __init__(self,**keywords):
 
+class ConfPermafrost(Confrontation):
+    def __init__(self, **keywords):
         # Ugly, but this is how we call the Confrontation constructor
-        super(ConfPermafrost,self).__init__(**keywords)
+        super(ConfPermafrost, self).__init__(**keywords)
 
         # Now we overwrite some things which are different here
         self.layout
-        self.regions        = ["global"]
+        self.regions = ["global"]
         self.layout.regions = self.regions
-        self.weight         = { "Missed Score" : 1.,
-                                "Excess Score" : 1. }
+        self.weight = {"Missed Score": 1.0, "Excess Score": 1.0}
         for page in self.layout.pages:
-            page.setMetricPriority(["Total Area"  ,
-                                    "Overlap Area",
-                                    "Missed Area" ,
-                                    "Excess Area" ,
-                                    "Missed Score"   ,
-                                    "Excess Score"   ,
-                                    "Overall Score"])
-
-    def stageData(self,m):
+            page.setMetricPriority(
+                [
+                    "Total Area",
+                    "Overlap Area",
+                    "Missed Area",
+                    "Excess Area",
+                    "Missed Score",
+                    "Excess Score",
+                    "Overall Score",
+                ]
+            )
 
-        obs = Variable(filename      = self.source,
-                       variable_name = "permafrost_extent")
+    def stageData(self, m):
+        obs = Variable(filename=self.source, variable_name="permafrost_extent")
 
         # These parameters may be changed from the configure file
-        y0   = float(self.keywords.get("y0"  ,1970.))  # [yr]      beginning year to include in analysis
-        yf   = float(self.keywords.get("yf"  ,2000.))  # [yr]      end year to include in analysis
-        dmax = float(self.keywords.get("dmax",3.5))    # [m]       consider layers where depth in is the range [0,dmax]
-        Teps = float(self.keywords.get("Teps",273.15)) # [K]       temperature below which we assume permafrost occurs
-
-        t0  = (y0  -1850.)*365.
-        tf  = (yf+1-1850.)*365.
-        mod = m.extractTimeSeries(self.variable,
-                                  initial_time = t0,
-                                  final_time   = tf)
-        mod.trim(t   = [t0           ,tf  ],
-                 lat = [max(obs.lat.min(),mod.lat.min()), 90  ],
-                 d   = [0            ,dmax]) #.annualCycle()
+        y0 = float(
+            self.keywords.get("y0", 1970.0)
+        )  # [yr]      beginning year to include in analysis
+        yf = float(
+            self.keywords.get("yf", 2000.0)
+        )  # [yr]      end year to include in analysis
+        dmax = float(
+            self.keywords.get("dmax", 3.5)
+        )  # [m]       consider layers where depth in is the range [0,dmax]
+        Teps = float(
+            self.keywords.get("Teps", 273.15)
+        )  # [K]       temperature below which we assume permafrost occurs
+
+        t0 = (y0 - 1850.0) * 365.0
+        tf = (yf + 1 - 1850.0) * 365.0
+        mod = m.extractTimeSeries(self.variable, initial_time=t0, final_time=tf)
+        mod.trim(
+            t=[t0, tf], lat=[max(obs.lat.min(), mod.lat.min()), 90], d=[0, dmax]
+        )  # .annualCycle()
 
         alt = _ALTFromTSL(mod)
-        mod = Variable(name = "permafrost_extent",
-                       unit = "1",
-                       data = np.ma.masked_array((alt>=0).astype(float),mask=alt.mask),
-                       lat  = mod.lat,
-                       lon  = mod.lon,
-                       area = mod.area)
-        return obs,mod
+        mod = Variable(
+            name="permafrost_extent",
+            unit="1",
+            data=np.ma.masked_array((alt >= 0).astype(float), mask=alt.mask),
+            lat=mod.lat,
+            lon=mod.lon,
+            area=mod.area,
+        )
+        return obs, mod
 
-    def confront(self,m):
-
-        obs,mod  = self.stageData(m)
+    def confront(self, m):
+        obs, mod = self.stageData(m)
         obs_area = obs.integrateInSpace().convert("1e6 km2")
         mod_area = mod.integrateInSpace().convert("1e6 km2")
 
         # Interpolate to a composed grid
-        lat,lon,lat_bnds,lon_bnds = il._composeGrids(obs,mod)
-        OBS = obs.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
-        MOD = mod.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
+        lat, lon, lat_bnds, lon_bnds = il._composeGrids(obs, mod)
+        OBS = obs.interpolate(lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds)
+        MOD = mod.interpolate(lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds)
 
         # Compute the different extent areas
-        o_mask       =  OBS.data.mask
-        o_land       = (OBS.area > 1e-12)*(o_mask==0)
-        o_data       =  np.copy(OBS.data.data)
-        o_data[np.where(OBS.data.mask)] = 0.
-
-        m_mask       =  MOD.data.mask
-        m_land       = (MOD.area > 1e-12)
-        m_data       =  np.copy(MOD.data.data)
-        m_data[np.where(MOD.data.mask)] = 0.
-
-        o_and_m      = (np.abs(o_data-1)<1e-12)*(np.abs(m_data-1)<1e-12)
-        o_not_m_land = (np.abs(o_data-1)<1e-12)*(np.abs(m_data  )<1e-12)*(m_land==0)
-        o_not_m_miss = (np.abs(o_data-1)<1e-12)*(np.abs(m_data  )<1e-12)*(m_land==1)
-        o_zones      = 1.*o_and_m
-        o_zones     += 2.*o_not_m_land
-        o_zones     += 4.*o_not_m_miss
-        o_zones = np.ma.masked_array(o_zones,mask=o_mask)
-
-        m_and_o      = (np.abs(m_data-1)<1e-12)*(np.abs(o_data-1)<1e-12)
-        m_not_o_land = (np.abs(m_data-1)<1e-12)*(np.abs(o_data  )<1e-12)*(o_land==0)
-        m_not_o_miss = (np.abs(m_data-1)<1e-12)*(np.abs(o_data  )<1e-12)*(o_land==1)
-        m_zones      = 1.*m_and_o
-        m_zones     += 2.*m_not_o_land
-        m_zones     += 4.*m_not_o_miss
-        m_zones = np.ma.masked_array(m_zones,mask=m_mask)
-
-        zones  = 1.*o_and_m
-        zones += 2.*o_not_m_miss
-        zones += 4.*m_not_o_miss
-        zones += 8.*m_not_o_land
-        zones  = np.ma.masked_less(zones,1)
-        for i,u in enumerate(np.unique(zones.compressed())):
-            zones[np.where(zones==u)] = i
+        o_mask = OBS.data.mask
+        o_land = (OBS.area > 1e-12) * (o_mask == 0)
+        o_data = np.copy(OBS.data.data)
+        o_data[np.where(OBS.data.mask)] = 0.0
+
+        m_mask = MOD.data.mask
+        m_land = MOD.area > 1e-12
+        m_data = np.copy(MOD.data.data)
+        m_data[np.where(MOD.data.mask)] = 0.0
+
+        o_and_m = (np.abs(o_data - 1) < 1e-12) * (np.abs(m_data - 1) < 1e-12)
+        o_not_m_land = (
+            (np.abs(o_data - 1) < 1e-12) * (np.abs(m_data) < 1e-12) * (m_land == 0)
+        )
+        o_not_m_miss = (
+            (np.abs(o_data - 1) < 1e-12) * (np.abs(m_data) < 1e-12) * (m_land == 1)
+        )
+        o_zones = 1.0 * o_and_m
+        o_zones += 2.0 * o_not_m_land
+        o_zones += 4.0 * o_not_m_miss
+        o_zones = np.ma.masked_array(o_zones, mask=o_mask)
+
+        m_and_o = (np.abs(m_data - 1) < 1e-12) * (np.abs(o_data - 1) < 1e-12)
+        m_not_o_land = (
+            (np.abs(m_data - 1) < 1e-12) * (np.abs(o_data) < 1e-12) * (o_land == 0)
+        )
+        m_not_o_miss = (
+            (np.abs(m_data - 1) < 1e-12) * (np.abs(o_data) < 1e-12) * (o_land == 1)
+        )
+        m_zones = 1.0 * m_and_o
+        m_zones += 2.0 * m_not_o_land
+        m_zones += 4.0 * m_not_o_miss
+        m_zones = np.ma.masked_array(m_zones, mask=m_mask)
+
+        zones = 1.0 * o_and_m
+        zones += 2.0 * o_not_m_miss
+        zones += 4.0 * m_not_o_miss
+        zones += 8.0 * m_not_o_land
+        zones = np.ma.masked_less(zones, 1)
+        for i, u in enumerate(np.unique(zones.compressed())):
+            zones[np.where(zones == u)] = i
 
         # compute the intersection of obs and mod
-        obs_and_mod = Variable(name = "obs_and_mod",
-                               unit = "1",
-                               data = np.ma.masked_values(zones==0,0).astype(float),
-                               lat  = lat, lon = lon, area = MOD.area)
+        obs_and_mod = Variable(
+            name="obs_and_mod",
+            unit="1",
+            data=np.ma.masked_values(zones == 0, 0).astype(float),
+            lat=lat,
+            lon=lon,
+            area=MOD.area,
+        )
 
         # compute the obs that is not the mod
-        data = (OBS.data.mask==0)*(MOD.data.mask==1)
-        obs_not_mod = Variable(name = "obs_not_mod",
-                               unit = "1",
-                               data = np.ma.masked_values(zones==1,0).astype(float),
-                               lat  = lat, lon  = lon, area = OBS.area)
+        data = (OBS.data.mask == 0) * (MOD.data.mask == 1)
+        obs_not_mod = Variable(
+            name="obs_not_mod",
+            unit="1",
+            data=np.ma.masked_values(zones == 1, 0).astype(float),
+            lat=lat,
+            lon=lon,
+            area=OBS.area,
+        )
 
         # compute the mod that is not the obs
-        data = (OBS.data.mask==1)*(MOD.data.mask==0)
-        mod_not_obs = Variable(name = "mod_not_obs",
-                               unit = "1",
-                               data = np.ma.masked_values(zones==2,0).astype(float),
-                               lat  = lat, lon  = lon, area = MOD.area)
+        data = (OBS.data.mask == 1) * (MOD.data.mask == 0)
+        mod_not_obs = Variable(
+            name="mod_not_obs",
+            unit="1",
+            data=np.ma.masked_values(zones == 2, 0).astype(float),
+            lat=lat,
+            lon=lon,
+            area=MOD.area,
+        )
 
         # compute the mod that is not the obs but because of land representation
-        data = (OBS.data.mask==1)*(MOD.data.mask==0)
-        mod_not_obs_land = Variable(name = "mod_not_obs_land",
-                                    unit = "1",
-                                    data = np.ma.masked_values(zones==3,0).astype(float),
-                                    lat  = lat, lon  = lon, area = MOD.area)
+        data = (OBS.data.mask == 1) * (MOD.data.mask == 0)
+        mod_not_obs_land = Variable(
+            name="mod_not_obs_land",
+            unit="1",
+            data=np.ma.masked_values(zones == 3, 0).astype(float),
+            lat=lat,
+            lon=lon,
+            area=MOD.area,
+        )
 
         # compute areas
         obs_and_mod_area = obs_and_mod.integrateInSpace().convert("1e6 km2")
         obs_not_mod_area = obs_not_mod.integrateInSpace().convert("1e6 km2")
         mod_not_obs_area = mod_not_obs.integrateInSpace().convert("1e6 km2")
         mod_not_obs_land_area = mod_not_obs_land.integrateInSpace().convert("1e6 km2")
 
         # determine score
-        obs_score = Variable(name = "Missed Score global",
-                             unit = "1",
-                             data = obs_and_mod_area.data / (obs_and_mod_area.data + obs_not_mod_area.data))
-        mod_score = Variable(name = "Excess Score global",
-                             unit = "1",
-                             data = obs_and_mod_area.data / (obs_and_mod_area.data + mod_not_obs_area.data))
+        obs_score = Variable(
+            name="Missed Score global",
+            unit="1",
+            data=obs_and_mod_area.data
+            / (obs_and_mod_area.data + obs_not_mod_area.data),
+        )
+        mod_score = Variable(
+            name="Excess Score global",
+            unit="1",
+            data=obs_and_mod_area.data
+            / (obs_and_mod_area.data + mod_not_obs_area.data),
+        )
 
         # Write to datafiles --------------------------------------
 
-        obs_area.name         = "Total Area"
-        mod_area.name         = "Total Area"
+        obs_area.name = "Total Area"
+        mod_area.name = "Total Area"
         obs_and_mod_area.name = "Overlap Area"
         obs_not_mod_area.name = "Missed Area"
         mod_not_obs_area.name = "Excess Area"
         mod_not_obs_land_area.name = "Excess Area (Land Representation)"
 
-        results = Dataset("%s/%s_%s.nc" % (self.output_path,self.name,m.name),mode="w")
-        results.setncatts({"name" :m.name, "color":m.color, "complete":0, "weight":self.cweight})
-        mod                  .toNetCDF4(results,group="MeanState")
-        obs_and_mod          .toNetCDF4(results,group="MeanState")
-        obs_not_mod          .toNetCDF4(results,group="MeanState")
-        mod_not_obs          .toNetCDF4(results,group="MeanState")
-        mod_not_obs_land     .toNetCDF4(results,group="MeanState")
-        mod_area             .toNetCDF4(results,group="MeanState")
-        obs_and_mod_area     .toNetCDF4(results,group="MeanState")
-        obs_not_mod_area     .toNetCDF4(results,group="MeanState")
-        mod_not_obs_area     .toNetCDF4(results,group="MeanState")
-        mod_not_obs_land_area.toNetCDF4(results,group="MeanState")
-        obs_score            .toNetCDF4(results,group="MeanState")
-        mod_score            .toNetCDF4(results,group="MeanState")
-        results.setncattr("complete",1)
+        results = Dataset(
+            "%s/%s_%s.nc" % (self.output_path, self.name, m.name), mode="w"
+        )
+        results.setncatts(
+            {"name": m.name, "color": m.color, "complete": 0, "weight": self.cweight}
+        )
+        mod.toNetCDF4(results, group="MeanState")
+        obs_and_mod.toNetCDF4(results, group="MeanState")
+        obs_not_mod.toNetCDF4(results, group="MeanState")
+        mod_not_obs.toNetCDF4(results, group="MeanState")
+        mod_not_obs_land.toNetCDF4(results, group="MeanState")
+        mod_area.toNetCDF4(results, group="MeanState")
+        obs_and_mod_area.toNetCDF4(results, group="MeanState")
+        obs_not_mod_area.toNetCDF4(results, group="MeanState")
+        mod_not_obs_area.toNetCDF4(results, group="MeanState")
+        mod_not_obs_land_area.toNetCDF4(results, group="MeanState")
+        obs_score.toNetCDF4(results, group="MeanState")
+        mod_score.toNetCDF4(results, group="MeanState")
+        results.setncattr("complete", 1)
         results.close()
 
         if self.master:
-            results = Dataset("%s/%s_Benchmark.nc" % (self.output_path,self.name),mode="w")
-            results.setncatts({"name" :"Benchmark", "color":np.asarray([0.5,0.5,0.5]),"weight":self.cweight,"complete":0})
-            obs_area.toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
+            results = Dataset(
+                "%s/%s_Benchmark.nc" % (self.output_path, self.name), mode="w"
+            )
+            results.setncatts(
+                {
+                    "name": "Benchmark",
+                    "color": np.asarray([0.5, 0.5, 0.5]),
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
+            obs_area.toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
             results.close()
 
-    def modelPlots(self,m):
-
-        fname   = "%s/%s_%s.nc" % (self.output_path,self.name,m.name)
+    def modelPlots(self, m):
+        fname = "%s/%s_%s.nc" % (self.output_path, self.name, m.name)
         try:
-            mod = Variable(filename      = fname,
-                           variable_name = "permafrost_extent",
-                           groupname     = "MeanState")
+            mod = Variable(
+                filename=fname, variable_name="permafrost_extent", groupname="MeanState"
+            )
         except:
             return
 
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
 
-        page.addFigure("Temporally integrated period mean",
-                       "benchmark_timeint",
-                       "Benchmark_global_timeint.png",
-                       side   = "BENCHMARK EXTENT",
-                       legend = False)
-        page.addFigure("Temporally integrated period mean",
-                       "timeint",
-                       "MNAME_global_timeint.png",
-                       side   = "MODEL EXTENT",
-                       legend = False)
-        page.addFigure("Temporally integrated period mean",
-                       "bias",
-                       "MNAME_global_bias.png",
-                       side   = "BIAS",
-                       legend = True)
-
-        fig,ax = plt.subplots(figsize=(10,10),dpi=60,
-                              subplot_kw={'projection':ccrs.Orthographic(central_latitude=+90,central_longitude=180)})
-        lat = np.hstack([mod.lat_bnds[:,0],mod.lat_bnds[-1,-1]])
-        lon = np.hstack([mod.lon_bnds[:,0],mod.lon_bnds[-1,-1]])
-        ax.pcolormesh(lon,lat,np.ma.masked_values(mod.data,0),
-                      cmap="Blues",vmin=0,vmax=2,transform=ccrs.PlateCarree())
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.875'),zorder=-1)
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.750'),zorder=-1)
-        plt.savefig("%s/%s_global_timeint.png" % (self.output_path,m.name),dpi='figure')
+        page.addFigure(
+            "Temporally integrated period mean",
+            "benchmark_timeint",
+            "Benchmark_global_timeint.png",
+            side="BENCHMARK EXTENT",
+            legend=False,
+        )
+        page.addFigure(
+            "Temporally integrated period mean",
+            "timeint",
+            "MNAME_global_timeint.png",
+            side="MODEL EXTENT",
+            legend=False,
+        )
+        page.addFigure(
+            "Temporally integrated period mean",
+            "bias",
+            "MNAME_global_bias.png",
+            side="BIAS",
+            legend=True,
+        )
+
+        fig, ax = plt.subplots(
+            figsize=(10, 10),
+            dpi=60,
+            subplot_kw={
+                "projection": ccrs.Orthographic(
+                    central_latitude=+90, central_longitude=180
+                )
+            },
+        )
+        lat = np.hstack([mod.lat_bnds[:, 0], mod.lat_bnds[-1, -1]])
+        lon = np.hstack([mod.lon_bnds[:, 0], mod.lon_bnds[-1, -1]])
+        ax.pcolormesh(
+            lon,
+            lat,
+            np.ma.masked_values(mod.data, 0),
+            cmap="Blues",
+            vmin=0,
+            vmax=2,
+            transform=ccrs.PlateCarree(),
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+            ),
+            zorder=-1,
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "ocean", "110m", edgecolor="face", facecolor="0.750"
+            ),
+            zorder=-1,
+        )
+        plt.savefig(
+            "%s/%s_global_timeint.png" % (self.output_path, m.name), dpi="figure"
+        )
         plt.close()
 
-        tmp = Variable(filename = fname,variable_name = "obs_not_mod",groupname = "MeanState")
+        tmp = Variable(
+            filename=fname, variable_name="obs_not_mod", groupname="MeanState"
+        )
         bias = np.zeros(tmp.data.shape)
         bias[...] = np.NAN
-        bias[tmp.data.mask==0] = -1.0
-        tmp = Variable(filename = fname,variable_name = "obs_and_mod",groupname = "MeanState")
-        bias[tmp.data.mask==0] =  0.0
-        tmp = Variable(filename = fname,variable_name = "mod_not_obs",groupname = "MeanState")
-        bias[tmp.data.mask==0] = +1.0
+        bias[tmp.data.mask == 0] = -1.0
+        tmp = Variable(
+            filename=fname, variable_name="obs_and_mod", groupname="MeanState"
+        )
+        bias[tmp.data.mask == 0] = 0.0
+        tmp = Variable(
+            filename=fname, variable_name="mod_not_obs", groupname="MeanState"
+        )
+        bias[tmp.data.mask == 0] = +1.0
         bias = np.ma.masked_invalid(bias)
 
-        cmap = plt.get_cmap('bwr',3)
-        fig,ax = plt.subplots(figsize=(10,10),dpi=60,
-                              subplot_kw={'projection':ccrs.Orthographic(central_latitude=+90,central_longitude=180)})
-        lat = np.hstack([tmp.lat_bnds[:,0],tmp.lat_bnds[-1,-1]])
-        lon = np.hstack([tmp.lon_bnds[:,0],tmp.lon_bnds[-1,-1]])
-        ax.pcolormesh(lon,lat,bias,cmap=cmap,vmin=-1.5,vmax=+1.5,transform=ccrs.PlateCarree())
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.875'),zorder=-1)
-        ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                    edgecolor='face',
-                                                    facecolor='0.750'),zorder=-1)
-        plt.savefig("%s/%s_global_bias.png" % (self.output_path,m.name),dpi='figure')
+        cmap = plt.get_cmap("bwr", 3)
+        fig, ax = plt.subplots(
+            figsize=(10, 10),
+            dpi=60,
+            subplot_kw={
+                "projection": ccrs.Orthographic(
+                    central_latitude=+90, central_longitude=180
+                )
+            },
+        )
+        lat = np.hstack([tmp.lat_bnds[:, 0], tmp.lat_bnds[-1, -1]])
+        lon = np.hstack([tmp.lon_bnds[:, 0], tmp.lon_bnds[-1, -1]])
+        ax.pcolormesh(
+            lon,
+            lat,
+            bias,
+            cmap=cmap,
+            vmin=-1.5,
+            vmax=+1.5,
+            transform=ccrs.PlateCarree(),
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+            ),
+            zorder=-1,
+        )
+        ax.add_feature(
+            cfeature.NaturalEarthFeature(
+                "physical", "ocean", "110m", edgecolor="face", facecolor="0.750"
+            ),
+            zorder=-1,
+        )
+        plt.savefig("%s/%s_global_bias.png" % (self.output_path, m.name), dpi="figure")
         plt.close()
 
         if self.master:
-            obs = Variable(filename      = self.source,
-                           variable_name = "permafrost_extent")
+            obs = Variable(filename=self.source, variable_name="permafrost_extent")
             # plot result
 
-            
-            fig,ax = plt.subplots(figsize=(10,10),dpi=60,
-                                  subplot_kw={'projection':ccrs.Orthographic(central_latitude=+90,central_longitude=180)})
-            lat = np.hstack([obs.lat_bnds[:,0],obs.lat_bnds[-1,-1]])
-            lon = np.hstack([obs.lon_bnds[:,0],obs.lon_bnds[-1,-1]])
-            ax.pcolormesh(lon,lat,np.ma.masked_values(obs.data,0),
-                          cmap="Blues",vmin=0,vmax=2,transform=ccrs.PlateCarree())
-            ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                        edgecolor='face',
-                                                        facecolor='0.875'),zorder=-1)
-            ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                        edgecolor='face',
-                                                        facecolor='0.750'),zorder=-1)
-            plt.savefig("%s/Benchmark_global_timeint.png" % (self.output_path),dpi='figure')
+            fig, ax = plt.subplots(
+                figsize=(10, 10),
+                dpi=60,
+                subplot_kw={
+                    "projection": ccrs.Orthographic(
+                        central_latitude=+90, central_longitude=180
+                    )
+                },
+            )
+            lat = np.hstack([obs.lat_bnds[:, 0], obs.lat_bnds[-1, -1]])
+            lon = np.hstack([obs.lon_bnds[:, 0], obs.lon_bnds[-1, -1]])
+            ax.pcolormesh(
+                lon,
+                lat,
+                np.ma.masked_values(obs.data, 0),
+                cmap="Blues",
+                vmin=0,
+                vmax=2,
+                transform=ccrs.PlateCarree(),
+            )
+            ax.add_feature(
+                cfeature.NaturalEarthFeature(
+                    "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+                ),
+                zorder=-1,
+            )
+            ax.add_feature(
+                cfeature.NaturalEarthFeature(
+                    "physical", "ocean", "110m", edgecolor="face", facecolor="0.750"
+                ),
+                zorder=-1,
+            )
+            plt.savefig(
+                "%s/Benchmark_global_timeint.png" % (self.output_path), dpi="figure"
+            )
             plt.close()
 
             # plot legend for bias
-            fig,ax = plt.subplots(figsize=(6.8,0.8),tight_layout=True)
-            ColorBar(ax,
-                     vmin = -1.5,
-                     vmax = +1.5,
-                     cmap = cmap,
-                     ticks = [-1,0,1],
-                     ticklabels = ["Missed Area","Shared Area","Excess Area"],
-                     label = "")
-            fig.savefig("%s/legend_%s.png" % (self.output_path,"bias"))
+            fig, ax = plt.subplots(figsize=(6.8, 0.8), tight_layout=True)
+            ColorBar(
+                ax,
+                vmin=-1.5,
+                vmax=+1.5,
+                cmap=cmap,
+                ticks=[-1, 0, 1],
+                ticklabels=["Missed Area", "Shared Area", "Excess Area"],
+                label="",
+            )
+            fig.savefig("%s/legend_%s.png" % (self.output_path, "bias"))
             plt.close()
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfRunoff.py` & `ILAMB-2.7/src/ILAMB/ConfRunoff.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,35 +1,39 @@
-from .Confrontation import Confrontation
-from .Regions import Regions
-from .Variable import Variable
-from netCDF4 import Dataset
-from . import ilamblib as il
-from . import Post as post
-import pylab as plt
-import numpy as np
 import os
 
-class ConfRunoff(Confrontation):
-    """A confrontation for examining the runoff in 50 of the world's largest river basins.
+import numpy as np
+import pylab as plt
+from cf_units import Unit
+from netCDF4 import Dataset
+
+from ILAMB import Post as post
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Regions import Regions
+from ILAMB.Variable import Variable
 
-    """
-    def __init__(self,**keywords):
 
+class ConfRunoff(Confrontation):
+    """A confrontation for examining the runoff in 50 of the world's largest river basins."""
+
+    def __init__(self, **keywords):
         # Ugly, but this is how we call the Confrontation constructor
-        super(ConfRunoff,self).__init__(**keywords)
+        super(ConfRunoff, self).__init__(**keywords)
 
         # Now we overwrite some things which are different here
-        self.regions        = ['global']
+        self.regions = ["global"]
         self.layout.regions = self.regions
 
         # Adding a member variable called basins, add them as regions
         r = Regions()
-        self.basins = r.addRegionNetCDF4(self.source.replace("runoff.nc","basins_0.5x0.5.nc"))
+        self.basins = r.addRegionNetCDF4(
+            self.source.replace("runoff.nc", "basins_0.5x0.5.nc")
+        )
 
-    def stageData(self,m):
+    def stageData(self, m):
         """Extracts model data and transforms it to make it comparable to the runoff dataset.
 
         Parameters
         ----------
         m : ILAMB.ModelResult.ModelResult
             the model result context
 
@@ -37,55 +41,67 @@
         -------
         obs : ILAMB.Variable.Variable
             the variable context associated with the observational dataset
         mod : ILAMB.Variable.Variable
             the variable context associated with the model result
         """
         # Extract the observational data for basins
-        obs = Variable(filename      = self.source,
-                       variable_name = self.variable,
-                       t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                       tf = None if len(self.study_limits) != 2 else self.study_limits[1]).convert("mm d-1")
+        obs = Variable(
+            filename=self.source,
+            variable_name=self.variable,
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        ).convert("mm d-1")
 
         # Extract the globally gridded runoff
-        mod = m.extractTimeSeries(self.variable,
-                                  alt_vars     = self.alternate_vars,
-                                  initial_time = obs.time_bnds[ 0,0],
-                                  final_time   = obs.time_bnds[-1,1])
+        mod = m.extractTimeSeries(
+            self.variable,
+            alt_vars=self.alternate_vars,
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+        )
+
+        # Some models output total volumetric rate instead of an area density
+        if (Unit(mod.unit) / Unit("m3 s-1")).is_dimensionless():
+            mod.data /= mod.area
+            mod.unit = mod.unit + " m-2"
 
         # We want annual mean, not monthly mean
-        years = np.asarray([obs.time_bnds[  ::12,0],
-                            obs.time_bnds[11::12,1]]).T
-        obs   = obs.coarsenInTime(years)
-        mod   = mod.coarsenInTime(years)
+        years = np.asarray([obs.time_bnds[::12, 0], obs.time_bnds[11::12, 1]]).T
+        obs = obs.coarsenInTime(years)
+        mod = mod.coarsenInTime(years)
         obs.name = "runoff"
         mod.name = "runoff"
 
         # Operate on model data to compute mean runoff values in each basin.
-        data   = np.ma.zeros(obs.data.shape)
-        for i,basin in enumerate(self.basins):
-            b = il.ClipTime(mod.integrateInSpace(region=basin,mean=True),
-                            obs.time_bnds[ 0,0],
-                            obs.time_bnds[-1,1]).convert(obs.unit)
-            data[:,i] = b.data
+        data = np.ma.zeros(obs.data.shape)
+        for i, basin in enumerate(self.basins):
+            b = il.ClipTime(
+                mod.integrateInSpace(region=basin, mean=True),
+                obs.time_bnds[0, 0],
+                obs.time_bnds[-1, 1],
+            ).convert(obs.unit)
+            data[:, i] = b.data
 
         # Create a variable to return for the model
-        mod  = Variable(name      = obs.name,
-                        unit      = obs.unit,
-                        data      = np.ma.masked_array(data,mask=obs.data.mask),
-                        time      = obs.time,
-                        time_bnds = obs.time_bnds,
-                        ndata     = obs.ndata,
-                        lat       = obs.lat,
-                        lat_bnds  = obs.lat_bnds,
-                        lon       = obs.lon,
-                        lon_bnds  = obs.lon_bnds)
-        return obs,mod
+        mod = Variable(
+            name=obs.name,
+            unit=obs.unit,
+            data=np.ma.masked_array(data, mask=obs.data.mask),
+            time=obs.time,
+            time_bnds=obs.time_bnds,
+            ndata=obs.ndata,
+            lat=obs.lat,
+            lat_bnds=obs.lat_bnds,
+            lon=obs.lon,
+            lon_bnds=obs.lon_bnds,
+        )
+        return obs, mod
 
-    def _extendSitesToMap(self,var):
+    def _extendSitesToMap(self, var):
         """A local function to extend site data to the basins.
 
         Parameters
         ----------
         var : ILAMB.Variable.Variable
             the site-based variable we wish to extend to basins
 
@@ -95,124 +111,147 @@
             the spatial variable which is the extended version of the
             input variable
         """
 
         # determine the global mask
         global_mask = None
         global_data = None
-        for i,basin in enumerate(self.basins):
-            name,lat,lon,mask = Regions._regions[basin]
-            keep = (mask == False)
+        for i, basin in enumerate(self.basins):
+            name, lat, lon, mask = Regions._regions[basin]
+            keep = mask == False
             if global_mask is None:
-                global_mask  = np.copy(mask)
-                global_data  = keep*var.data[i]
+                global_mask = np.copy(mask)
+                global_data = keep * var.data[i]
             else:
                 global_mask *= mask
-                global_data += keep*var.data[i]
-        return Variable(name      = var.name,
-                        unit      = var.unit,
-                        data      = np.ma.masked_array(global_data,mask=global_mask),
-                        lat       = lat,
-                        lon       = lon)
+                global_data += keep * var.data[i]
+        return Variable(
+            name=var.name,
+            unit=var.unit,
+            data=np.ma.masked_array(global_data, mask=global_mask),
+            lat=lat,
+            lon=lon,
+        )
 
-    def confront(self,m):
+    def confront(self, m):
         """The analysis portion applied to basins as if they were datasites
 
         Parameters
         ----------
         m : ILAMB.ModelResult.ModelResult
             the model result context
         """
         # Grab the data
-        obs,mod = self.stageData(m)
+        obs, mod = self.stageData(m)
 
         # Basic analysis from ilamblib.AnalysisMeanState() for
         # datasites and only the global region
-        obs_timeint     = obs.integrateInTime(mean=True)
-        mod_timeint     = mod.integrateInTime(mean=True)
-        bias_map        = obs_timeint.bias(mod_timeint)
-        normalizer      = obs_timeint.data
-        bias_score_map  = il.Score(bias_map,obs_timeint)
-        obs_period_mean = obs_timeint    .siteStats()
-        mod_period_mean = mod_timeint    .siteStats()
-        bias            = bias_map       .siteStats()
-        bias_score      = bias_score_map .siteStats(weight=normalizer)
-        std,R,sd_score  = obs_timeint.spatialDistribution(mod_timeint)
-        obs_iav_map     = obs.interannualVariability()
-        mod_iav_map     = mod.interannualVariability()
-        iav_score_map   = obs_iav_map.spatialDifference(mod_iav_map)
-        iav_score_map   = il.Score(iav_score_map,obs_iav_map)
-        iav_score       = iav_score_map.siteStats()
+        obs_timeint = obs.integrateInTime(mean=True)
+        mod_timeint = mod.integrateInTime(mean=True)
+        bias_map = obs_timeint.bias(mod_timeint)
+        normalizer = obs_timeint.data
+        bias_score_map = il.Score(bias_map, obs_timeint)
+        obs_period_mean = obs_timeint.siteStats()
+        mod_period_mean = mod_timeint.siteStats()
+        bias = bias_map.siteStats()
+        bias_score = bias_score_map.siteStats(weight=normalizer)
+        std, R, sd_score = obs_timeint.spatialDistribution(mod_timeint)
+        obs_iav_map = obs.interannualVariability()
+        mod_iav_map = mod.interannualVariability()
+        iav_score_map = obs_iav_map.spatialDifference(mod_iav_map)
+        iav_score_map = il.Score(iav_score_map, obs_iav_map)
+        iav_score = iav_score_map.siteStats()
 
         # Extend a few quantities from datasites to their
         # corresponding basins (plotting only)
-        obs_timeint     = self._extendSitesToMap(obs_timeint)
-        mod_timeint     = self._extendSitesToMap(mod_timeint)
-        bias_map        = self._extendSitesToMap(bias_map)
+        obs_timeint = self._extendSitesToMap(obs_timeint)
+        mod_timeint = self._extendSitesToMap(mod_timeint)
+        bias_map = self._extendSitesToMap(bias_map)
 
         # Rename some quantities for parsing later in the HTML
         # generation
         obs_period_mean.name = "Period Mean global"
         mod_period_mean.name = "Period Mean global"
-        bias           .name = "Bias global"
-        bias_score     .name = "Bias Score global"
-        sd_score       .name = "Spatial Distribution Score global"
-        obs_timeint    .name = "timeint_of_runoff"
-        mod_timeint    .name = "timeint_of_runoff"
-        bias_map       .name = "bias_map_of_runoff"
-        iav_score      .name = "Interannual Variability Score global"
+        bias.name = "Bias global"
+        bias_score.name = "Bias Score global"
+        sd_score.name = "Spatial Distribution Score global"
+        obs_timeint.name = "timeint_of_runoff"
+        mod_timeint.name = "timeint_of_runoff"
+        bias_map.name = "bias_map_of_runoff"
+        iav_score.name = "Interannual Variability Score global"
 
         # Dump to files
-        results = Dataset(os.path.join(self.output_path,"%s_%s.nc" % (self.name,m.name)),mode="w")
-        results.setncatts({"name" :m.name, "color":m.color,"weight":self.cweight,"complete":0})
-        for var in [mod,
-                    mod_period_mean,
-                    mod_timeint,
-                    bias,
-                    bias_score,
-                    bias_map,
-                    iav_score]:
-            var.toNetCDF4(results,group="MeanState")
-        sd_score.toNetCDF4(results,group="MeanState",attributes={"std":std.data,"R":R.data})
-        results.setncattr("complete",1)
+        results = Dataset(
+            os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name)), mode="w"
+        )
+        results.setncatts(
+            {"name": m.name, "color": m.color, "weight": self.cweight, "complete": 0}
+        )
+        for var in [
+            mod,
+            mod_period_mean,
+            mod_timeint,
+            bias,
+            bias_score,
+            bias_map,
+            iav_score,
+        ]:
+            var.toNetCDF4(results, group="MeanState")
+        sd_score.toNetCDF4(
+            results, group="MeanState", attributes={"std": std.data, "R": R.data}
+        )
+        results.setncattr("complete", 1)
         results.close()
         if self.master:
-            results = Dataset(os.path.join(self.output_path,"%s_Benchmark.nc" % self.name),mode="w")
-            results.setncatts({"name" :"Benchmark", "color":np.asarray([0.5,0.5,0.5]),"weight":self.cweight,"complete":0})
-            for var in [obs,
-                        obs_period_mean,
-                        obs_timeint]:
-                var.toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
+            results = Dataset(
+                os.path.join(self.output_path, "%s_Benchmark.nc" % self.name), mode="w"
+            )
+            results.setncatts(
+                {
+                    "name": "Benchmark",
+                    "color": np.asarray([0.5, 0.5, 0.5]),
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
+            for var in [obs, obs_period_mean, obs_timeint]:
+                var.toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
             results.close()
 
-    def modelPlots(self,m):
-
+    def modelPlots(self, m):
         # some of the plots can be generated using the standard
         # routine, with some modifications
-        super(ConfRunoff,self).modelPlots(m)
+        super(ConfRunoff, self).modelPlots(m)
 
         #
-        bname = os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name       ))
-        fname = os.path.join(self.output_path,"%s_%s.nc"        % (self.name,m.name))
+        bname = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name))
+        fname = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
 
         # get the HTML page
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
 
-        if not os.path.isfile(bname): return
-        if not os.path.isfile(fname): return
-        obs = Variable(filename = bname, variable_name = "runoff", groupname = "MeanState")
-        mod = Variable(filename = fname, variable_name = "runoff", groupname = "MeanState")
-        for i,basin in enumerate(self.basins):
-
-            page.addFigure("Spatially integrated regional mean",
-                           basin,
-                           "MNAME_global_%s.png" % basin,
-                           basin,False,longname=basin)
-
-            fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-            ax.plot(obs.time/365+1850,obs.data[:,i],lw=2,color='k',alpha=0.5)
-            ax.plot(mod.time/365+1850,mod.data[:,i],lw=2,color=m.color      )
+        if not os.path.isfile(bname):
+            return
+        if not os.path.isfile(fname):
+            return
+        obs = Variable(filename=bname, variable_name="runoff", groupname="MeanState")
+        mod = Variable(filename=fname, variable_name="runoff", groupname="MeanState")
+        for i, basin in enumerate(self.basins):
+            page.addFigure(
+                "Spatially integrated regional mean",
+                basin,
+                "MNAME_global_%s.png" % basin,
+                basin,
+                False,
+                longname=basin,
+            )
+
+            fig, ax = plt.subplots(figsize=(6.8, 2.8), tight_layout=True)
+            ax.plot(obs.time / 365 + 1850, obs.data[:, i], lw=2, color="k", alpha=0.5)
+            ax.plot(mod.time / 365 + 1850, mod.data[:, i], lw=2, color=m.color)
             ax.grid()
             ax.set_ylabel(post.UnitStringToMatplotlib(obs.unit))
-            fig.savefig(os.path.join(self.output_path,"%s_global_%s.png" % (m.name,basin)))
+            fig.savefig(
+                os.path.join(self.output_path, "%s_global_%s.png" % (m.name, basin))
+            )
             plt.close()
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfSoilCarbon.py` & `ILAMB-2.7/src/ILAMB/ConfSoilCarbon.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,209 +1,298 @@
-from .Confrontation import Confrontation
-from .Variable import Variable
-from .Relationship import Relationship
+import os
+
 import matplotlib.pyplot as plt
-from netCDF4 import Dataset
 import numpy as np
-import os
+from netCDF4 import Dataset
+
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Relationship import Relationship
+from ILAMB.Variable import Variable
 
-def getSource(vname,filename,unit):
-    if vname == "soilc": vname = "cSoilAbove1m"
+
+def getSource(vname, filename, unit):
+    if vname == "soilc":
+        vname = "cSoilAbove1m"
     files = filename.split(",") if "," in filename else [filename]
-    data,lat,lon = None,None,None
+    data, lat, lon = None, None, None
     for f in files:
-        v = Variable(filename = os.path.join(os.environ["ILAMB_ROOT"],f.strip()),
-                     variable_name = vname)
-        if v.temporal: v = v.integrateInTime(mean=True)
+        v = Variable(
+            filename=os.path.join(os.environ["ILAMB_ROOT"], f.strip()),
+            variable_name=vname,
+        )
+        if v.temporal:
+            v = v.integrateInTime(mean=True)
         v.convert(unit)
         if data is None:
             data = v.data
-            lat  = v.lat
-            lon  = v.lon
+            lat = v.lat
+            lon = v.lon
         else:
-            assert np.allclose(v.lat,lat)*np.allclose(v.lon,lon)
-            ind = np.where(data.mask + (data < 1e-12*data.max()))
+            assert np.allclose(v.lat, lat) * np.allclose(v.lon, lon)
+            ind = np.where(data.mask + (data < 1e-12 * data.max()))
             data[ind] = v.data[ind]
-    return lat,lon,data
-    
+    return lat, lon, data
+
+
 class ConfSoilCarbon(Confrontation):
     """A soil carbon temperature sensivity benchmark.
 
     For details of the metric, see the following publication:
 
     Koven, Hugelius, Lawrence, Wieder, Higher climatological
     temperature sensitivity of soil carbon in cold than warm
     climates. Nature Climate Change, October 2017, doi:
     10.1038/NCLIMATE3421
 
     """
-    def __init__(self,**keywords):
-        super(ConfSoilCarbon,self).__init__(**keywords)
-        self.regions        = ["global"]
+
+    def __init__(self, **keywords):
+        super(ConfSoilCarbon, self).__init__(**keywords)
+        self.regions = ["global"]
         self.layout.regions = self.regions
-    
-    def stageData(self,m):
+
+    def stageData(self, m):
         pass
-    
-    def confront(self,m):
 
+    def confront(self, m):
         # Constants
-        soilc_threshold   =  1e-12 # kg m-2
-        npp_threshold     =  1e-4  # kg m-2 yr-1
+        soilc_threshold = 1e-12  # kg m-2
+        npp_threshold = 1e-4  # kg m-2 yr-1
         aridity_threshold = -1000  # mm yr-1
-        sat_threshold     =  0.5   # 1
-        gauss_critval     =  0.674 # for 50%
-        relationship_bins = self.keywords.get("relationship_bins",np.arange(-15.5,28.6,1))
+        sat_threshold = 0.5  # 1
+        gauss_critval = 0.674  # for 50%
+        relationship_bins = self.keywords.get(
+            "relationship_bins", np.arange(-15.5, 28.6, 1)
+        )
         if type(relationship_bins) == str:
-            relationship_bins = np.asarray(relationship_bins.split(","),dtype=float)
-        T10 = np.asarray([-10,+10,25]) # temperatures at which to report Q10
-        
+            relationship_bins = np.asarray(relationship_bins.split(","), dtype=float)
+        T10 = np.asarray([-10, +10, 25])  # temperatures at which to report Q10
+
         # Get the source datafiles
-        lat,lon,soilc = getSource("soilc",self.keywords.get("soilc_source"),"kg m-2")
-        LAT,LON,npp = getSource("npp",self.keywords.get("npp_source"),"kg m-2 yr-1")
-        assert np.allclose(LAT,lat)*np.allclose(LON,lon)
-        LAT,LON,tas = getSource("tas",self.keywords.get("tas_source"),"degC")
-        assert np.allclose(LAT,lat)*np.allclose(LON,lon)
-        LAT,LON,pr = getSource("pr",self.keywords.get("pr_source"),"mm yr-1")
-        assert np.allclose(LAT,lat)*np.allclose(LON,lon)
-        LAT,LON,pet = getSource("pet",self.keywords.get("pet_source"),"mm yr-1")
-        assert np.allclose(LAT,lat)*np.allclose(LON,lon)
-        LAT,LON,fracpeat = getSource("fracpeat",self.keywords.get("fracpeat_source"),"1")
-        assert np.allclose(LAT,lat)*np.allclose(LON,lon)
+        lat, lon, soilc = getSource(
+            "soilc", self.keywords.get("soilc_source"), "kg m-2"
+        )
+        LAT, LON, npp = getSource("npp", self.keywords.get("npp_source"), "kg m-2 yr-1")
+        assert np.allclose(LAT, lat) * np.allclose(LON, lon)
+        LAT, LON, tas = getSource("tas", self.keywords.get("tas_source"), "degC")
+        assert np.allclose(LAT, lat) * np.allclose(LON, lon)
+        LAT, LON, pr = getSource("pr", self.keywords.get("pr_source"), "mm yr-1")
+        assert np.allclose(LAT, lat) * np.allclose(LON, lon)
+        LAT, LON, pet = getSource("pet", self.keywords.get("pet_source"), "mm yr-1")
+        assert np.allclose(LAT, lat) * np.allclose(LON, lon)
+        LAT, LON, fracpeat = getSource(
+            "fracpeat", self.keywords.get("fracpeat_source"), "1"
+        )
+        assert np.allclose(LAT, lat) * np.allclose(LON, lon)
 
         # Determine what will be masked
-        mask  = soilc.mask + tas.mask + npp.mask + pr.mask  # where any source is masked
-        mask += (soilc < soilc_threshold)  # where there is no soilc
-        mask += (npp < npp_threshold)  # where npp is small or negative
-        mask += ((pr-pet) < aridity_threshold)  # where aridity dominates
-        mask += (fracpeat > sat_threshold)  # where mostly peatland
-        soilc = np.ma.masked_array(soilc,mask=mask).compressed()
-        npp   = np.ma.masked_array(npp  ,mask=mask).compressed()
-        tas   = Variable(name="Mean air temperature",unit="degC",data=np.ma.masked_array(tas,mask=mask).compressed())
-        pr    = Variable(name="Precipitation",unit="mm yr-1",data=np.ma.masked_array(pr,mask=mask).compressed())
-        tau   = Variable(name="Inferred turnover time",unit="yr",data=soilc/npp)
-        r     = Relationship(tas,tau,dep_log=True,order=2,color=pr)
-        r.limits = [[1.,1e3],[-22.,30.]]
-        
+        mask = soilc.mask + tas.mask + npp.mask + pr.mask  # where any source is masked
+        mask += soilc < soilc_threshold  # where there is no soilc
+        mask += npp < npp_threshold  # where npp is small or negative
+        mask += (pr - pet) < aridity_threshold  # where aridity dominates
+        mask += fracpeat > sat_threshold  # where mostly peatland
+        soilc = np.ma.masked_array(soilc, mask=mask).compressed()
+        npp = np.ma.masked_array(npp, mask=mask).compressed()
+        tas = Variable(
+            name="Mean air temperature",
+            unit="degC",
+            data=np.ma.masked_array(tas, mask=mask).compressed(),
+        )
+        pr = Variable(
+            name="Precipitation",
+            unit="mm yr-1",
+            data=np.ma.masked_array(pr, mask=mask).compressed(),
+        )
+        tau = Variable(name="Inferred turnover time", unit="yr", data=soilc / npp)
+        r = Relationship(tas, tau, dep_log=True, order=2, color=pr)
+        r.limits = [[1.0, 1e3], [-22.0, 30.0]]
+
         # Get model results
-        y0 = self.keywords.get("y0",1980.)
-        yf = self.keywords.get("yf",2006.)
-        t0 = (y0-1850  )*365
-        tf = (yf-1850+1)*365
-        mod_soilc = m.extractTimeSeries("cSoilAbove1m",
-                                        alt_vars     = ["soilc","cSoil"],
-                                        initial_time = t0,
-                                        final_time   = tf).integrateInTime(mean=True).convert("kg m-2")
-        mod_npp   = m.extractTimeSeries("npp",
-                                        initial_time = t0,
-                                        final_time   = tf,
-                                        expression   = "gpp-ra").integrateInTime(mean=True).convert("kg m-2 yr-1")
-        mod_tas   = m.extractTimeSeries("tas",
-                                        initial_time = t0,
-                                        final_time   = tf).integrateInTime(mean=True).convert("degC")
-        mod_pr    = m.extractTimeSeries("pr",
-                                        initial_time = t0,
-                                        final_time   = tf).integrateInTime(mean=True).convert("mm yr-1")
-        mod_pet   = Variable(lat=LAT,lon=LON,unit="mm yr-1",data=pet).interpolate(lat=mod_pr.lat,lon=mod_pr.lon)
-        
+        y0 = self.keywords.get("y0", 1980.0)
+        yf = self.keywords.get("yf", 2006.0)
+        t0 = (y0 - 1850) * 365
+        tf = (yf - 1850 + 1) * 365
+        mod_soilc = (
+            m.extractTimeSeries(
+                "cSoilAbove1m",
+                alt_vars=["soilc", "cSoil"],
+                initial_time=t0,
+                final_time=tf,
+            )
+            .integrateInTime(mean=True)
+            .convert("kg m-2")
+        )
+        mod_npp = (
+            m.extractTimeSeries(
+                "npp", initial_time=t0, final_time=tf, expression="gpp-ra"
+            )
+            .integrateInTime(mean=True)
+            .convert("kg m-2 yr-1")
+        )
+        mod_tas = (
+            m.extractTimeSeries("tas", initial_time=t0, final_time=tf)
+            .integrateInTime(mean=True)
+            .convert("degC")
+        )
+        mod_pr = (
+            m.extractTimeSeries("pr", initial_time=t0, final_time=tf)
+            .integrateInTime(mean=True)
+            .convert("mm yr-1")
+        )
+        mod_pet = Variable(lat=LAT, lon=LON, unit="mm yr-1", data=pet).interpolate(
+            lat=mod_pr.lat, lon=mod_pr.lon
+        )
+
         # Determine what will be masked
-        mask  = mod_soilc.data.mask + mod_npp.data.mask + mod_tas.data.mask + mod_pr.data.mask
-        mask += (mod_soilc.data < soilc_threshold)
-        mask += (mod_npp.data < npp_threshold)
-        mask += ((mod_pr.data-mod_pet.data) < aridity_threshold)
-        mod_soilc = np.ma.masked_array(mod_soilc.data,mask=mask).compressed()
-        mod_npp   = np.ma.masked_array(mod_npp.data  ,mask=mask).compressed()
-        mod_tas   = Variable(name="Mean air temperature",unit="degC",data=np.ma.masked_array(mod_tas.data,mask=mask).compressed())
-        mod_pr    = Variable(name="Precipitation",unit="mm yr-1",data=np.ma.masked_array(mod_pr.data,mask=mask).compressed())
+        mask = (
+            mod_soilc.data.mask
+            + mod_npp.data.mask
+            + mod_tas.data.mask
+            + mod_pr.data.mask
+        )
+        mask += mod_soilc.data < soilc_threshold
+        mask += mod_npp.data < npp_threshold
+        mask += (mod_pr.data - mod_pet.data) < aridity_threshold
+        mod_soilc = np.ma.masked_array(mod_soilc.data, mask=mask).compressed()
+        mod_npp = np.ma.masked_array(mod_npp.data, mask=mask).compressed()
+        mod_tas = Variable(
+            name="Mean air temperature",
+            unit="degC",
+            data=np.ma.masked_array(mod_tas.data, mask=mask).compressed(),
+        )
+        mod_pr = Variable(
+            name="Precipitation",
+            unit="mm yr-1",
+            data=np.ma.masked_array(mod_pr.data, mask=mask).compressed(),
+        )
 
         # Compute inferred turnover and fit quadratic
-        mod_tau = Variable(name="Inferred turnover time",unit="yr",data=mod_soilc/mod_npp)
-        mod_r   = Relationship(mod_tas,mod_tau,dep_log=True,order=2,color=mod_pr)
+        mod_tau = Variable(
+            name="Inferred turnover time", unit="yr", data=mod_soilc / mod_npp
+        )
+        mod_r = Relationship(mod_tas, mod_tau, dep_log=True, order=2, color=mod_pr)
         mod_r.limits = r.limits
-        
+
         # Outputs and plots
-        page = [page for page in self.layout.pages if "MeanState" in page.name][0]        
+        page = [page for page in self.layout.pages if "MeanState" in page.name][0]
         if self.master:
-            page.addFigure("Temporally integrated period mean",
-                           "benchmark_timeint",
-                           "Benchmark_global_timeint.png",
-                           side   = "BENCHMARK",
-                           legend = False)
-            
-            fig,ax = plt.subplots(figsize=(5,5.5),tight_layout=True,dpi=100)
-            r.plotPointCloud(ax,vmin=0,vmax=2000,cmap='wetdry')
-            r.plotModel(ax,color='k',prediction=True)
-            ax.set_xlim(-22,30)
-            ax.set_ylim(1,3e3)
+            page.addFigure(
+                "Temporally integrated period mean",
+                "benchmark_timeint",
+                "Benchmark_global_timeint.png",
+                side="BENCHMARK",
+                legend=False,
+            )
+
+            fig, ax = plt.subplots(figsize=(5, 5.5), tight_layout=True, dpi=100)
+            r.plotPointCloud(ax, vmin=0, vmax=2000, cmap="wetdry")
+            r.plotModel(ax, color="k", prediction=True)
+            ax.set_xlim(-22, 30)
+            ax.set_ylim(1, 3e3)
             plt.savefig("%s/Benchmark_global_timeint.png" % (self.output_path))
             plt.close()
-    
-            with Dataset("%s/%s_Benchmark.nc" % (self.output_path,self.name),mode="w") as results:
-                results.setncatts({"name" :"Benchmark", "color":np.asarray([0.5,0.5,0.5]),"weight":self.cweight,"complete":0})
+
+            with Dataset(
+                "%s/%s_Benchmark.nc" % (self.output_path, self.name), mode="w"
+            ) as results:
+                results.setncatts(
+                    {
+                        "name": "Benchmark",
+                        "color": np.asarray([0.5, 0.5, 0.5]),
+                        "weight": self.cweight,
+                        "complete": 0,
+                    }
+                )
                 p = r.dist["default"][5]
-                Q10 = 10**(-10*(np.polyval(np.polyder(p),T10)))
-                for q,t in zip(Q10,T10):
-                    Variable(name = "Q10(%+d [C])" % int(t),unit="1",data=q).toNetCDF4(results,group="MeanState")
-                results.setncattr("complete",1)
-
-        page.addFigure("Temporally integrated period mean",
-                       "timeint",
-                       "MNAME_global_timeint.png",
-                       side   = "MODEL",
-                       legend = False)
-        fig,ax = plt.subplots(figsize=(5,5.5),tight_layout=True,dpi=100)
-        mod_r.plotPointCloud(ax,vmin=0,vmax=2000,cmap='wetdry')
-        r    .plotModel(ax,color='k',prediction=True)
-        ax.set_xlim(-22,30)
-        ax.set_ylim(1,3e3)
-        plt.savefig("%s/%s_global_timeint.png" % (self.output_path,m.name))
+                Q10 = 10 ** (-10 * (np.polyval(np.polyder(p), T10)))
+                for q, t in zip(Q10, T10):
+                    Variable(name="Q10(%+d [C])" % int(t), unit="1", data=q).toNetCDF4(
+                        results, group="MeanState"
+                    )
+                results.setncattr("complete", 1)
+
+        page.addFigure(
+            "Temporally integrated period mean",
+            "timeint",
+            "MNAME_global_timeint.png",
+            side="MODEL",
+            legend=False,
+        )
+        fig, ax = plt.subplots(figsize=(5, 5.5), tight_layout=True, dpi=100)
+        mod_r.plotPointCloud(ax, vmin=0, vmax=2000, cmap="wetdry")
+        r.plotModel(ax, color="k", prediction=True)
+        ax.set_xlim(-22, 30)
+        ax.set_ylim(1, 3e3)
+        plt.savefig("%s/%s_global_timeint.png" % (self.output_path, m.name))
         plt.close()
-        
-        page.addFigure("Temporally integrated period mean",
-                       "rel_tas",
-                       "MNAME_RNAME_rel_tas.png",
-                       side   = "MODEL",
-                       legend = False)
-        fig,ax = plt.subplots(figsize=(5,4.5),tight_layout=True,dpi=100)
-        r    .plotFunction(ax,color='k'    ,shift=-0.1)
-        mod_r.plotFunction(ax,color=m.color,shift=+0.1)
-        ax.set_xlim(-22,30)
-        ax.set_ylim(1,3e3)
-        plt.savefig("%s/%s_global_rel_tas.png" % (self.output_path,m.name))
+
+        page.addFigure(
+            "Temporally integrated period mean",
+            "rel_tas",
+            "MNAME_RNAME_rel_tas.png",
+            side="MODEL",
+            legend=False,
+        )
+        fig, ax = plt.subplots(figsize=(5, 4.5), tight_layout=True, dpi=100)
+        r.plotFunction(ax, color="k", shift=-0.1)
+        mod_r.plotFunction(ax, color=m.color, shift=+0.1)
+        ax.set_xlim(-22, 30)
+        ax.set_ylim(1, 3e3)
+        plt.savefig("%s/%s_global_rel_tas.png" % (self.output_path, m.name))
         plt.close()
-        
-        with Dataset("%s/%s_%s.nc" % (self.output_path,self.name,m.name),mode="w") as results:
-            results.setncatts({"name" :m.name, "color":m.color,"weight":self.cweight,"complete":0})
+
+        with Dataset(
+            "%s/%s_%s.nc" % (self.output_path, self.name, m.name), mode="w"
+        ) as results:
+            results.setncatts(
+                {
+                    "name": m.name,
+                    "color": m.color,
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
             mod_p = mod_r.dist["default"][5]
-            Q10 = 10**(-10*(np.polyval(np.polyder(mod_p),T10)))
-            for q,t in zip(Q10,T10):
-                Variable(name = "Q10(%+d [C])" % int(t),unit="1",data=q).toNetCDF4(results,group="MeanState")
-            Variable(name = "RMSE Score global",unit="1",data=r.scoreRMSE(mod_r)).toNetCDF4(results,group="MeanState")
-            Variable(name = "Distribution Score global",unit="1",data=r.scoreHellinger(mod_r)).toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
-        
+            Q10 = 10 ** (-10 * (np.polyval(np.polyder(mod_p), T10)))
+            for q, t in zip(Q10, T10):
+                Variable(name="Q10(%+d [C])" % int(t), unit="1", data=q).toNetCDF4(
+                    results, group="MeanState"
+                )
+            Variable(
+                name="RMSE Score global", unit="1", data=r.scoreRMSE(mod_r)
+            ).toNetCDF4(results, group="MeanState")
+            Variable(
+                name="Distribution Score global", unit="1", data=r.scoreHellinger(mod_r)
+            ).toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
+
     def determinePlotLimits(self):
         pass
-    
+
     def compositePlots(self):
         pass
-    
-    def modelPlots(self,m):
-        
+
+    def modelPlots(self, m):
         # Outputs and plots
-        page = [page for page in self.layout.pages if "MeanState" in page.name][0]        
+        page = [page for page in self.layout.pages if "MeanState" in page.name][0]
         if self.master:
-            page.addFigure("Temporally integrated period mean",
-                           "benchmark_timeint",
-                           "Benchmark_global_timeint.png",
-                           side   = "BENCHMARK",
-                           legend = False)
-        page.addFigure("Temporally integrated period mean",
-                       "timeint",
-                       "MNAME_global_timeint.png",
-                       side   = "MODEL",
-                       legend = False)        
-        page.addFigure("Temporally integrated period mean",
-                       "rel_tas",
-                       "MNAME_RNAME_rel_tas.png",
-                       side   = "MODEL",
-                       legend = False)
+            page.addFigure(
+                "Temporally integrated period mean",
+                "benchmark_timeint",
+                "Benchmark_global_timeint.png",
+                side="BENCHMARK",
+                legend=False,
+            )
+        page.addFigure(
+            "Temporally integrated period mean",
+            "timeint",
+            "MNAME_global_timeint.png",
+            side="MODEL",
+            legend=False,
+        )
+        page.addFigure(
+            "Temporally integrated period mean",
+            "rel_tas",
+            "MNAME_RNAME_rel_tas.png",
+            side="MODEL",
+            legend=False,
+        )
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfTWSA.py` & `ILAMB-2.7/src/ILAMB/ConfBasin.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,197 +1,118 @@
-from .Confrontation import Confrontation
-import matplotlib.pyplot as plt
-from .Variable import Variable
-from .Regions import Regions
-from netCDF4 import Dataset
-from . import ilamblib as il
-from . import Post as post
-import numpy as np
 import os
 
-class ConfTWSA(Confrontation):
-    """A confrontation for examining the terrestrial water storage anomaly.
+import numpy as np
+import pylab as plt
+from cf_units import Unit
+from netCDF4 import Dataset
 
-    """
-    def __init__(self,**keywords):
+from ILAMB import Post as post
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Regions import Regions
+from ILAMB.Variable import Variable
 
-        # Ugly, but this is how we call the Confrontation constructor
-        super(ConfTWSA,self).__init__(**keywords)
 
-        # Now we overwrite some things which are different here
-        self.weight = {"Diff Score"                    :1.,
-                       "RMSE Score"                    :2.,
-                       "Seasonal Cycle Score"          :1.,
-                       "Interannual Variability Score" :1.,
-                       "Spatial Distribution Score"    :1.}
+class ConfBasin(Confrontation):
+    """A confrontation for examining the runoff/drainage in defined drainage basins."""
+
+    def __init__(self, **keywords):
+        # Ugly, but this is how we call the Confrontation constructor
+        super(ConfBasin, self).__init__(**keywords)
 
         # Now we overwrite some things which are different here
-        self.regions        = ['global']
+        self.regions = ["global"]
         self.layout.regions = self.regions
 
         # Adding a member variable called basins, add them as regions
         r = Regions()
-        nbasins = self.keywords.get("nbasins",30)
-        self.basins = r.addRegionNetCDF4(os.path.join("/".join(self.source.split("/")[:-3]),"runoff/Dai/basins_0.5x0.5.nc"))[:nbasins]
-
-    def stageData(self,m):
-        r"""Extracts model data which is comparable to the observations.
+        basin_file = self.keywords.get("drainage_basins", False)
+        if not basin_file:
+            msg = "ConfBasin requires a configure keyword 'drainage_basins' which points to the netCDF or shapefile which represents the basins"
+            raise ValueError(msg)
+        ext = basin_file.split(".")[-1]
+        if ext == "nc":
+            self.basins = r.addRegionNetCDF4(basin_file)
+        elif ext == "shp":
+            self.basins = r.addRegionShapeFile(basin_file)
+        else:
+            raise ValueError("Unknown file extension (.%s) for adding regions" % ext)
 
-        The observational data measure the anomaly in terrestrial
-        water storage, 'twsa' in terms of [kg m-2]. We convert this
-        unit to [cm] using the density of water. The models are
-        expected to provide the terrestrial water storage variable,
-        'tws', and we need to find the anomaly. First, the model
-        result is trimmed to match the temporal extents of the
-        observational data. Then, to get the anomaly, we subtract off
-        the temporal mean,
-
-        .. math:: \mathit{twsa}(t,\mathbf{x}) = tws(t,\mathbf{x}) - \frac{1}{t_f-t_0}\int_{t_0}^{t_f} tws(t,\mathbf{x})\ dt
-
-        We do this for the model 'tws' variable, and optionally for
-        the observation, treating its 'twsa' variable as 'tws' in the
-        above expression. This is because the observational data can
-        have a small mean even though it represents only the anomaly.
+    def stageData(self, m):
+        """Extracts model data and transforms it to make it comparable to the runoff dataset.
 
         Parameters
         ----------
         m : ILAMB.ModelResult.ModelResult
             the model result context
 
         Returns
         -------
         obs : ILAMB.Variable.Variable
             the variable context associated with the observational dataset
         mod : ILAMB.Variable.Variable
             the variable context associated with the model result
-
         """
-        # get the observational data
-        obs = Variable(filename       = self.source,
-                       variable_name  = self.variable,
-                       alternate_vars = self.alternate_vars,
-                       t0 = None if len(self.study_limits) != 2 else self.study_limits[0],
-                       tf = None if len(self.study_limits) != 2 else self.study_limits[1]).convert("cm")
-
-        # get the model data, in the units of the obseravtions
-        mod = m.extractTimeSeries(self.variable,
-                                  alt_vars     = self.alternate_vars,
-                                  expression   = self.derived,
-                                  initial_time = obs.time_bnds[ 0,0],
-                                  final_time   = obs.time_bnds[-1,1])
-        obs.trim(t=[mod.time_bnds[0,0],mod.time_bnds[-1,1]])
- 
-        # if the derived expression is used, then we get a mass flux
-        # rate and need to accumulate
-        try:
-            mod.convert(obs.unit)
-        except:
-            mod = mod.accumulateInTime()
-            # once we accumulate, the data is defined at the breaks of
-            # the obs, so we average to restore compatibility
-            mod.data = 0.5*(mod.data[:-1]+mod.data[+1:])
-            mod.time = obs.time[...]
-            mod.time_bnds = obs.time_bnds[...]
-            mod.name = obs.name
-            
-        obs,mod = il.MakeComparable(obs,mod,clip_ref=True)
-
-        # subtract off the mean
-        mean      = obs.integrateInTime(mean=True)
-        obs.data -= mean.data
-        mean      = mod.integrateInTime(mean=True)
-        mod.data -= mean.data
-
-        # compute mean values over each basin
-        odata = np.ma.zeros((obs.time.size,len(self.basins)))
-        mdata = np.ma.zeros((mod.time.size,len(self.basins)))
-        for i,basin in enumerate(self.basins):
-            odata[:,i] = obs.integrateInSpace(region=basin,mean=True).data
-            mdata[:,i] = mod.integrateInSpace(region=basin,mean=True).data
-        obs.data = odata; obs.ndata = odata.shape[1]; obs.spatial = False
-        mod.data = mdata; mod.ndata = mdata.shape[1]; mod.spatial = False
-        mod.data.mask = obs.data.mask
-
-        return obs,mod
-
-    def requires(self):
-        return ["tws"],["pr","evspsbl","mrro"]
-
-    def confront(self,m):
-        """Confront the GRACE data by computing a mean over river
-        basins. Fine-scale, point comparisons aren't meaningful as the
-        underlying resolution of the GRACE data is 300-400 [m]. See
-        the following publication for more information.
-
-        Swenson, Sean & National Center for Atmospheric Research Staff
-        (Eds). Last modified 08 Oct 2013. "The Climate Data Guide:
-        GRACE: Gravity Recovery and Climate Experiment: Surface mass,
-        total water storage, and derived variables." Retrieved from
-        https://climatedataguide.ucar.edu/climate-data/grace-gravity-recovery-and-climate-experiment-surface-mass-total-water-storage-and.
-
-        """
-        obs,mod = self.stageData(m)
-
-        # find the magnitude of the anomaly
-        obs_anom     = obs.rms()
-        obs_anom_val = obs_anom.siteStats()
-        mod_anom     = mod.rms()
-        mod_anom_val = mod_anom.siteStats()
-        rmse         = obs.rmse(mod).convert(obs.unit)
-        rmse_val     = rmse.siteStats()
-        rmse_smap    = Variable(name = "",
-                                unit = "1",
-                                data = np.exp(-rmse.data/obs_anom.data),
-                                ndata = obs.ndata,
-                                lat   = obs.lat,
-                                lon   = obs.lon)
-        rmse_score   = rmse_smap.siteStats()
-        iav_score    = Variable(name = "Interannual Variability Score global",
-                                unit = "1",
-                                data = np.exp(-np.abs(mod_anom_val.data-obs_anom_val.data)/obs_anom_val.data))
-
-        # remap for plotting
-        obs_anom_map = self._extendSitesToMap(obs_anom )
-        mod_anom_map = self._extendSitesToMap(mod_anom )
-        rmse_map     = self._extendSitesToMap(rmse     )
-        rmse_smap    = self._extendSitesToMap(rmse_smap)
-
-        # renames
-        obs_anom_val.name = "Anomaly Magnitude global"
-        mod_anom_val.name = "Anomaly Magnitude global"
-        obs_anom_map.name = "timeint_of_anomaly"
-        mod_anom_map.name = "timeint_of_anomaly"
-        rmse_map    .name = "rmse_of_anomaly"
-        rmse_smap   .name = "rmsescore_of_anomaly"
-        rmse_val    .name = "RMSE global"
-        rmse_score  .name = "RMSE Score global"
-
-        # dump results to a netCDF4 file
-        results = Dataset(os.path.join(self.output_path,"%s_%s.nc" % (self.name,m.name)),mode="w")
-        results.setncatts({"name" :m.name, "color":m.color, "weight":self.cweight,"complete":0})
-        mod         .toNetCDF4(results,group="MeanState")
-        mod_anom_val.toNetCDF4(results,group="MeanState")
-        mod_anom_map.toNetCDF4(results,group="MeanState")
-        rmse_map    .toNetCDF4(results,group="MeanState")
-        rmse_smap   .toNetCDF4(results,group="MeanState")
-        rmse_val    .toNetCDF4(results,group="MeanState")
-        rmse_score  .toNetCDF4(results,group="MeanState")
-        iav_score   .toNetCDF4(results,group="MeanState")
-        results.setncattr("complete",1)
-        results.close()
-        if self.master:
-            results = Dataset(os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name)),mode="w")
-            results.setncatts({"name" :"Benchmark", "color":np.asarray([0.5,0.5,0.5]),"weight":self.cweight,"complete":0})
-            obs.toNetCDF4(results,group="MeanState")
-            obs_anom_val.toNetCDF4(results,group="MeanState")
-            obs_anom_map.toNetCDF4(results,group="MeanState")
-            results.setncattr("complete",1)
-            results.close()
+        # Extract the observational data for basins
+        obs = Variable(
+            filename=self.source,
+            variable_name=self.variable,
+            t0=None if len(self.study_limits) != 2 else self.study_limits[0],
+            tf=None if len(self.study_limits) != 2 else self.study_limits[1],
+        ).convert("mm d-1")
+
+        # Extract the globally gridded runoff
+        mod = m.extractTimeSeries(
+            self.variable,
+            alt_vars=self.alternate_vars,
+            initial_time=obs.time_bnds[0, 0],
+            final_time=obs.time_bnds[-1, 1],
+        )
+
+        # Save original model data as omod
+        omod = mod
+
+        # Some models output total volumetric rate instead of an area density
+        if (Unit(mod.unit) / Unit("m3 s-1")).is_dimensionless():
+            mod.data /= mod.area
+            mod.unit = mod.unit + " m-2"
+
+        # We want annual mean, not monthly mean
+        years = np.asarray([obs.time_bnds[::12, 0], obs.time_bnds[11::12, 1]]).T
+        obs = obs.coarsenInTime(years)
+        mod = mod.coarsenInTime(years)
+        obs.name = "runoff"
+        mod.name = "runoff"
+
+        # Operate on model data to compute mean runoff values in each basin.
+        data = np.ma.zeros(obs.data.shape)
+        for i, basin in enumerate(self.basins):
+            b = il.ClipTime(
+                mod.integrateInSpace(region=basin, mean=True),
+                obs.time_bnds[0, 0],
+                obs.time_bnds[-1, 1],
+            ).convert(obs.unit)
+            data[:, i] = b.data
+
+        # Create a variable to return for the model
+        mod = Variable(
+            name=obs.name,
+            unit=obs.unit,
+            data=np.ma.masked_array(data, mask=obs.data.mask),
+            time=obs.time,
+            time_bnds=obs.time_bnds,
+            ndata=obs.ndata,
+            lat=obs.lat,
+            lat_bnds=obs.lat_bnds,
+            lon=obs.lon,
+            lon_bnds=obs.lon_bnds,
+        )
+        return obs, mod, omod
 
-    def _extendSitesToMap(self,var):
+    def _extendSitesToMap(self, var, modvar):
         """A local function to extend site data to the basins.
 
         Parameters
         ----------
         var : ILAMB.Variable.Variable
             the site-based variable we wish to extend to basins
 
@@ -201,57 +122,150 @@
             the spatial variable which is the extended version of the
             input variable
         """
 
         # determine the global mask
         global_mask = None
         global_data = None
-        for i,basin in enumerate(self.basins):
-            name,lat,lon,mask = Regions._regions[basin]
-            keep = (mask == False)
+        for i, basin in enumerate(self.basins):
+            if len(Regions._regions[basin]) == 3:
+                name, catid, shape = Regions._regions[basin]
+                r = Regions()
+                lat, lon, mask = r.getMaskLatLon(basin, modvar)
+            if len(Regions._regions[basin]) == 4:
+                name, lat, lon, mask = Regions._regions[basin]
+            keep = mask == False
             if global_mask is None:
-                global_mask  = np.copy(mask)
-                global_data  = keep*var.data[i]
+                global_mask = np.copy(mask)
+                global_data = keep * var.data[i]
             else:
                 global_mask *= mask
-                global_data += keep*var.data[i]
-        return Variable(name      = var.name,
-                        unit      = var.unit,
-                        data      = np.ma.masked_array(global_data,mask=global_mask),
-                        lat       = lat,
-                        lon       = lon)
+                global_data += keep * var.data[i]
+        return Variable(
+            name=var.name,
+            unit=var.unit,
+            data=np.ma.masked_array(global_data, mask=global_mask),
+            lat=lat,
+            lon=lon,
+        )
+
+    def confront(self, m):
+        """The analysis portion applied to basins as if they were datasites
 
-    def modelPlots(self,m):
+        Parameters
+        ----------
+        m : ILAMB.ModelResult.ModelResult
+            the model result context
+        """
+        # Grab the data
+        obs, mod, omod = self.stageData(m)
+
+        # Basic analysis from ilamblib.AnalysisMeanState() for
+        # datasites and only the global region
+        obs_timeint = obs.integrateInTime(mean=True)
+        mod_timeint = mod.integrateInTime(mean=True)
+        bias_map = obs_timeint.bias(mod_timeint)
+        normalizer = obs_timeint.data
+        bias_score_map = il.Score(bias_map, obs_timeint)
+        obs_period_mean = obs_timeint.siteStats()
+        mod_period_mean = mod_timeint.siteStats()
+        bias = bias_map.siteStats()
+        bias_score = bias_score_map.siteStats(weight=normalizer)
+        std, R, sd_score = obs_timeint.spatialDistribution(mod_timeint)
+        obs_iav_map = obs.interannualVariability()
+        mod_iav_map = mod.interannualVariability()
+        iav_score_map = obs_iav_map.spatialDifference(mod_iav_map)
+        iav_score_map = il.Score(iav_score_map, obs_iav_map)
+        iav_score = iav_score_map.siteStats()
+
+        # Extend a few quantities from datasites to their
+        # corresponding basins (plotting only)
+        obs_timeint = self._extendSitesToMap(obs_timeint, omod)
+        mod_timeint = self._extendSitesToMap(mod_timeint, omod)
+        bias_map = self._extendSitesToMap(bias_map, omod)
+
+        # Rename some quantities for parsing later in the HTML
+        # generation
+        obs_period_mean.name = "Period Mean global"
+        mod_period_mean.name = "Period Mean global"
+        bias.name = "Bias global"
+        bias_score.name = "Bias Score global"
+        sd_score.name = "Spatial Distribution Score global"
+        obs_timeint.name = "timeint_of_runoff"
+        mod_timeint.name = "timeint_of_runoff"
+        bias_map.name = "bias_map_of_runoff"
+        iav_score.name = "Interannual Variability Score global"
+
+        # Dump to files
+        results = Dataset(
+            os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name)), mode="w"
+        )
+        results.setncatts(
+            {"name": m.name, "color": m.color, "weight": self.cweight, "complete": 0}
+        )
+        for var in [
+            mod,
+            mod_period_mean,
+            mod_timeint,
+            bias,
+            bias_score,
+            bias_map,
+            iav_score,
+        ]:
+            var.toNetCDF4(results, group="MeanState")
+        sd_score.toNetCDF4(
+            results, group="MeanState", attributes={"std": std.data, "R": R.data}
+        )
+        results.setncattr("complete", 1)
+        results.close()
+        if self.master:
+            results = Dataset(
+                os.path.join(self.output_path, "%s_Benchmark.nc" % self.name), mode="w"
+            )
+            results.setncatts(
+                {
+                    "name": "Benchmark",
+                    "color": np.asarray([0.5, 0.5, 0.5]),
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
+            for var in [obs, obs_period_mean, obs_timeint]:
+                var.toNetCDF4(results, group="MeanState")
+            results.setncattr("complete", 1)
+            results.close()
 
+    def modelPlots(self, m):
         # some of the plots can be generated using the standard
         # routine, with some modifications
-        super(ConfTWSA,self).modelPlots(m)
-        for page in self.layout.pages:
-            for sec in page.figures.keys():
-                for fig in page.figures[sec]:
-                    fig.side = fig.side.replace("MEAN","ANOMALY MAGNITUDE")
-
-        #
-        bname = os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name       ))
-        fname = os.path.join(self.output_path,"%s_%s.nc"        % (self.name,m.name))
+        super(ConfBasin, self).modelPlots(m)
+        bname = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name))
+        fname = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
 
         # get the HTML page
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
 
-        if not os.path.isfile(bname): return
-        if not os.path.isfile(fname): return
-        obs = Variable(filename = bname, variable_name = "twsa", groupname = "MeanState")
-        mod = Variable(filename = fname, variable_name = "twsa", groupname = "MeanState")
-        for i,basin in enumerate(self.basins):
-
-            page.addFigure("Spatially integrated regional mean",
-                           basin,
-                           "MNAME_global_%s.png" % basin,
-                           basin,False,longname=basin)
-
-            fig,ax = plt.subplots(figsize=(6.8,2.8),tight_layout=True)
-            ax.plot(obs.time/365+1850,obs.data[:,i],lw=2,color='k',alpha=0.5)
-            ax.plot(mod.time/365+1850,mod.data[:,i],lw=2,color=m.color      )
+        if not os.path.isfile(bname):
+            return
+        if not os.path.isfile(fname):
+            return
+        obs = Variable(filename=bname, variable_name="runoff", groupname="MeanState")
+        mod = Variable(filename=fname, variable_name="runoff", groupname="MeanState")
+        for i, basin in enumerate(self.basins):
+            page.addFigure(
+                "Spatially integrated regional mean",
+                basin,
+                "MNAME_global_%s.png" % basin,
+                basin,
+                False,
+                longname=basin,
+            )
+
+            fig, ax = plt.subplots(figsize=(6.8, 2.8), tight_layout=True)
+            ax.plot(obs.time / 365 + 1850, obs.data[:, i], lw=2, color="k", alpha=0.5)
+            ax.plot(mod.time / 365 + 1850, mod.data[:, i], lw=2, color=m.color)
             ax.grid()
             ax.set_ylabel(post.UnitStringToMatplotlib(obs.unit))
-            fig.savefig(os.path.join(self.output_path,"%s_global_%s.png" % (m.name,basin)))
+            fig.savefig(
+                os.path.join(self.output_path, "%s_global_%s.png" % (m.name, basin))
+            )
             plt.close()
```

### Comparing `ILAMB-2.6/src/ILAMB/ConfUncertainty.py` & `ILAMB-2.7/src/ILAMB/ConfUncertainty.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,321 +1,445 @@
-from .Confrontation import Confrontation
-from .Variable import Variable
-from .Regions import Regions
-from . import Post as post
-from . import ilamblib as il
+import os
+import re
+
 import matplotlib.pyplot as plt
 import numpy as np
-import re
-import os
 
-def AnalysisUncertaintySpatial(ref,com,**keywords):
-    """blah
-    """
-    regions            = keywords.get("regions"           ,["global"])
-    dataset            = keywords.get("dataset"           ,None)
-    benchmark_dataset  = keywords.get("benchmark_dataset" ,None)
-    space_mean         = keywords.get("space_mean"        ,True)
-    table_unit         = keywords.get("table_unit"        ,None)
-    plot_unit          = keywords.get("plot_unit"         ,None)
-    mass_weighting     = keywords.get("mass_weighting"    ,False)
-    expert_uncertainty = keywords.get("expert_uncertainty",None)            
-    use_expert         = keywords.get("use_expert"        ,False)
-    ILAMBregions       = Regions()
-    spatial            = ref.spatial
+from ILAMB import Post as post
+from ILAMB import ilamblib as il
+from ILAMB.Confrontation import Confrontation
+from ILAMB.Regions import Regions
+from ILAMB.Variable import Variable
+
+
+def AnalysisUncertaintySpatial(ref, com, **keywords):
+    """blah"""
+    regions = keywords.get("regions", ["global"])
+    dataset = keywords.get("dataset", None)
+    benchmark_dataset = keywords.get("benchmark_dataset", None)
+    space_mean = keywords.get("space_mean", True)
+    table_unit = keywords.get("table_unit", None)
+    plot_unit = keywords.get("plot_unit", None)
+    mass_weighting = keywords.get("mass_weighting", False)
+    expert_uncertainty = keywords.get("expert_uncertainty", None)
+    use_expert = keywords.get("use_expert", False)
+    ILAMBregions = Regions()
+    spatial = ref.spatial
     name = ref.name
     ref_timeint = None
     com_timeint = None
-                
+
     # Interpolate both reference and comparison to a grid composed of
     # their cell breaks
     ref.convert(plot_unit)
     com.convert(plot_unit)
-    lat,lon,lat_bnds,lon_bnds = il._composeGrids(ref,com)
-    REF   = ref.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
-    COM   = com.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
-    unit  = REF.unit
-    area  = REF.area
+    lat, lon, lat_bnds, lon_bnds = il._composeGrids(ref, com)
+    REF = ref.interpolate(lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds)
+    COM = com.interpolate(lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds)
+    unit = REF.unit
+    area = REF.area
     ndata = REF.ndata
 
     # Find the mean values over the time period
     if ref_timeint is None:
         ref_timeint = ref.integrateInTime(mean=True).convert(plot_unit)
         REF_timeint = REF.integrateInTime(mean=True).convert(plot_unit)
     else:
         ref_timeint.convert(plot_unit)
-        REF_timeint = ref_timeint.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
+        REF_timeint = ref_timeint.interpolate(
+            lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds
+        )
     if com_timeint is None:
         com_timeint = com.integrateInTime(mean=True).convert(plot_unit)
         COM_timeint = COM.integrateInTime(mean=True).convert(plot_unit)
     else:
         com_timeint.convert(plot_unit)
-        COM_timeint = com_timeint.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
-    normalizer  = REF_timeint.data if mass_weighting else None
+        COM_timeint = com_timeint.interpolate(
+            lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds
+        )
+    normalizer = REF_timeint.data if mass_weighting else None
 
     # Report period mean values over all possible representations of
     # land
     ref_and_com = (REF_timeint.data.mask == False) * (COM_timeint.data.mask == False)
-    ref_not_com = (REF_timeint.data.mask == False) * (COM_timeint.data.mask == True )
-    com_not_ref = (REF_timeint.data.mask == True ) * (COM_timeint.data.mask == False)
+    ref_not_com = (REF_timeint.data.mask == False) * (COM_timeint.data.mask == True)
+    com_not_ref = (REF_timeint.data.mask == True) * (COM_timeint.data.mask == False)
     if benchmark_dataset is not None:
-
         ref_timeint.name = "timeint_of_%s" % name
-        ref_timeint.toNetCDF4(benchmark_dataset,group="MeanState")
+        ref_timeint.toNetCDF4(benchmark_dataset, group="MeanState")
         for region in regions:
-
             # reference period mean on original grid
-            ref_period_mean = ref_timeint.integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            ref_period_mean = ref_timeint.integrateInSpace(
+                region=region, mean=space_mean
+            ).convert(table_unit)
             ref_period_mean.name = "Period Mean (original grids) %s" % region
-            ref_period_mean.toNetCDF4(benchmark_dataset,group="MeanState")
+            ref_period_mean.toNetCDF4(benchmark_dataset, group="MeanState")
 
     if dataset is not None:
-
         com_timeint.name = "timeint_of_%s" % name
-        com_timeint.toNetCDF4(dataset,group="MeanState")
+        com_timeint.toNetCDF4(dataset, group="MeanState")
         for region in regions:
-
             # reference period mean on intersection of land
-            ref_union_mean = Variable(name = "REF_and_com", unit = REF_timeint.unit,
-                                      data = np.ma.masked_array(REF_timeint.data,mask=(ref_and_com==False)),
-                                      lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                      area = REF_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            ref_union_mean = (
+                Variable(
+                    name="REF_and_com",
+                    unit=REF_timeint.unit,
+                    data=np.ma.masked_array(
+                        REF_timeint.data, mask=(ref_and_com == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=REF_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
             ref_union_mean.name = "Benchmark Period Mean (intersection) %s" % region
-            ref_union_mean.toNetCDF4(dataset,group="MeanState")
+            ref_union_mean.toNetCDF4(dataset, group="MeanState")
 
             # reference period mean on complement of land
-            ref_comp_mean = Variable(name = "REF_not_com", unit = REF_timeint.unit,
-                                     data = np.ma.masked_array(REF_timeint.data,mask=(ref_not_com==False)),
-                                     lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                     area = REF_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
-            if ref_comp_mean.data.mask: ref_comp_mean.data = np.ma.masked_array([0.])
+            ref_comp_mean = (
+                Variable(
+                    name="REF_not_com",
+                    unit=REF_timeint.unit,
+                    data=np.ma.masked_array(
+                        REF_timeint.data, mask=(ref_not_com == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=REF_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
+            if ref_comp_mean.data.mask:
+                ref_comp_mean.data = np.ma.masked_array([0.0])
             ref_comp_mean.name = "Benchmark Period Mean (complement) %s" % region
-            ref_comp_mean.toNetCDF4(dataset,group="MeanState")
+            ref_comp_mean.toNetCDF4(dataset, group="MeanState")
 
             # comparison period mean on original grid
-            com_period_mean = com_timeint.integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            com_period_mean = com_timeint.integrateInSpace(
+                region=region, mean=space_mean
+            ).convert(table_unit)
             com_period_mean.name = "Period Mean (original grids) %s" % region
-            com_period_mean.toNetCDF4(dataset,group="MeanState")
+            com_period_mean.toNetCDF4(dataset, group="MeanState")
 
             # comparison period mean on intersection of land
-            com_union_mean = Variable(name = "ref_and_COM", unit = COM_timeint.unit,
-                                      data = np.ma.masked_array(COM_timeint.data,mask=(ref_and_com==False)),
-                                      lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                      area = COM_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            com_union_mean = (
+                Variable(
+                    name="ref_and_COM",
+                    unit=COM_timeint.unit,
+                    data=np.ma.masked_array(
+                        COM_timeint.data, mask=(ref_and_com == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=COM_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
             com_union_mean.name = "Model Period Mean (intersection) %s" % region
-            com_union_mean.toNetCDF4(dataset,group="MeanState")
+            com_union_mean.toNetCDF4(dataset, group="MeanState")
 
             # comparison period mean on complement of land
-            com_comp_mean = Variable(name = "COM_not_ref", unit = COM_timeint.unit,
-                                     data = np.ma.masked_array(COM_timeint.data,mask=(com_not_ref==False)),
-                                     lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                     area = COM_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
-            if com_comp_mean.data.mask: com_comp_mean.data = np.ma.masked_array([0.])
+            com_comp_mean = (
+                Variable(
+                    name="COM_not_ref",
+                    unit=COM_timeint.unit,
+                    data=np.ma.masked_array(
+                        COM_timeint.data, mask=(com_not_ref == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=COM_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
+            if com_comp_mean.data.mask:
+                com_comp_mean.data = np.ma.masked_array([0.0])
             com_comp_mean.name = "Model Period Mean (complement) %s" % region
-            com_comp_mean.toNetCDF4(dataset,group="MeanState")
+            com_comp_mean.toNetCDF4(dataset, group="MeanState")
 
     # Now that we are done reporting on the intersection / complement,
     # set all masks to the intersection
-    REF.data.mask += np.ones(REF.time.size,dtype=bool)[:,np.newaxis,np.newaxis] * (ref_and_com==False)
-    COM.data.mask += np.ones(COM.time.size,dtype=bool)[:,np.newaxis,np.newaxis] * (ref_and_com==False)
-    REF_timeint.data.mask = (ref_and_com==False)
-    COM_timeint.data.mask = (ref_and_com==False)
-    if mass_weighting: normalizer.mask = (ref_and_com==False)
+    REF.data.mask += np.ones(REF.time.size, dtype=bool)[:, np.newaxis, np.newaxis] * (
+        ref_and_com == False
+    )
+    COM.data.mask += np.ones(COM.time.size, dtype=bool)[:, np.newaxis, np.newaxis] * (
+        ref_and_com == False
+    )
+    REF_timeint.data.mask = ref_and_com == False
+    COM_timeint.data.mask = ref_and_com == False
+    if mass_weighting:
+        normalizer.mask = ref_and_com == False
 
     # Bias: maps, scalars, and scores
     bias = REF_timeint.bias(COM_timeint).convert(plot_unit)
-    cREF = Variable(name = "centralized %s" % name, unit = REF.unit,
-                    data = np.ma.masked_array(REF.data-REF_timeint.data[np.newaxis,...],mask=REF.data.mask),
-                    time = REF.time, time_bnds = REF.time_bnds, ndata = REF.ndata,
-                    lat  = lat, lat_bnds = lat_bnds, lon = lon, lon_bnds = lon_bnds, area = REF.area).convert(plot_unit)
+    cREF = Variable(
+        name="centralized %s" % name,
+        unit=REF.unit,
+        data=np.ma.masked_array(
+            REF.data - REF_timeint.data[np.newaxis, ...], mask=REF.data.mask
+        ),
+        time=REF.time,
+        time_bnds=REF.time_bnds,
+        ndata=REF.ndata,
+        lat=lat,
+        lat_bnds=lat_bnds,
+        lon=lon,
+        lon_bnds=lon_bnds,
+        area=REF.area,
+    ).convert(plot_unit)
     REF_std = cREF.rms()
-    bias_score_map = il.Score(bias,REF_std if REF.time.size > 1 else REF_timeint)
-    bias_score_map.data.mask = (ref_and_com==False) # for some reason I need to explicitly force the mask
+    bias_score_map = il.Score(bias, REF_std if REF.time.size > 1 else REF_timeint)
+    bias_score_map.data.mask = (
+        ref_and_com == False
+    )  # for some reason I need to explicitly force the mask
     if dataset is not None:
         bias.name = "bias_map_of_%s" % name
-        bias.toNetCDF4(dataset,group="MeanState")
+        bias.toNetCDF4(dataset, group="MeanState")
         bias_score_map.name = "biasscore_map_of_%s" % name
-        bias_score_map.toNetCDF4(dataset,group="MeanState")
+        bias_score_map.toNetCDF4(dataset, group="MeanState")
         for region in regions:
-            bias_val = bias.integrateInSpace(region=region,mean=True).convert(plot_unit)
+            bias_val = bias.integrateInSpace(region=region, mean=True).convert(
+                plot_unit
+            )
             bias_val.name = "Bias %s" % region
-            bias_val.toNetCDF4(dataset,group="MeanState")
-            bias_score = bias_score_map.integrateInSpace(region=region,mean=True,weight=normalizer)
+            bias_val.toNetCDF4(dataset, group="MeanState")
+            bias_score = bias_score_map.integrateInSpace(
+                region=region, mean=True, weight=normalizer
+            )
             bias_score.name = "Bias Score %s" % region
-            bias_score.toNetCDF4(dataset,group="MeanState")
-    del bias,bias_score,cREF
-    
+            bias_score.toNetCDF4(dataset, group="MeanState")
+    del bias, bias_score, cREF
+
     # Changes with uncertainty ----------------------------------------------------------------
-    
+
     # convert units of the expert value is present
     if expert_uncertainty is not None:
-        match = re.search("(-?\d*\.?\d*)\s(.*)",expert_uncertainty.strip())
+        match = re.search("(-?\d*\.?\d*)\s(.*)", expert_uncertainty.strip())
         if match:
-            expert_uncertainty = Variable(data = np.asarray(float(match.group(1))),
-                                          unit = match.group(2)).convert(ref.unit).data
+            expert_uncertainty = (
+                Variable(data=np.asarray(float(match.group(1))), unit=match.group(2))
+                .convert(ref.unit)
+                .data
+            )
         else:
             msg = "The option 'expert_uncertainty' must follow a VALUE UNIT format, where the UNIT must be convertible to the reference dataset unit"
             raise ValueError(msg)
-            
+
     # perform some logic/checks on what uncertainty to use
     if use_expert:
         if expert_uncertainty is None:
             msg = "The 'use_expert' option was given, but no 'expert_uncertainty' specified in the configure file"
             raise ValueError(msg)
         Dref = expert_uncertainty
     else:
         if REF.data_bnds is None:
             if expert_uncertainty is None:
                 msg = "No uncertainty is specified for this variable, try specifying 'expert_uncertainty' in the configure file"
                 raise ValueError(msg)
             Dref = expert_uncertainty
         else:
-            Dref = np.sqrt((REF.data-REF.data_bnds[...,0])**2 + (REF.data_bnds[...,1]-REF.data)**2)
+            Dref = np.sqrt(
+                (REF.data - REF.data_bnds[..., 0]) ** 2
+                + (REF.data_bnds[..., 1] - REF.data) ** 2
+            )
 
-    with np.errstate(under='ignore',over='ignore'):
+    with np.errstate(under="ignore", over="ignore"):
         norm = REF_std if REF.time.size > 1 else REF_timeint
-        diff = (COM.data-REF.data_bnds[...,1]).clip(min=0) - (COM.data-REF.data_bnds[...,0]).clip(max=0)
+        diff = (COM.data - REF.data_bnds[..., 1]).clip(min=0) - (
+            COM.data - REF.data_bnds[..., 0]
+        ).clip(max=0)
         assert diff.min() >= 0
-        bias_uscore_map = np.exp(-diff/norm.data[np.newaxis,...])
-    bias_uscore_map = Variable(name = "biasuscore_map_of_u%s" % name,
-                               unit = "1",
-                               data = bias_uscore_map,
-                               time = REF.time, time_bnds = REF.time_bnds,
-                               lat  = REF.lat, lat_bnds = REF.lat_bnds,
-                               lon  = REF.lon, lon_bnds = REF.lon_bnds,
-                               area = REF.area, ndata = REF.ndata).integrateInTime(mean=True)
+        bias_uscore_map = np.exp(-diff / norm.data[np.newaxis, ...])
+    bias_uscore_map = Variable(
+        name="biasuscore_map_of_u%s" % name,
+        unit="1",
+        data=bias_uscore_map,
+        time=REF.time,
+        time_bnds=REF.time_bnds,
+        lat=REF.lat,
+        lat_bnds=REF.lat_bnds,
+        lon=REF.lon,
+        lon_bnds=REF.lon_bnds,
+        area=REF.area,
+        ndata=REF.ndata,
+    ).integrateInTime(mean=True)
     if benchmark_dataset is not None:
         if Dref.size == 1:
-            Variable(name = "Expert Uncertainty",
-                     unit = REF.unit,
-                     data = Dref).toNetCDF4(benchmark_dataset,group="MeanState")
+            Variable(name="Expert Uncertainty", unit=REF.unit, data=Dref).toNetCDF4(
+                benchmark_dataset, group="MeanState"
+            )
         else:
-            Dref = Variable(name = "uncertain",
-                            unit = ref.unit,
-                            data = Dref,
-                            time = REF.time, time_bnds = REF.time_bnds,
-                            lat  = REF.lat, lat_bnds = REF.lat_bnds,
-                            lon  = REF.lon, lon_bnds = REF.lon_bnds,
-                            area = REF.area, ndata = REF.ndata).integrateInTime(mean=True)
+            Dref = Variable(
+                name="uncertain",
+                unit=ref.unit,
+                data=Dref,
+                time=REF.time,
+                time_bnds=REF.time_bnds,
+                lat=REF.lat,
+                lat_bnds=REF.lat_bnds,
+                lon=REF.lon,
+                lon_bnds=REF.lon_bnds,
+                area=REF.area,
+                ndata=REF.ndata,
+            ).integrateInTime(mean=True)
             Dref.name = "uncertain"
-            Dref.toNetCDF4(benchmark_dataset,group="MeanState")
+            Dref.toNetCDF4(benchmark_dataset, group="MeanState")
 
     if dataset is not None:
         bias_uscore_map.name = "biasuscore_map_of_%s" % name
-        bias_uscore_map.toNetCDF4(dataset,group="MeanState")
+        bias_uscore_map.toNetCDF4(dataset, group="MeanState")
         for region in regions:
-            bias_uscore = bias_uscore_map.integrateInSpace(region=region,mean=True,weight=normalizer)
+            bias_uscore = bias_uscore_map.integrateInSpace(
+                region=region, mean=True, weight=normalizer
+            )
             bias_uscore.name = "Uncertainty Bias Score %s" % region
-            bias_uscore.toNetCDF4(dataset,group="MeanState")
-            
-class ConfUncertainty(Confrontation):
+            bias_uscore.toNetCDF4(dataset, group="MeanState")
 
-    def confront(self,m):
 
+class ConfUncertainty(Confrontation):
+    def confront(self, m):
         # parse options
-        expert_uncertainty = self.keywords.get("expert_uncertainty",None)            
-        use_expert         = self.keywords.get("use_expert"        ,"False").lower() == "true"
-        
-        # get reference and model data
-        ref,mod = self.stageData(m)
+        expert_uncertainty = self.keywords.get("expert_uncertainty", None)
+        use_expert = self.keywords.get("use_expert", "False").lower() == "true"
 
-        mod_file = os.path.join(self.output_path,"%s_%s.nc"        % (self.name,m.name))
-        ref_file = os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name,      ))
-        with il.FileContextManager(self.master,mod_file,ref_file) as fcm:
+        # get reference and model data
+        ref, mod = self.stageData(m)
 
+        mod_file = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
+        ref_file = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name,))
+        with il.FileContextManager(self.master, mod_file, ref_file) as fcm:
             # Encode some names and colors
-            fcm.mod_dset.setncatts({"name" :m.name,
-                                    "color":m.color,
-                                    "weight":self.cweight,
-                                    "complete":0})
+            fcm.mod_dset.setncatts(
+                {
+                    "name": m.name,
+                    "color": m.color,
+                    "weight": self.cweight,
+                    "complete": 0,
+                }
+            )
+            if self.master:
+                fcm.obs_dset.setncatts(
+                    {
+                        "name": "Benchmark",
+                        "color": np.asarray([0.5, 0.5, 0.5]),
+                        "weight": self.cweight,
+                        "complete": 0,
+                    }
+                )
+            AnalysisUncertaintySpatial(
+                ref,
+                mod,
+                dataset=fcm.mod_dset,
+                regions=self.regions,
+                benchmark_dataset=fcm.obs_dset,
+                table_unit=self.table_unit,
+                plot_unit=self.plot_unit,
+                space_mean=self.space_mean,
+                expert_uncertainty=expert_uncertainty,
+                use_expert=use_expert,
+            )
+            fcm.mod_dset.setncattr("complete", 1)
             if self.master:
-                fcm.obs_dset.setncatts({"name" :"Benchmark",
-                                        "color":np.asarray([0.5,0.5,0.5]),
-                                        "weight":self.cweight,
-                                        "complete":0})
-            AnalysisUncertaintySpatial(ref,mod,dataset    = fcm.mod_dset,
-                                       regions            = self.regions,
-                                       benchmark_dataset  = fcm.obs_dset,
-                                       table_unit         = self.table_unit,
-                                       plot_unit          = self.plot_unit,
-                                       space_mean         = self.space_mean,
-                                       expert_uncertainty = expert_uncertainty,
-                                       use_expert         = use_expert)
-            fcm.mod_dset.setncattr("complete",1)
-            if self.master: fcm.obs_dset.setncattr("complete",1)
-        
-    def modelPlots(self,m):
-        super(ConfUncertainty,self).modelPlots(m)
+                fcm.obs_dset.setncattr("complete", 1)
+
+    def modelPlots(self, m):
+        super(ConfUncertainty, self).modelPlots(m)
         for page in self.layout.pages:
             for sec in page.figures.keys():
                 for fig in page.figures[sec]:
-                    if fig.side == "BIAS SCORE": fig.side = "ORIGINAL BIAS SCORE"
-        bname = os.path.join(self.output_path,"%s_Benchmark.nc" % (self.name       ))
-        fname = os.path.join(self.output_path,"%s_%s.nc"        % (self.name,m.name))
+                    if fig.side == "BIAS SCORE":
+                        fig.side = "ORIGINAL BIAS SCORE"
+        bname = os.path.join(self.output_path, "%s_Benchmark.nc" % (self.name))
+        fname = os.path.join(self.output_path, "%s_%s.nc" % (self.name, m.name))
         page = [page for page in self.layout.pages if "MeanState" in page.name][0]
-        if not os.path.isfile(bname): return
-        if not os.path.isfile(fname): return
+        if not os.path.isfile(bname):
+            return
+        if not os.path.isfile(fname):
+            return
 
         v = None
         try:
-            v = Variable(filename = bname, variable_name = "uncertain", groupname = "MeanState")
+            v = Variable(
+                filename=bname, variable_name="uncertain", groupname="MeanState"
+            )
         except:
             pass
         if v is not None:
             pname = "uncertain"
-            page.addFigure("Temporally integrated period mean",
-                           pname,
-                           "MNAME_RNAME_uncertain.png",
-                           side = "BENCHMARK UNCERTAINTY",
-                           legend = True)
-            vmin,vmax = np.percentile(v.data.compressed(),[1,99])
-            
-            fig,ax = plt.subplots(figsize=(6.8,1.0),tight_layout=True)
-            post.ColorBar(ax,
-                          vmin = vmin,
-                          vmax = vmax,
-                          cmap = "Reds",
-                          label = v.unit)
-            fig.savefig(os.path.join(self.output_path,"legend_%s.png" % (pname)))
+            page.addFigure(
+                "Temporally integrated period mean",
+                pname,
+                "MNAME_RNAME_uncertain.png",
+                side="BENCHMARK UNCERTAINTY",
+                legend=True,
+            )
+            vmin, vmax = np.percentile(v.data.compressed(), [1, 99])
+
+            fig, ax = plt.subplots(figsize=(6.8, 1.0), tight_layout=True)
+            post.ColorBar(ax, vmin=vmin, vmax=vmax, cmap="Reds", label=v.unit)
+            fig.savefig(os.path.join(self.output_path, "legend_%s.png" % (pname)))
             plt.close()
-            
+
             for region in self.regions:
-                ax = v.plot(None,
-                            region = region,
-                            vmin   = vmin,
-                            vmax   = vmax,
-                            cmap   = "Reds")
+                ax = v.plot(None, region=region, vmin=vmin, vmax=vmax, cmap="Reds")
                 fig = ax.get_figure()
-                fig.savefig(os.path.join(self.output_path,"%s_%s_%s.png" % (m.name,region,pname)))
+                fig.savefig(
+                    os.path.join(
+                        self.output_path, "%s_%s_%s.png" % (m.name, region, pname)
+                    )
+                )
                 plt.close()
 
         v = None
         try:
-            v = Variable(filename = fname, variable_name = "biasuscore_map_of_%s" % self.variable, groupname = "MeanState")
+            v = Variable(
+                filename=fname,
+                variable_name="biasuscore_map_of_%s" % self.variable,
+                groupname="MeanState",
+            )
         except:
             pass
         if v is not None:
             pname = "biasuscore"
-            page.addFigure("Temporally integrated period mean",
-                           pname,
-                           "MNAME_RNAME_%s.png" % pname,
-                           side = "UNCERTAINTY BIAS SCORE",
-                           longname = "Temporally integrated period mean uncertainty bias score",
-                           legend = True)          
-            fig,ax = plt.subplots(figsize=(6.8,1.0),tight_layout=True)
-            post.ColorBar(ax,
-                          vmin = 0,
-                          vmax = 1,
-                          cmap = plt.cm.get_cmap("score",3),
-                          label = "1")
-            fig.savefig(os.path.join(self.output_path,"legend_%s.png" % (pname)))
+            page.addFigure(
+                "Temporally integrated period mean",
+                pname,
+                "MNAME_RNAME_%s.png" % pname,
+                side="UNCERTAINTY BIAS SCORE",
+                longname="Temporally integrated period mean uncertainty bias score",
+                legend=True,
+            )
+            fig, ax = plt.subplots(figsize=(6.8, 1.0), tight_layout=True)
+            post.ColorBar(
+                ax, vmin=0, vmax=1, cmap=plt.cm.get_cmap("score", 3), label="1"
+            )
+            fig.savefig(os.path.join(self.output_path, "legend_%s.png" % (pname)))
             plt.close()
-            
+
             for region in self.regions:
-                ax = v.plot(None,
-                            region = region,
-                            vmin   = 0,
-                            vmax   = 1,
-                            cmap   = plt.cm.get_cmap("score",3))
+                ax = v.plot(
+                    None,
+                    region=region,
+                    vmin=0,
+                    vmax=1,
+                    cmap=plt.cm.get_cmap("score", 3),
+                )
                 fig = ax.get_figure()
-                fig.savefig(os.path.join(self.output_path,"%s_%s_%s.png" % (m.name,region,pname)))
+                fig.savefig(
+                    os.path.join(
+                        self.output_path, "%s_%s_%s.png" % (m.name, region, pname)
+                    )
+                )
                 plt.close()
```

### Comparing `ILAMB-2.6/src/ILAMB/ModelResult.py` & `ILAMB-2.7/src/ILAMB/ModelResult.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,39 +1,49 @@
-from .Variable import Variable
-from netCDF4 import Dataset
-from . import ilamblib as il
-from cf_units import Unit
+import logging
+import os
+import re
+
 import numpy as np
-import glob,os,re
+from cf_units import Unit
 from mpi4py import MPI
-import logging
+from netCDF4 import Dataset
+
+from ILAMB import ilamblib as il
+from ILAMB.Variable import Variable
 
 logger = logging.getLogger("%i" % MPI.COMM_WORLD.rank)
 
-def _skipFile(pathName,altvars,lats,lons,same_site_epsilon):
+
+def _skipFile(pathName, altvars, lats, lons, same_site_epsilon):
     """Some simple logic intended to help speed up models which consist of
-       many single site runs.
+    many single site runs.
     """
-    if lats is None: return False
-    if lats.size > 1: return False
+    if lats is None:
+        return False
+    if lats.size > 1:
+        return False
     with Dataset(pathName) as dset:
         for v in altvars:
             if v in dset.variables:
                 V = dset.variables[v]
                 D = V.dimensions[-1]
-                if D not in ['data','lndgrid']: continue
-                if dset.dimensions[D].size > 1: continue
-                X = dset.variables['lat'][...]
-                Y = dset.variables['lon'][...]
-                Y = (Y<=180)*Y + (Y>180)*(Y-360) + (Y<-180)*360
-                if (np.sqrt((X-lats[0])**2+(Y-lons[0])**2) > same_site_epsilon): return True
+                if D not in ["data", "lndgrid"]:
+                    continue
+                if dset.dimensions[D].size > 1:
+                    continue
+                X = dset.variables["lat"][...]
+                Y = dset.variables["lon"][...]
+                Y = (Y <= 180) * Y + (Y > 180) * (Y - 360) + (Y < -180) * 360
+                if np.sqrt((X - lats[0]) ** 2 + (Y - lons[0]) ** 2) > same_site_epsilon:
+                    return True
 
     return False
 
-class ModelResult():
+
+class ModelResult:
     """A class for exploring model results.
 
     This class provides a simplified way of accessing model
     results. It is essentially a pointer to a top level directory and
     defines the model as all netCDF4 files found in its
     subdirectories. If this directory contains model output from
     several runs or experiments, you may wish to specify a string (the
@@ -54,83 +64,101 @@
     filter : str, optional
         this string must be in file's name for it to be considered as
         part of the model results
     model_year : 2-tuple of int, optional
         used to shift model times, all model years at model_year[0]
         are shifted to model_year[1]
     """
-    def __init__(self,path,modelname="unamed",color=(0,0,0),filter="",regex="",model_year=None,paths=None,description="",group=""):
-        self.path           = path
-        self.color          = color
-        self.filter         = filter
-        self.regex          = regex
-        self.shift          = 0.
-        if model_year is not None: self.shift = (model_year[1]-model_year[0])*365.
-        self.name           = modelname
+
+    def __init__(
+        self,
+        path,
+        modelname="unamed",
+        color=(0, 0, 0),
+        filter="",
+        regex="",
+        model_year=None,
+        paths=None,
+        description="",
+        group="",
+    ):
+        self.path = path
+        self.color = color
+        self.filter = filter
+        self.regex = regex
+        self.shift = 0.0
+        if model_year is not None:
+            self.shift = (model_year[1] - model_year[0]) * 365.0
+        self.name = modelname
         self.confrontations = {}
-        self.cell_areas     = None
-        self.land_fraction  = None
-        self.land_areas     = None
-        self.land_area      = None
-        self.variables      = None
-        self.names          = None
-        self.extents        = np.asarray([[-90.,+90.],[-180.,+180.]])
-        self.paths          = paths
-        self.description    = description
-        self.group          = group
+        self.cell_areas = None
+        self.land_fraction = None
+        self.land_areas = None
+        self.land_area = None
+        self.variables = None
+        self.names = None
+        self.extents = np.asarray([[-90.0, +90.0], [-180.0, +180.0]])
+        self.paths = paths
+        self.description = description
+        self.group = group
         self._findVariables()
         self._getGridInformation()
 
     def __str__(self):
-        s  = ""
+        s = ""
         s += "ModelResult: %s\n" % self.name
-        s += "-"*(len(self.name)+13) + "\n"
+        s += "-" * (len(self.name) + 13) + "\n"
         for key in self.names:
-            s += "{0:>20}: {1:<50}".format(key,self.names[key]) + "\n"
+            s += "{0:>20}: {1:<50}".format(key, self.names[key]) + "\n"
         return s
 
     def _findVariables(self):
-        """Loops through the netCDF4 files in a model's path and builds a dictionary of which variables are in which files.
-        """
-        def _get(key,dset):
+        """Loops through the netCDF4 files in a model's path and builds a dictionary of which variables are in which files."""
+
+        def _get(key, dset):
             dim_name = key
             try:
                 v = dset.variables[key]
                 dim_bnd_name = v.getncattr("bounds")
             except:
                 dim_bnd_name = None
-            return dim_name,dim_bnd_name
+            return dim_name, dim_bnd_name
 
         variables = {}
-        names     = {}
-        paths     = self.paths if self.paths is not None else [self.path]
+        names = {}
+        paths = self.paths if self.paths is not None else [self.path]
         experiment_id = None
         source_id = None
         for path in paths:
-            for subdir, dirs, files in os.walk(path,followlinks=True):
+            for subdir, dirs, files in os.walk(path, followlinks=True):
                 for fileName in files:
-                    if not fileName.endswith(".nc"): continue
-                    if self.filter not in fileName: continue
+                    if not fileName.endswith(".nc"):
+                        continue
+                    if self.filter not in fileName:
+                        continue
                     if self.regex is not "":
-                        m = re.search(self.regex,fileName)
-                        if not m: continue
-                    pathName  = os.path.join(subdir,fileName)
+                        m = re.search(self.regex, fileName)
+                        if not m:
+                            continue
+                    pathName = os.path.join(subdir, fileName)
 
                     try:
                         dataset = Dataset(pathName)
                     except:
-                        logger.debug("[%s] Error opening file %s" % (self.name,pathName))
+                        logger.debug(
+                            "[%s] Error opening file %s" % (self.name, pathName)
+                        )
                         continue
 
                     # harvest some model information
                     if experiment_id is None and "experiment_id" in dataset.ncattrs():
                         experiment_id = dataset.experiment_id
                     if source_id is None and "source_id" in dataset.ncattrs():
                         source_id = dataset.source_id
-                        
+
                     # populate dictionary for which variables are in which files
                     for key in dataset.variables.keys():
                         if key not in variables:
                             variables[key] = []
                         variables[key].append(pathName)
 
                         v = dataset.variables[key]
@@ -141,101 +169,145 @@
                             if "standard_name" in v.ncattrs():
                                 names[key] = v.standard_name
                                 continue
 
         # modify the description if none exists
         if self.description == "":
             d = []
-            if source_id is not None: d.append(source_id)
-            if experiment_id is not None: d.append(experiment_id)
+            if source_id is not None:
+                d.append(source_id)
+            if experiment_id is not None:
+                d.append(experiment_id)
             self.description = " ".join(d)
-            
+
         # determine spatial extents
-        lats = [key for key in variables.keys() if (key.lower().startswith("lat" ) or
-                                                    key.lower().  endswith("lat" ))]
-        lons = [key for key in variables.keys() if (key.lower().startswith("lon" ) or
-                                                    key.lower().  endswith("lon" ) or
-                                                    key.lower().startswith("long") or
-                                                    key.lower().  endswith("long"))]
+        lats = [
+            key
+            for key in variables.keys()
+            if (key.lower().startswith("lat") or key.lower().endswith("lat"))
+        ]
+        lons = [
+            key
+            for key in variables.keys()
+            if (
+                key.lower().startswith("lon")
+                or key.lower().endswith("lon")
+                or key.lower().startswith("long")
+                or key.lower().endswith("long")
+            )
+        ]
         for key in lats:
             for pathName in variables[key]:
                 with Dataset(pathName) as dset:
                     lat = dset.variables[key][...]
-                    if lat.size == 1: continue
-                    self.extents[0,0] = max(self.extents[0,0],lat.min())
-                    self.extents[0,1] = min(self.extents[0,1],lat.max())
+                    if lat.size == 1:
+                        continue
+                    self.extents[0, 0] = max(self.extents[0, 0], lat.min())
+                    self.extents[0, 1] = min(self.extents[0, 1], lat.max())
         for key in lons:
             for pathName in variables[key]:
                 with Dataset(pathName) as dset:
                     lon = dset.variables[key][...]
-                    if lon.size == 1: continue
-                    if lon.ndim < 1 or lon.ndim > 2: continue
-                    lon = (lon<=180)*lon + (lon>180)*(lon-360) + (lon<-180)*360
-                    self.extents[1,0] = max(self.extents[1,0],lon.min())
-                    self.extents[1,1] = min(self.extents[1,1],lon.max())
+                    if lon.size == 1:
+                        continue
+                    if lon.ndim < 1 or lon.ndim > 2:
+                        continue
+                    lon = (
+                        (lon <= 180) * lon
+                        + (lon > 180) * (lon - 360)
+                        + (lon < -180) * 360
+                    )
+                    self.extents[1, 0] = max(self.extents[1, 0], lon.min())
+                    self.extents[1, 1] = min(self.extents[1, 1], lon.max())
 
         # fix extents
-        eps = 5.
-        if self.extents[0,0] < (- 90.+eps): self.extents[0,0] = - 90.
-        if self.extents[0,1] > (+ 90.-eps): self.extents[0,1] = + 90.
-        if self.extents[1,0] < (-180.+eps): self.extents[1,0] = -180.
-        if self.extents[1,1] > (+180.-eps): self.extents[1,1] = +180.
+        eps = 5.0
+        if self.extents[0, 0] < (-90.0 + eps):
+            self.extents[0, 0] = -90.0
+        if self.extents[0, 1] > (+90.0 - eps):
+            self.extents[0, 1] = +90.0
+        if self.extents[1, 0] < (-180.0 + eps):
+            self.extents[1, 0] = -180.0
+        if self.extents[1, 1] > (+180.0 - eps):
+            self.extents[1, 1] = +180.0
         self.variables = variables
         self.names = names
 
     def _getGridInformation(self):
-        """Looks in the model output for cell areas as well as land fractions.
-        """
+        """Looks in the model output for cell areas as well as land fractions."""
+
         def _shiftLon(lon):
-            return (lon<=180)*lon + (lon>180)*(lon-360) + (lon<-180)*360
-        
+            return (lon <= 180) * lon + (lon > 180) * (lon - 360) + (lon < -180) * 360
+
         # Are there cell areas associated with this model?
         area_name = None
-        area_name = "area"      if "area"      in self.variables.keys() else area_name
+        area_name = "area" if "area" in self.variables.keys() else area_name
         area_name = "areacella" if "areacella" in self.variables.keys() else area_name
         if area_name is not None:
             with Dataset(self.variables[area_name][0]) as f:
                 A = f.variables[area_name]
                 unit = Unit(A.units) if "units" in A.ncattrs() else Unit("m2")
-                self.cell_areas = unit.convert(A[...],"m2",inplace=True)
+                self.cell_areas = unit.convert(A[...], "m2", inplace=True)
         else:
-            if not ("lat_bnds" in self.variables.keys() and
-                    "lon_bnds" in self.variables.keys()): return
+            if not (
+                "lat_bnds" in self.variables.keys()
+                and "lon_bnds" in self.variables.keys()
+            ):
+                return
             with Dataset(self.variables["lat_bnds"][0]) as f:
                 x = f.variables["lat_bnds"][...]
             with Dataset(self.variables["lon_bnds"][0]) as f:
                 y = f.variables["lon_bnds"][...]
                 s = y.mean(axis=1).argmin()
-                y = np.roll(_shiftLon(y),-s,axis=0)
-                if y[ 0,0] > y[ 0,1]: y[ 0,0] = -180.
-                if y[-1,0] > y[-1,1]: y[-1,1] = +180.
-            self.cell_areas = il.CellAreas(None,None,lat_bnds=x,lon_bnds=y)
-            
+                y = np.roll(_shiftLon(y), -s, axis=0)
+                if y[0, 0] > y[0, 1]:
+                    y[0, 0] = -180.0
+                if y[-1, 0] > y[-1, 1]:
+                    y[-1, 1] = +180.0
+            self.cell_areas = il.CellAreas(None, None, lat_bnds=x, lon_bnds=y)
+
         # Now we do the same for land fractions
         frac_name = None
         frac_name = "landfrac" if "landfrac" in self.variables.keys() else frac_name
-        frac_name = "sftlf"    if "sftlf"    in self.variables.keys() else frac_name
+        frac_name = "sftlf" if "sftlf" in self.variables.keys() else frac_name
         if frac_name is None:
             self.land_areas = self.cell_areas
         else:
             with Dataset(self.variables[frac_name][0]) as f:
                 self.land_fraction = f.variables[frac_name][...]
             # some models represent the fraction as a percent
-            if np.ma.max(self.land_fraction) > 10: self.land_fraction *= 0.01
-            with np.errstate(over='ignore',under='ignore'):
-                if not np.allclose(self.cell_areas.shape,self.land_fraction.shape):
-                    msg = "The model %s has areacella %s which is a different shape than sftlf %s" % (self.name,
-                                                                                                      str(self.cell_areas.shape),
-                                                                                                      str(self.land_fraction.shape))
+            if np.ma.max(self.land_fraction) > 10:
+                self.land_fraction *= 0.01
+            with np.errstate(over="ignore", under="ignore"):
+                if not np.allclose(self.cell_areas.shape, self.land_fraction.shape):
+                    msg = (
+                        "The model %s has areacella %s which is a different shape than sftlf %s"
+                        % (
+                            self.name,
+                            str(self.cell_areas.shape),
+                            str(self.land_fraction.shape),
+                        )
+                    )
                     raise ValueError(msg)
-                self.land_areas = self.cell_areas*self.land_fraction
+                self.land_areas = self.cell_areas * self.land_fraction
         self.land_area = np.ma.sum(self.land_areas)
         return
 
-    def extractTimeSeries(self,variable,lats=None,lons=None,alt_vars=[],initial_time=-1e20,final_time=1e20,output_unit="",expression=None,convert_calendar=True):
+    def extractTimeSeries(
+        self,
+        variable,
+        lats=None,
+        lons=None,
+        alt_vars=[],
+        initial_time=-1e20,
+        final_time=1e20,
+        output_unit="",
+        expression=None,
+        convert_calendar=True,
+    ):
         """Extracts a time series of the given variable from the model.
 
         Parameters
         ----------
         variable : str
             name of the variable to extract
         alt_vars : list of str, optional
@@ -258,83 +330,126 @@
         -------
         var : ILAMB.Variable.Variable
             the extracted variable
 
         """
         # prepend the target variable to the list of possible variables
         altvars = list(alt_vars)
-        altvars.insert(0,variable)
+        altvars.insert(0, variable)
 
         # checks on input consistency
-        if lats is not None: assert lons is not None
-        if lons is not None: assert lats is not None
-        if lats is not None: assert lats.shape == lons.shape
+        if lats is not None:
+            assert lons is not None
+        if lons is not None:
+            assert lats is not None
+        if lats is not None:
+            assert lats.shape == lons.shape
 
         # create a list of datafiles which have a non-null intersection
         # over the desired time range
         V = []
-        tmin =  1e20
+        tmin = 1e20
         tmax = -1e20
         same_site_epsilon = 0.5
         for v in altvars:
-            if v not in self.variables: continue
-            for ifile,pathName in enumerate(self.variables[v]):
-                if _skipFile(pathName,altvars,lats,lons,same_site_epsilon): continue
-                var = Variable(filename       = pathName,
-                               variable_name  = variable,
-                               alternate_vars = altvars[1:],
-                               area           = self.land_areas,
-                               convert_calendar = convert_calendar,
-                               t0             = initial_time - self.shift,
-                               tf             = final_time   - self.shift)
-                if var.time is None: continue
-                tmin = min(tmin,var.time_bnds.min())
-                tmax = max(tmax,var.time_bnds.max())
-                if ((var.time_bnds.max() < initial_time - self.shift) or
-                    (var.time_bnds.min() >   final_time - self.shift)): continue
+            if v not in self.variables:
+                continue
+            for ifile, pathName in enumerate(self.variables[v]):
+                if _skipFile(pathName, altvars, lats, lons, same_site_epsilon):
+                    continue
+                var = Variable(
+                    filename=pathName,
+                    variable_name=variable,
+                    alternate_vars=altvars[1:],
+                    area=self.land_areas,
+                    convert_calendar=convert_calendar,
+                    t0=initial_time - self.shift,
+                    tf=final_time - self.shift,
+                )
+                # hack to avoid an issue where an unstructured grid
+                # has an area and then when ILAMB interpolates it
+                # passes areas that are the wrong shape.
+                if not np.allclose(var.area.shape, [var.lat.size, var.lon.size]):
+                    var.area = il.CellAreas(
+                        None, None, lat_bnds=var.lat_bnds, lon_bnds=var.lon_bnds
+                    )
+                if var.time is None:
+                    continue
+                tmin = min(tmin, var.time_bnds.min())
+                tmax = max(tmax, var.time_bnds.max())
+                if (var.time_bnds.max() < initial_time - self.shift) or (
+                    var.time_bnds.min() > final_time - self.shift
+                ):
+                    continue
                 if lats is not None and var.ndata:
-                    r = np.sqrt((lats[:,np.newaxis]-var.lat)**2 +
-                                (lons[:,np.newaxis]-var.lon)**2)
+                    r = np.sqrt(
+                        (lats[:, np.newaxis] - var.lat) ** 2
+                        + (lons[:, np.newaxis] - var.lon) ** 2
+                    )
                     imin = r.argmin(axis=1)
-                    rmin = r.   min(axis=1)
-                    imin = imin[np.where(rmin<same_site_epsilon)]
+                    rmin = r.min(axis=1)
+                    imin = imin[np.where(rmin < same_site_epsilon)]
                     if imin.size == 0:
                         continue
-                    var.lat   = var.lat [  imin]
-                    var.lon   = var.lon [  imin]
-                    var.data  = var.data[:,imin]
+                    var.lat = var.lat[imin]
+                    var.lon = var.lon[imin]
+                    var.data = var.data[:, imin]
                     var.ndata = var.data.shape[1]
-                if lats is not None and var.spatial: var = var.extractDatasites(lats,lons)
-                var.time      += self.shift
+                if lats is not None and var.spatial:
+                    var = var.extractDatasites(lats, lons)
+                var.time += self.shift
                 var.time_bnds += self.shift
                 V.append(var)
-            if len(V) > 0: break
+            if len(V) > 0:
+                break
 
         # If we didn't find any files, try to put together the
         # variable from a given expression
         if len(V) == 0:
             if expression is not None:
-                v = self.derivedVariable(variable,
-                                         expression,
-                                         lats         = lats,
-                                         lons         = lons,
-                                         initial_time = initial_time,
-                                         final_time   = final_time)
+                v = self.derivedVariable(
+                    variable,
+                    expression,
+                    lats=lats,
+                    lons=lons,
+                    initial_time=initial_time,
+                    final_time=final_time,
+                )
             else:
                 tstr = ""
-                if tmin < tmax: tstr = " in the given time frame, tinput = [%.1f,%.1f], tmodel = [%.1f,%.1f]" % (initial_time,final_time,tmin+self.shift,tmax+self.shift)
-                logger.debug("[%s] Could not find [%s] in the model results%s" % (self.name,",".join(altvars),tstr))
+                if tmin < tmax:
+                    tstr = (
+                        " in the given time frame, tinput = [%.1f,%.1f], tmodel = [%.1f,%.1f]"
+                        % (
+                            initial_time,
+                            final_time,
+                            tmin + self.shift,
+                            tmax + self.shift,
+                        )
+                    )
+                logger.debug(
+                    "[%s] Could not find [%s] in the model results%s"
+                    % (self.name, ",".join(altvars), tstr)
+                )
                 raise il.VarNotInModel()
         else:
             v = il.CombineVariables(V)
 
-
         return v
 
-    def derivedVariable(self,variable_name,expression,lats=None,lons=None,initial_time=-1e20,final_time=1e20,convert_calendar=True):
+    def derivedVariable(
+        self,
+        variable_name,
+        expression,
+        lats=None,
+        lons=None,
+        initial_time=-1e20,
+        final_time=1e20,
+        convert_calendar=True,
+    ):
         """Creates a variable from an algebraic expression of variables in the model results.
 
         Parameters
         ----------
         variable_name : str
             name of the variable to create
         expression : str
@@ -351,93 +466,101 @@
         Returns
         -------
         var : ILAMB.Variable.Variable
             the new variable
 
         """
         from sympy import sympify
-        if expression is None: raise il.VarNotInModel()
-        args  = {}
+
+        if expression is None:
+            raise il.VarNotInModel()
+        args = {}
         units = {}
-        unit  = expression
-        mask  = None
-        time  = None
-        tbnd  = None
-        lat   = None
-        lon   = None
+        unit = expression
+        mask = None
+        time = None
+        tbnd = None
+        lat = None
+        lon = None
         ndata = None
-        area  = None
+        area = None
         depth = None
         dbnds = None
 
         # first pass to make sure all variables are defined in the interval
         free_symbols = sympify(expression).free_symbols
-        Vs = {}; t0 = initial_time; tf = final_time
+        Vs = {}
+        t0 = initial_time
+        tf = final_time
         for arg in free_symbols:
-            Vs[arg] = self.extractTimeSeries(arg.name,
-                                             lats = lats,
-                                             lons = lons,
-                                             convert_calendar = convert_calendar,
-                                             initial_time = initial_time,
-                                             final_time   = final_time)
-            t0 = max(Vs[arg].time_bnds[ 0,0],t0)
-            tf = min(Vs[arg].time_bnds[-1,1],tf)
+            Vs[arg] = self.extractTimeSeries(
+                arg.name,
+                lats=lats,
+                lons=lons,
+                convert_calendar=convert_calendar,
+                initial_time=initial_time,
+                final_time=final_time,
+            )
+            t0 = max(Vs[arg].time_bnds[0, 0], t0)
+            tf = min(Vs[arg].time_bnds[-1, 1], tf)
 
         for arg in free_symbols:
-            var = Vs[arg].trim(t=[t0,tf])
+            var = Vs[arg].trim(t=[t0, tf])
             units[arg.name] = var.unit
-            args [arg.name] = var.data.data
+            args[arg.name] = var.data.data
 
             if mask is None:
-                mask  = var.data.mask
+                mask = var.data.mask
             else:
                 mask += var.data.mask
             if time is None:
-                time  = var.time
+                time = var.time
             else:
-                assert(np.allclose(time,var.time))
+                assert np.allclose(time, var.time)
             if tbnd is None:
-                tbnd  = var.time_bnds
+                tbnd = var.time_bnds
             else:
-                assert(np.allclose(tbnd,var.time_bnds))
+                assert np.allclose(tbnd, var.time_bnds)
             if lat is None:
-                lat  = var.lat
+                lat = var.lat
             else:
-                assert(np.allclose(lat,var.lat))
+                assert np.allclose(lat, var.lat)
             if lon is None:
-                lon  = var.lon
+                lon = var.lon
             else:
-                assert(np.allclose(lon,var.lon))
+                assert np.allclose(lon, var.lon)
             if area is None:
-                area  = var.area
+                area = var.area
             else:
-                assert(np.allclose(area,var.area))
+                assert np.allclose(area, var.area)
             if ndata is None:
-                ndata  = var.ndata
+                ndata = var.ndata
             else:
-                assert(np.allclose(ndata,var.ndata))
+                assert np.allclose(ndata, var.ndata)
             if depth is None:
-                depth  = var.depth
+                depth = var.depth
             else:
-                assert(np.allclose(depth,var.depth))
+                assert np.allclose(depth, var.depth)
             if dbnds is None:
-                dbnds  = var.depth_bnds
+                dbnds = var.depth_bnds
             else:
-                assert(np.allclose(dbnds,var.depth_bnds))
+                assert np.allclose(dbnds, var.depth_bnds)
 
-        np.seterr(divide='ignore',invalid='ignore')
-        result,unit = il.SympifyWithArgsUnits(expression,args,units)
-        np.seterr(divide='raise',invalid='raise')
-        mask  += np.isnan(result)
-        result = np.ma.masked_array(np.nan_to_num(result),mask=mask)
-
-        return Variable(data       = np.ma.masked_array(result,mask=mask),
-                        unit       = unit,
-                        name       = variable_name,
-                        time       = time,
-                        time_bnds  = tbnd,
-                        lat        = lat,
-                        lon        = lon,
-                        area       = area,
-                        ndata      = ndata,
-                        depth      = depth,
-                        depth_bnds = dbnds)
+        np.seterr(divide="ignore", invalid="ignore")
+        result, unit = il.SympifyWithArgsUnits(expression, args, units)
+        np.seterr(divide="raise", invalid="raise")
+        mask += np.isnan(result)
+        result = np.ma.masked_array(np.nan_to_num(result), mask=mask)
+
+        return Variable(
+            data=np.ma.masked_array(result, mask=mask),
+            unit=unit,
+            name=variable_name,
+            time=time,
+            time_bnds=tbnd,
+            lat=lat,
+            lon=lon,
+            area=area,
+            ndata=ndata,
+            depth=depth,
+            depth_bnds=dbnds,
+        )
```

### Comparing `ILAMB-2.6/src/ILAMB/Post.py` & `ILAMB-2.7/src/ILAMB/Post.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,53 +1,64 @@
-import pylab as plt
-import numpy as np
-from .constants import space_opts,time_opts
-from .Regions import Regions
+import glob
+import json
+import os
+import pickle
 import re
+
+import numpy as np
+import pandas as pd
+import pylab as plt
 from matplotlib.colors import LinearSegmentedColormap
-import os
 from netCDF4 import Dataset
-import pandas as pd
-import json
-import glob
-import pickle
+
+from ILAMB.constants import space_opts, time_opts
+from ILAMB.Regions import Regions
+
 
 def UseLatexPltOptions(fsize=18):
-    params = {'axes.titlesize':fsize,
-              'axes.labelsize':fsize,
-              'font.size':fsize,
-              'legend.fontsize':fsize,
-              'xtick.labelsize':fsize,
-              'ytick.labelsize':fsize}
+    params = {
+        "axes.titlesize": fsize,
+        "axes.labelsize": fsize,
+        "font.size": fsize,
+        "legend.fontsize": fsize,
+        "xtick.labelsize": fsize,
+        "ytick.labelsize": fsize,
+    }
     plt.rcParams.update(params)
 
-def UnitStringToMatplotlib(unit,add_carbon=False):
+
+def UnitStringToMatplotlib(unit, add_carbon=False):
     # replace 1e-9 with nano
-    match = re.findall("(1e-9\s)",unit)
-    for m in match: unit = unit.replace(m,"n")
+    match = re.findall("(1e-9\s)", unit)
+    for m in match:
+        unit = unit.replace(m, "n")
     # replace 1e-6 with micro
-    match = re.findall("(1e-6\s)",unit)
-    for m in match: unit = unit.replace(m,"$\mu$")
+    match = re.findall("(1e-6\s)", unit)
+    for m in match:
+        unit = unit.replace(m, "$\mu$")
     # replace rest of 1e with 10^
-    match = re.findall("1e(\-*\+*\d+)",unit)
-    for m in match: unit = unit.replace("1e%s" % m,"$10^{%s}$" % m)
+    match = re.findall("1e(\-*\+*\d+)", unit)
+    for m in match:
+        unit = unit.replace("1e%s" % m, "$10^{%s}$" % m)
     # raise exponents using Latex
     tokens = unit.split()
     for token in tokens:
         old_token = token
-        for m in re.findall("[a-zA-Z](\-*\+*\d)",token):
-            token = token.replace(m,"$^{%s}$" % m)
-        unit = unit.replace(old_token,token)
+        for m in re.findall("[a-zA-Z](\-*\+*\d)", token):
+            token = token.replace(m, "$^{%s}$" % m)
+        unit = unit.replace(old_token, token)
     # add carbon symbol to all mass units
     if add_carbon:
-        match = re.findall("(\D*g)",unit)
-        for m in match: unit = unit.replace(m,"%s C " % m)
+        match = re.findall("(\D*g)", unit)
+        for m in match:
+            unit = unit.replace(m, "%s C " % m)
     return unit
 
-def ColorBar(ax,**keywords):
+
+def ColorBar(ax, **keywords):
     """Plot a colorbar.
 
     We plot colorbars separately so they can be rendered once and used
     for multiple plots.
 
     Parameters
     ----------
@@ -59,29 +70,36 @@
         the maximum plotted value
     cmap : str, optional
         the name of the colormap to be used in plotting the spatial variable
     label : str, optional
         the text which appears with the colorbar
 
     """
-    from matplotlib import colorbar,colors
-    vmin  = keywords.get("vmin",None)
-    vmax  = keywords.get("vmax",None)
-    cmap  = keywords.get("cmap","jet")
-    ticks = keywords.get("ticks",None)
-    ticklabels = keywords.get("ticklabels",None)
-    label = keywords.get("label",None)
-    cb = colorbar.ColorbarBase(ax,cmap=plt.get_cmap(cmap),
-                               norm=colors.Normalize(vmin=vmin,vmax=vmax),
-                               orientation='horizontal')
+    from matplotlib import colorbar, colors
+
+    vmin = keywords.get("vmin", None)
+    vmax = keywords.get("vmax", None)
+    cmap = keywords.get("cmap", "jet")
+    ticks = keywords.get("ticks", None)
+    ticklabels = keywords.get("ticklabels", None)
+    label = keywords.get("label", None)
+    cb = colorbar.ColorbarBase(
+        ax,
+        cmap=plt.get_cmap(cmap),
+        norm=colors.Normalize(vmin=vmin, vmax=vmax),
+        orientation="horizontal",
+    )
     cb.set_label(label)
-    if ticks is not None: cb.set_ticks(ticks)
-    if ticklabels is not None: cb.set_ticklabels(ticklabels)
+    if ticks is not None:
+        cb.set_ticks(ticks)
+    if ticklabels is not None:
+        cb.set_ticklabels(ticklabels)
+
 
-def TaylorDiagram(stddev,corrcoef,refstd,fig,colors,normalize=True):
+def TaylorDiagram(stddev, corrcoef, refstd, fig, colors, normalize=True):
     """Plot a Taylor diagram.
 
     This is adapted from the code by Yannick Copin found here:
 
     https://gist.github.com/ycopin/3342888
 
     Parameters
@@ -96,45 +114,44 @@
         the matplotlib figure
     colors : array
         an array of colors for each element of the input arrays
     normalize : bool, optional
         disable to skip normalization of the standard deviation
 
     """
-    from matplotlib.projections import PolarAxes
     import mpl_toolkits.axisartist.floating_axes as FA
     import mpl_toolkits.axisartist.grid_finder as GF
+    from matplotlib.projections import PolarAxes
 
     # define transform
     tr = PolarAxes.PolarTransform()
 
     # correlation labels
-    rlocs = np.concatenate((np.arange(10)/10.,[0.95,0.99]))
+    rlocs = np.concatenate((np.arange(10) / 10.0, [0.95, 0.99]))
     tlocs = np.arccos(rlocs)
-    gl1   = GF.FixedLocator(tlocs)
-    tf1   = GF.DictFormatter(dict(zip(tlocs,map(str,rlocs))))
+    gl1 = GF.FixedLocator(tlocs)
+    tf1 = GF.DictFormatter(dict(zip(tlocs, map(str, rlocs))))
 
     # standard deviation axis extent
     if normalize:
-        stddev = stddev/refstd
-        refstd = 1.
+        stddev = stddev / refstd
+        refstd = 1.0
     smin = 0
-    smax = max(2.0,1.1*stddev.max())
+    smax = max(2.0, 1.1 * stddev.max())
 
     # add the curvilinear grid
-    ghelper = FA.GridHelperCurveLinear(tr,
-                                       extremes=(0,np.pi/2,smin,smax),
-                                       grid_locator1=gl1,
-                                       tick_formatter1=tf1)
+    ghelper = FA.GridHelperCurveLinear(
+        tr, extremes=(0, np.pi / 2, smin, smax), grid_locator1=gl1, tick_formatter1=tf1
+    )
     ax = FA.FloatingSubplot(fig, 111, grid_helper=ghelper)
     fig.add_subplot(ax)
 
     # adjust axes
     ax.axis["top"].set_axis_direction("bottom")
-    ax.axis["top"].toggle(ticklabels=True,label=True)
+    ax.axis["top"].toggle(ticklabels=True, label=True)
     ax.axis["top"].major_ticklabels.set_axis_direction("top")
     ax.axis["top"].label.set_axis_direction("top")
     ax.axis["top"].label.set_text("Correlation")
     ax.axis["left"].set_axis_direction("bottom")
     if normalize:
         ax.axis["left"].label.set_text("Normalized standard deviation")
     else:
@@ -143,293 +160,425 @@
     ax.axis["right"].toggle(ticklabels=True)
     ax.axis["right"].major_ticklabels.set_axis_direction("left")
     ax.axis["bottom"].set_visible(False)
     ax.grid(True)
 
     ax = ax.get_aux_axes(tr)
     # Plot data
-    corrcoef = corrcoef.clip(-1,1)
+    corrcoef = corrcoef.clip(-1, 1)
     for i in range(len(corrcoef)):
-        ax.plot(np.arccos(corrcoef[i]),stddev[i],'o',color=colors[i],mew=0,ms=8)
+        ax.plot(np.arccos(corrcoef[i]), stddev[i], "o", color=colors[i], mew=0, ms=8)
 
     # Add reference point and stddev contour
-    l, = ax.plot([0],refstd,'k*',ms=12,mew=0)
-    t = np.linspace(0, np.pi/2)
+    (l,) = ax.plot([0], refstd, "k*", ms=12, mew=0)
+    t = np.linspace(0, np.pi / 2)
     r = np.zeros_like(t) + refstd
-    ax.plot(t,r, 'k--')
+    ax.plot(t, r, "k--")
 
     # centralized rms contours
-    rs,ts = np.meshgrid(np.linspace(smin,smax),
-                        np.linspace(0,np.pi/2))
-    rms = np.sqrt(refstd**2 + rs**2 - 2*refstd*rs*np.cos(ts))
-    contours = ax.contour(ts,rs,rms,5,colors='k',alpha=0.4)
-    ax.clabel(contours,fmt='%1.1f')
+    rs, ts = np.meshgrid(np.linspace(smin, smax), np.linspace(0, np.pi / 2))
+    rms = np.sqrt(refstd**2 + rs**2 - 2 * refstd * rs * np.cos(ts))
+    contours = ax.contour(ts, rs, rms, 5, colors="k", alpha=0.4)
+    ax.clabel(contours, fmt="%1.1f")
 
     return ax
 
-class HtmlFigure():
-
-    def __init__(self,name,pattern,side=None,legend=False,benchmark=False,longname=None,width=None,br=False):
 
-        self.name      = name
-        self.pattern   = pattern
-        self.side      = side
-        self.legend    = legend
+class HtmlFigure:
+    def __init__(
+        self,
+        name,
+        pattern,
+        side=None,
+        legend=False,
+        benchmark=False,
+        longname=None,
+        width=None,
+        br=False,
+    ):
+        self.name = name
+        self.pattern = pattern
+        self.side = side
+        self.legend = legend
         self.benchmark = benchmark
-        self.longname  = longname
-        self.width     = width
-        self.br        = br
+        self.longname = longname
+        self.width = width
+        self.br = br
 
-    def generateClickRow(self,allModels=False):
+    def generateClickRow(self, allModels=False):
         name = self.pattern
-        if allModels: name = name.replace(self.name,"PNAME")
-        for token in ['CNAME','MNAME','RNAME','PNAME']:
+        if allModels:
+            name = name.replace(self.name, "PNAME")
+        for token in ["CNAME", "MNAME", "RNAME", "PNAME"]:
             name = name.split(token)
             name = ("' + %s + '" % token).join(name)
         name = "'%s'" % name
-        name = name.replace("'' + ","")
+        name = name.replace("'' + ", "")
         code = """
-          document.getElementById('%s').src =  %s""" % (self.name,name)
+          document.getElementById('%s').src =  %s""" % (
+            self.name,
+            name,
+        )
         if self.benchmark:
-            name = self.pattern.replace('MNAME','Benchmark')
-            for token in ['CNAME','MNAME','RNAME']:
+            name = self.pattern.replace("MNAME", "Benchmark")
+            for token in ["CNAME", "MNAME", "RNAME"]:
                 name = name.split(token)
                 name = ("' + %s + '" % token).join(name)
             name = "'%s'" % name
-            name = name.replace("'' + ","")
+            name = name.replace("'' + ", "")
             code += """
-          document.getElementById('benchmark_%s').src =  %s""" % (self.name,name)
+          document.getElementById('benchmark_%s').src =  %s""" % (
+                self.name,
+                name,
+            )
         return code
 
     def __str__(self):
-
         opts = "width = %d" % self.width if self.width else ""
-        cls  = "break" if self.br else "container"
+        cls = "break" if self.br else "container"
         code = """
         <div class="%s" id="%s_div">
-          <div class="child">""" % (cls,self.name)
+          <div class="child">""" % (
+            cls,
+            self.name,
+        )
         if self.side is not None:
             code += """
-          <center>%s</center>""" % (self.side.replace(" ","&nbsp;"))
-        code += """
-          <img src="" id="%s" alt="Data not available" %s></img>""" % (self.name,opts)
+          <center>%s</center>""" % (
+                self.side.replace(" ", "&nbsp;")
+            )
+        code += """
+          <img src="" id="%s" alt="Data not available" %s></img>""" % (
+            self.name,
+            opts,
+        )
         if self.legend:
             code += """
-          <center><img src="legend_%s.png" id="leg"  alt="Data not available" %s></img></center>""" % (self.name.replace("benchmark_",""),opts)
+          <center><img src="legend_%s.png" id="leg"  alt="Data not available" %s></img></center>""" % (
+                self.name.replace("benchmark_", ""),
+                opts,
+            )
         code += """
           </div>
         </div>"""
         return code
 
+
 def SortRegions(regions):
-    if len(regions) == 0: return []
+    if len(regions) == 0:
+        return []
     rnames = []
     r = Regions()
     for region in regions:
         try:
             n = r.getRegionName(region)
             rnames.append(n)
         except:
             rnames.append(region)
-    sorts = sorted(zip(rnames,regions))
-    rnames,regions = [list(t) for t in zip(*sorts)]
+    sorts = sorted(zip(rnames, regions))
+    rnames, regions = [list(t) for t in zip(*sorts)]
     return regions
-    
-class HtmlPage(object):
 
-    def __init__(self,name,title):
-        self.name  = name
+
+class HtmlPage(object):
+    def __init__(self, name, title):
+        self.name = name
         self.title = title
         self.cname = ""
         self.pages = []
         self.metric_dict = None
-        self.models      = None
-        self.regions     = None
-        self.metrics     = None
-        self.units       = None
-        self.priority    = ["original","Model","intersection","Benchmark","complement","Bias","RMSE","Phase","Seasonal","Spatial","Interannual","Score","Overall"]
-        self.header      = "CNAME"
-        self.sections    = []
-        self.figures     = {}
-        self.text        = None
-        self.inserts     = []
+        self.models = None
+        self.regions = None
+        self.metrics = None
+        self.units = None
+        self.priority = [
+            "original",
+            "Model",
+            "intersection",
+            "Benchmark",
+            "complement",
+            "Bias",
+            "RMSE",
+            "Phase",
+            "Seasonal",
+            "Spatial",
+            "Interannual",
+            "Score",
+            "Overall",
+        ]
+        self.header = "CNAME"
+        self.sections = []
+        self.figures = {}
+        self.text = None
+        self.inserts = []
 
     def __str__(self):
-
         r = Regions()
+
         def _sortFigures(figure):
-            macro = ["timeint","timelonint","bias","rmse","iav","phase","shift","variance","spaceint","accumulate","cycle"]
-            val = 1.
-            for i,m in enumerate(macro):
-                if m in figure.name: val += 3**i
-            if figure.name.startswith("benchmark"): val -= 1.
-            if figure.name.endswith("score"): val += 1.
+            macro = [
+                "timeint",
+                "timelonint",
+                "bias",
+                "rmse",
+                "iav",
+                "phase",
+                "shift",
+                "variance",
+                "spaceint",
+                "accumulate",
+                "cycle",
+            ]
+            val = 1.0
+            for i, m in enumerate(macro):
+                if m in figure.name:
+                    val += 3**i
+            if figure.name.startswith("benchmark"):
+                val -= 1.0
+            if figure.name.endswith("score"):
+                val += 1.0
             if figure.name.startswith("legend"):
                 if "variance" in figure.name:
-                    val += 1.
+                    val += 1.0
                 else:
-                    val  = 0.
+                    val = 0.0
             return val
 
         code = """
     <div data-role="page" id="%s">
       <div data-role="header" data-position="fixed" data-tap-toggle="false">
-        <h1 id="%sHead">%s</h1>""" % (self.name,self.name,self.title)
+        <h1 id="%sHead">%s</h1>""" % (
+            self.name,
+            self.name,
+            self.title,
+        )
         if self.pages:
             code += """
         <div data-role="navbar">
           <ul>"""
             for page in self.pages:
                 opts = ""
-                if page == self: opts = " class=ui-btn-active ui-state-persist"
+                if page == self:
+                    opts = " class=ui-btn-active ui-state-persist"
                 code += """
-            <li><a href='#%s'%s>%s</a></li>""" % (page.name,opts,page.title)
+            <li><a href='#%s'%s>%s</a></li>""" % (
+                    page.name,
+                    opts,
+                    page.title,
+                )
             code += """
           </ul>"""
         code += """
         </div>
       </div>"""
 
         if self.regions:
             code += """
-      <select id="%sRegion" onchange="changeRegion%s()">""" % (self.name,self.name)
+      <select id="%sRegion" onchange="changeRegion%s()">""" % (
+                self.name,
+                self.name,
+            )
             for region in self.regions:
                 try:
                     rname = r.getRegionName(region)
                 except:
                     rname = region
-                opts  = ''
+                opts = ""
                 if region == "global" or len(self.regions) == 1:
-                    opts  = ' selected="selected"'
+                    opts = ' selected="selected"'
                 code += """
-        <option value='%s'%s>%s</option>""" % (region,opts,rname)
+        <option value='%s'%s>%s</option>""" % (
+                    region,
+                    opts,
+                    rname,
+                )
             code += """
       </select>"""
 
         if self.models:
             code += """
       <div style="display:none">
-      <select id="%sModel">""" % (self.name)
-            for i,model in enumerate(self.models):
-                opts  = ' selected="selected"' if i == 1 else ''
+      <select id="%sModel">""" % (
+                self.name
+            )
+            for i, model in enumerate(self.models):
+                opts = ' selected="selected"' if i == 1 else ""
                 code += """
-        <option value='%s'%s>%s</option>""" % (model,opts,model)
+        <option value='%s'%s>%s</option>""" % (
+                    model,
+                    opts,
+                    model,
+                )
             code += """
       </select>
       </div>"""
 
-        if self.metric_dict: code += self.metricsToHtmlTables()
+        if self.metric_dict:
+            code += self.metricsToHtmlTables()
 
         if self.text is not None:
-            code += """
-      %s""" % self.text
+            code += (
+                """
+      %s"""
+                % self.text
+            )
 
         for section in self.sections:
-            if len(self.figures[section]) == 0: continue
+            if len(self.figures[section]) == 0:
+                continue
             self.figures[section].sort(key=_sortFigures)
-            code += """
-        <div data-role="collapsible" data-collapsed="false"><h1>%s</h1>""" % section
+            code += (
+                """
+        <div data-role="collapsible" data-collapsed="false"><h1>%s</h1>"""
+                % section
+            )
             for figure in self.figures[section]:
-                if figure.name == "spatial_variance": code += "<br>"
+                if figure.name == "spatial_variance":
+                    code += "<br>"
                 code += "%s" % (figure)
             code += """
         </div>"""
 
         code += """
     </div>"""
         return code
 
-    def setHeader(self,header):
+    def setHeader(self, header):
         self.header = header
 
-    def setSections(self,sections):
+    def setSections(self, sections):
         assert type(sections) == type([])
         self.sections = sections
-        for section in sections: self.figures[section] = []
+        for section in sections:
+            self.figures[section] = []
 
-    def addFigure(self,section,name,pattern,side=None,legend=False,benchmark=False,longname=None,width=None,br=False):
+    def addFigure(
+        self,
+        section,
+        name,
+        pattern,
+        side=None,
+        legend=False,
+        benchmark=False,
+        longname=None,
+        width=None,
+        br=False,
+    ):
         assert section in self.sections
         for fig in self.figures[section]:
-            if fig.name == name: return
-        self.figures[section].append(HtmlFigure(name,pattern,side=side,legend=legend,benchmark=benchmark,longname=longname,width=width,br=br))
+            if fig.name == name:
+                return
+        self.figures[section].append(
+            HtmlFigure(
+                name,
+                pattern,
+                side=side,
+                legend=legend,
+                benchmark=benchmark,
+                longname=longname,
+                width=width,
+                br=br,
+            )
+        )
 
-    def setMetricPriority(self,priority):
+    def setMetricPriority(self, priority):
         self.priority = priority
 
     def metricsToHtmlTables(self):
-        if not self.metric_dict: return ""
+        if not self.metric_dict:
+            return ""
         regions = self.regions
         metrics = self.metrics
-        units   = self.units
-        cname   = self.cname.split(" / ")
+        units = self.units
+        cname = self.cname.split(" / ")
         if len(cname) == 3:
             cname = cname[1].strip()
         else:
             cname = cname[-1].strip()
-        html    = ""
+        html = ""
         inserts = self.inserts
         j0 = 0 if "Benchmark" in self.models else -1
-        score_sig = 3 # number of significant digits used in the score tables
-        other_sig = 3 # number of significant digits used for non-score quantities
+        score_sig = 3  # number of significant digits used in the score tables
+        other_sig = 3  # number of significant digits used for non-score quantities
         for region in regions:
             html += """
         <center>
         <table class="table-header-rotated" id="%s_table_%s">
            <thead>
              <tr>
                <th></th>
-               <th class="rotate"><div><span>Download Data</span></div></th>""" % (self.name,region)
-            for i,metric in enumerate(metrics):
-                if i in inserts: html += """
+               <th class="rotate"><div><span>Download Data</span></div></th>""" % (
+                self.name,
+                region,
+            )
+            for i, metric in enumerate(metrics):
+                if i in inserts:
+                    html += """
                <th></th>"""
                 html += """
-               <th class="rotate"><div><span>%s [%s]</span></div></th>""" % (metric,units[metric])
+               <th class="rotate"><div><span>%s [%s]</span></div></th>""" % (
+                    metric,
+                    units[metric],
+                )
             html += """
              </tr>
            </thead>
            <tbody>"""
 
-            for j,model in enumerate(self.models):
-                opts = ' onclick="highlightRow%s(this)"' % (self.name) if j > j0 else ''
+            for j, model in enumerate(self.models):
+                opts = ' onclick="highlightRow%s(this)"' % (self.name) if j > j0 else ""
                 html += """
              <tr>
                <td%s class="row-header">%s</td>
-               <td%s><a href="%s_%s.nc" download>[-]</a></td>""" % (opts,model,opts,cname,model)
-                for i,metric in enumerate(metrics):
+               <td%s><a href="%s_%s.nc" download>[-]</a></td>""" % (
+                    opts,
+                    model,
+                    opts,
+                    cname,
+                    model,
+                )
+                for i, metric in enumerate(metrics):
                     sig = score_sig if "score" in metric.lower() else other_sig
-                    if i in inserts: html += """
-               <td%s class="divider"></td>""" % (opts)
+                    if i in inserts:
+                        html += """
+               <td%s class="divider"></td>""" % (
+                            opts
+                        )
                     add = ""
                     try:
                         tmp = self.metric_dict[model][region][metric].data
                         if tmp.mask.all():
                             add = ""
                         else:
                             add = ("%#." + "%d" % sig + "g") % tmp
-                            add = add.replace("nan","")
+                            add = add.replace("nan", "")
                     except:
                         pass
                     html += """
-               <td%s>%s</td>""" % (opts,add)
+               <td%s>%s</td>""" % (
+                        opts,
+                        add,
+                    )
                 html += """
              </tr>"""
             html += """
           </tbody>
         </table>
         </center>"""
 
         return html
 
     def googleScript(self):
-        if not self.metric_dict: return ""
-        models   = self.models
-        regions  = self.regions
-        metrics  = self.metrics
-        units    = self.units
-        cname    = self.cname.split(" / ")
+        if not self.metric_dict:
+            return ""
+        models = self.models
+        regions = self.regions
+        metrics = self.metrics
+        units = self.units
+        cname = self.cname.split(" / ")
         if len(cname) == 3:
             cname = cname[1].strip()
         else:
             cname = cname[-1].strip()
         rows = ""
         for section in self.sections:
             for figure in self.figures[section]:
@@ -445,19 +594,28 @@
             var RNAME = rsel.options[rid].value;
             var MNAME = msel.options[mid].value;
             var CNAME = "%s";
             var head  = "%s";
             head      = head.replace("CNAME",CNAME).replace("RNAME",RNAME).replace("MNAME",MNAME);
             $("#%sHead").text(head);
             %s
-        }""" % (self.name,self.name,self.name,self.cname,self.header,self.name,rows)
+        }""" % (
+            self.name,
+            self.name,
+            self.name,
+            self.cname,
+            self.header,
+            self.name,
+            rows,
+        )
 
         nscores = len(metrics)
-        if len(self.inserts) > 0: nscores -= self.inserts[-1]
-        r0      = 2 if "Benchmark" in models else 1
+        if len(self.inserts) > 0:
+            nscores -= self.inserts[-1]
+        r0 = 2 if "Benchmark" in models else 1
 
         head += """
 
         function highlightRow%s(cell) {
             var select = document.getElementById("%sRegion");
             for (var i = 0; i < select.length; i++){
                 var table = document.getElementById("%s_table_" + select.options[i].value);
@@ -470,15 +628,24 @@
                 var r = cell.closest("tr").rowIndex;
                 document.getElementById("%sModel").selectedIndex = r-1;
                 for (var c = 0; c < rows[r].cells.length-%d; c++) {
                     rows[r].cells[c].style.backgroundColor = "#c1c1c1";
                 }
             }
             updateImagesAndHeaders%s();
-        }""" % (self.name,self.name,self.name,r0,nscores+1,self.name,nscores+1,self.name)
+        }""" % (
+            self.name,
+            self.name,
+            self.name,
+            r0,
+            nscores + 1,
+            self.name,
+            nscores + 1,
+            self.name,
+        )
 
         head += """
 
         function paintScoreCells%s(RNAME) {
 
 	    var PuOr = ['#b35806','#e08214','#fdb863','#fee0b6','#f7f7f7','#d8daeb','#b2abd2','#8073ac','#542788'];
 	    var GnRd = ['#b2182b','#d6604d','#f4a582','#fddbc7','#f7f7f7','#d9f0d3','#a6dba0','#5aae61','#1b7837'];
@@ -510,15 +677,27 @@
                     }else{
 		      clr = math.round(4*e+4);
                     }
                     clr = math.min(math.max(0,clr),8);
                     rows[r].cells[c].style.backgroundColor = colors[clr];
                 }
             }
-        }""" % (self.name,self.name,nscores,r0,r0,r0,r0,r0,r0,r0,r0)
+        }""" % (
+            self.name,
+            self.name,
+            nscores,
+            r0,
+            r0,
+            r0,
+            r0,
+            r0,
+            r0,
+            r0,
+            r0,
+        )
 
         head += """
 
         function pageLoad%s() {
             var select = document.getElementById("%sRegion");
             var region = getQueryVariable("region");
             var model  = getQueryVariable("model");
@@ -549,306 +728,418 @@
                 if (i == select.selectedIndex) {
                     document.getElementById("%s_table_" + RNAME).style.display = "table";
                 }else{
                     document.getElementById("%s_table_" + RNAME).style.display = "none";
                 }
             }
             updateImagesAndHeaders%s();
-        }""" % (self.name,self.name,self.name,self.name,self.name,r0,self.name,self.name,self.name,self.name,self.name,self.name,self.name)
+        }""" % (
+            self.name,
+            self.name,
+            self.name,
+            self.name,
+            self.name,
+            r0,
+            self.name,
+            self.name,
+            self.name,
+            self.name,
+            self.name,
+            self.name,
+            self.name,
+        )
 
-        return head,"pageLoad%s" % self.name,""
+        return head, "pageLoad%s" % self.name, ""
 
-    def setRegions(self,regions):
+    def setRegions(self, regions):
         assert type(regions) == type([])
         self.regions = SortRegions(regions)
 
-    def setMetrics(self,metric_dict):
-
+    def setMetrics(self, metric_dict):
         # Sorting function
-        def _sortMetrics(name,priority=self.priority):
-            val = 1.
-            for i,pname in enumerate(priority):
-                if pname in name: val += 2**i
+        def _sortMetrics(name, priority=self.priority):
+            val = 1.0
+            for i, pname in enumerate(priority):
+                if pname in name:
+                    val += 2**i
             return val
 
         assert type(metric_dict) == type({})
         self.metric_dict = metric_dict
 
         # Build and sort models, regions, and metrics
-        models  = list(self.metric_dict.keys())
+        models = list(self.metric_dict.keys())
         regions = []
         metrics = []
-        units   = {}
+        units = {}
         for model in models:
             for region in self.metric_dict[model].keys():
-                if region not in regions: regions.append(region)
+                if region not in regions:
+                    regions.append(region)
                 for metric in self.metric_dict[model][region].keys():
                     units[metric] = self.metric_dict[model][region][metric].unit
-                    if metric not in metrics: metrics.append(metric)
+                    if metric not in metrics:
+                        metrics.append(metric)
         models.sort(key=lambda key: key.lower())
-        if "Benchmark" in models: models.insert(0,models.pop(models.index("Benchmark")))
+        if "Benchmark" in models:
+            models.insert(0, models.pop(models.index("Benchmark")))
         regions = SortRegions(regions)
         metrics.sort(key=_sortMetrics)
-        self.models  = models
-        if self.regions is None: self.regions = regions
+        self.models = models
+        if self.regions is None:
+            self.regions = regions
         self.metrics = metrics
-        self.units   = units
+        self.units = units
 
         tmp = [("bias" in m.lower()) for m in metrics]
-        if tmp.count(True) > 0: self.inserts.append(tmp.index(True))
+        if tmp.count(True) > 0:
+            self.inserts.append(tmp.index(True))
         tmp = [("score" in m.lower()) for m in metrics]
-        if tmp.count(True) > 0: self.inserts.append(tmp.index(True))
+        if tmp.count(True) > 0:
+            self.inserts.append(tmp.index(True))
 
     def head(self):
         return ""
 
-class HtmlAllModelsPage(HtmlPage):
 
-    def __init__(self,name,title):
-
-        super(HtmlAllModelsPage,self).__init__(name,title)
-        self.plots    = None
-        self.nobench  = None
+class HtmlAllModelsPage(HtmlPage):
+    def __init__(self, name, title):
+        super(HtmlAllModelsPage, self).__init__(name, title)
+        self.plots = None
+        self.nobench = None
         self.nolegend = []
 
     def _populatePlots(self):
-
-        self.plots   = []
-        bench        = []
+        self.plots = []
+        bench = []
         for page in self.pages:
             if page.sections is not None:
                 for section in page.sections:
-                    if len(page.figures[section]) == 0: continue
+                    if len(page.figures[section]) == 0:
+                        continue
                     for figure in page.figures[section]:
-                        if (figure.name in ["spatial_variance","compcycle","profile",
-                                            "legend_spatial_variance","legend_compcycle"]): continue # ignores
+                        if figure.name in [
+                            "spatial_variance",
+                            "compcycle",
+                            "profile",
+                            "site",
+                            "legend_spatial_variance",
+                            "legend_compcycle",
+                        ]:
+                            continue  # ignores
                         if "benchmark" in figure.name:
-                            if figure.name not in bench: bench.append(figure.name)
+                            if figure.name not in bench:
+                                bench.append(figure.name)
                             continue
-                        if figure not in self.plots: self.plots.append(figure)
-                        if not figure.legend: self.nolegend.append(figure.name)
-        self.nobench = [plot.name for plot in self.plots if "benchmark_%s" % (plot.name) not in bench]
-        
-    def __str__(self):
+                        if figure not in self.plots:
+                            self.plots.append(figure)
+                        if not figure.legend:
+                            self.nolegend.append(figure.name)
+        self.nobench = [
+            plot.name
+            for plot in self.plots
+            if "benchmark_%s" % (plot.name) not in bench
+        ]
 
-        if self.plots is None: self._populatePlots()
+    def __str__(self):
+        if self.plots is None:
+            self._populatePlots()
         r = Regions()
 
         code = """
     <div data-role="page" id="%s">
       <div data-role="header" data-position="fixed" data-tap-toggle="false">
-        <h1 id="%sHead">%s</h1>""" % (self.name,self.name,self.title)
+        <h1 id="%sHead">%s</h1>""" % (
+            self.name,
+            self.name,
+            self.title,
+        )
         if self.pages:
             code += """
         <div data-role="navbar">
           <ul>"""
             for page in self.pages:
                 opts = ""
-                if page == self: opts = " class=ui-btn-active ui-state-persist"
+                if page == self:
+                    opts = " class=ui-btn-active ui-state-persist"
                 code += """
-            <li><a href='#%s'%s>%s</a></li>""" % (page.name,opts,page.title)
+            <li><a href='#%s'%s>%s</a></li>""" % (
+                    page.name,
+                    opts,
+                    page.title,
+                )
             code += """
           </ul>"""
         code += """
         </div>
       </div>"""
 
         if self.regions:
             code += """
-      <select id="%sRegion" onchange="AllSelect()">""" % (self.name)
+      <select id="%sRegion" onchange="AllSelect()">""" % (
+                self.name
+            )
             for region in self.regions:
                 try:
                     rname = r.getRegionName(region)
                 except:
                     rname = region
-                opts  = ''
+                opts = ""
                 if region == "global" or len(self.regions) == 1:
-                    opts  = ' selected="selected"'
+                    opts = ' selected="selected"'
                 code += """
-        <option value='%s'%s>%s</option>""" % (region,opts,rname)
+        <option value='%s'%s>%s</option>""" % (
+                    region,
+                    opts,
+                    rname,
+                )
             code += """
       </select>"""
 
         if self.plots:
             code += """
-      <select id="%sPlot" onchange="AllSelect()">""" % (self.name)
+      <select id="%sPlot" onchange="AllSelect()">""" % (
+                self.name
+            )
             for plot in self.plots:
-                name  = ''
+                name = ""
                 if plot.name in space_opts:
                     name = space_opts[plot.name]["name"]
                 elif plot.name in time_opts:
                     name = time_opts[plot.name]["name"]
                 elif plot.longname is not None:
                     name = plot.longname
-                if "rel_" in plot.name: name = plot.name.replace("rel_","Relationship with ")
-                if name == "": continue
-                opts  = ''
+                if "rel_" in plot.name:
+                    name = plot.name.replace("rel_", "Relationship with ")
+                if name == "":
+                    continue
+                opts = ""
                 if plot.name == "timeint" or len(self.plots) == 1:
-                    opts  = ' selected="selected"'
+                    opts = ' selected="selected"'
                 code += """
-        <option value='%s'%s>%s</option>""" % (plot.name,opts,name)
+        <option value='%s'%s>%s</option>""" % (
+                    plot.name,
+                    opts,
+                    name,
+                )
             code += """
       </select>"""
 
-            fig        = self.plots[0]
-            rem_side   = fig.side
-            fig.side   = "MNAME"
-            rem_leg    = fig.legend
+            fig = self.plots[0]
+            rem_side = fig.side
+            fig.side = "MNAME"
+            rem_leg = fig.legend
             fig.legend = True
-            img        = "%s" % (fig)
-            img        = img.replace('"leg"','"MNAME_legend"').replace("%s" % fig.name,"MNAME")
-            fig.side   = rem_side
+            img = "%s" % (fig)
+            img = img.replace('"leg"', '"MNAME_legend"').replace(
+                "%s" % fig.name, "MNAME"
+            )
+            fig.side = rem_side
             fig.legend = rem_leg
             if "Benchmark" not in self.pages[0].models:
                 code += '<div id="Benchmark_div"></div>'
             for model in self.pages[0].models:
-                code += img.replace("MNAME",model)
+                code += img.replace("MNAME", model)
 
         if self.text is not None:
-            code += """
-      %s""" % self.text
+            code += (
+                """
+      %s"""
+                % self.text
+            )
 
         code += """
     </div>"""
         return code
 
     def googleScript(self):
         head = self.head()
-        return head,"",""
+        return head, "", ""
 
     def head(self):
+        if self.plots is None:
+            self._populatePlots()
 
-        if self.plots is None: self._populatePlots()
-
-        models  = self.pages[0].models
+        models = self.pages[0].models
         regions = self.regions
         try:
             regions.sort()
         except:
             pass
-        head    = """
+        head = """
       function AllSelect() {
         var header = "%s";
         var CNAME  = "%s";
         header     = header.replace("CNAME",CNAME);
         var rid    = document.getElementById("%s").selectedIndex;
         var RNAME  = document.getElementById("%s").options[rid].value;
         var pid    = document.getElementById("%s").selectedIndex;
         var PNAME  = document.getElementById("%s").options[pid].value;
         header     = header.replace("RNAME",RNAME);
-        $("#%sHead").text(header);""" % (self.header,self.cname,self.name+"Region",self.name+"Region",self.name+"Plot",self.name+"Plot",self.name)
-        cond  = " || ".join(['PNAME == "%s"' % n for n in self.nobench])
-        if cond == "": cond = "0"
+        $("#%sHead").text(header);""" % (
+            self.header,
+            self.cname,
+            self.name + "Region",
+            self.name + "Region",
+            self.name + "Plot",
+            self.name + "Plot",
+            self.name,
+        )
+        cond = " || ".join(['PNAME == "%s"' % n for n in self.nobench])
+        if cond == "":
+            cond = "0"
         head += """
         if(%s){
           document.getElementById("Benchmark_div").style.display = 'none';
         }else{
           document.getElementById("Benchmark_div").style.display = 'inline';
-        }""" % (cond)
-
-        cond  = " || ".join(['PNAME == "%s"' % n for n in self.nolegend])
-        if cond == "": cond = "0"
-        head += """
-        if(%s){""" % cond
+        }""" % (
+            cond
+        )
+
+        cond = " || ".join(['PNAME == "%s"' % n for n in self.nolegend])
+        if cond == "":
+            cond = "0"
+        head += (
+            """
+        if(%s){"""
+            % cond
+        )
         for model in models:
-            head += """
-          document.getElementById("%s_legend").style.display = 'none';""" % model
+            head += (
+                """
+          document.getElementById("%s_legend").style.display = 'none';"""
+                % model
+            )
         head += """
         }else{"""
         for model in models:
-            head += """
-          document.getElementById("%s_legend").style.display = 'inline';""" % model
+            head += (
+                """
+          document.getElementById("%s_legend").style.display = 'inline';"""
+                % model
+            )
         head += """
         }"""
         for model in models:
             head += """
         document.getElementById('%s').src = '%s_' + RNAME + '_' + PNAME + '.png';
-        document.getElementById('%s_legend').src = 'legend_' + PNAME + '.png';""" % (model,model,model)
+        document.getElementById('%s_legend').src = 'legend_' + PNAME + '.png';""" % (
+                model,
+                model,
+                model,
+            )
         head += """
       }
 
       $(document).on('pageshow', '[data-role="page"]', function(){
         AllSelect()
       });"""
         return head
 
-class HtmlSitePlotsPage(HtmlPage):
-
-    def __init__(self,name,title):
 
-        super(HtmlSitePlotsPage,self).__init__(name,title)
+class HtmlSitePlotsPage(HtmlPage):
+    def __init__(self, name, title):
+        super(HtmlSitePlotsPage, self).__init__(name, title)
 
     def __str__(self):
-
         # setup page navigation
         code = """
     <div data-role="page" id="%s">
       <div data-role="header" data-position="fixed" data-tap-toggle="false">
-        <h1 id="%sHead">%s</h1>""" % (self.name,self.name,self.title)
+        <h1 id="%sHead">%s</h1>""" % (
+            self.name,
+            self.name,
+            self.title,
+        )
         if self.pages:
             code += """
         <div data-role="navbar">
           <ul>"""
             for page in self.pages:
                 opts = ""
-                if page == self: opts = " class=ui-btn-active ui-state-persist"
+                if page == self:
+                    opts = " class=ui-btn-active ui-state-persist"
                 code += """
-            <li><a href='#%s'%s>%s</a></li>""" % (page.name,opts,page.title)
+            <li><a href='#%s'%s>%s</a></li>""" % (
+                    page.name,
+                    opts,
+                    page.title,
+                )
             code += """
           </ul>"""
         code += """
         </div>
       </div>"""
 
         code += """
-      <select id="%sModel" onchange="%sMap()">""" % (self.name,self.name)
+      <select id="%sModel" onchange="%sMap()">""" % (
+            self.name,
+            self.name,
+        )
         for model in self.models:
             code += """
-        <option value='%s'>%s</option>""" % (model,model)
+        <option value='%s'>%s</option>""" % (
+                model,
+                model,
+            )
         code += """
       </select>"""
 
         code += """
-      <select id="%sSite" onchange="%sMap()">""" % (self.name,self.name)
+      <select id="%sSite" onchange="%sMap()">""" % (
+            self.name,
+            self.name,
+        )
         for site in self.sites:
             code += """
-        <option value='%s'>%s</option>""" % (site,site)
+        <option value='%s'>%s</option>""" % (
+                site,
+                site,
+            )
         code += """
       </select>"""
 
         code += """
       <center>
         <div id='map_canvas'></div>
         <div><img src="" id="time" alt="Data not available"></img></div>
       </center>"""
 
         code += """
     </div>"""
 
         return code
 
-    def setMetrics(self,metric_dict):
+    def setMetrics(self, metric_dict):
         self.models.sort()
 
     def googleScript(self):
-
         callback = "%sMap()" % (self.name)
         head = """
       function %sMap() {
         var sitedata = google.visualization.arrayToDataTable(
-          [['Latitude', 'Longitude', '%s [%s]'],\n""" % (self.name,self.vname,self.unit)
+          [['Latitude', 'Longitude', '%s [%s]'],\n""" % (
+            self.name,
+            self.vname,
+            self.unit,
+        )
 
-        for lat,lon,val in zip(self.lat,self.lon,self.vals):
+        for lat, lon, val in zip(self.lat, self.lon, self.vals):
             if val is np.ma.masked:
                 sval = "null"
             else:
                 sval = "%.2f" % val
-            head += "           [%.3f,%.3f,%s],\n" % (lat,lon,sval)
+            head += "           [%.3f,%.3f,%s],\n" % (lat, lon, sval)
         head = head[:-2] + "]);\n"
-        head += ("        var names = %s;" % (self.sites)).replace("u'","'").replace(", '",",'")
+        head += (
+            ("        var names = %s;" % (self.sites))
+            .replace("u'", "'")
+            .replace(", '", ",'")
+        )
         head += """
         var options = {
           dataMode: 'markers',
           magnifyingGlass: {enable: true, zoomFactor: 3.},
         };
         var container = document.getElementById('map_canvas');
         var geomap    = new google.visualization.GeoChart(container);
@@ -867,27 +1158,31 @@
             site.selectmenu('refresh');
           }
           updateMap();
         }
         google.visualization.events.addListener(geomap,'select',clickMap);
         geomap.draw(sitedata, options);
         updateMap();
-      };""" % (self.name,self.name,self.name,self.name)
+      };""" % (
+            self.name,
+            self.name,
+            self.name,
+            self.name,
+        )
 
-        return head,callback,"geomap"
+        return head, callback, "geomap"
 
     def head(self):
         return ""
 
-class HtmlLayout():
-
-    def __init__(self,pages,cname,years=None):
 
+class HtmlLayout:
+    def __init__(self, pages, cname, years=None):
         self.pages = pages
-        self.cname = cname.replace("/"," / ")
+        self.cname = cname.replace("/", " / ")
         if years is not None:
             try:
                 self.cname += " / %d-%d" % (years)
             except:
                 pass
         for page in self.pages:
             page.pages = self.pages
@@ -913,41 +1208,47 @@
             }
             return(false);
         }
     </script>"""
 
         functions = []
         callbacks = []
-        packages  = []
+        packages = []
         for page in self.pages:
             out = page.googleScript()
             if len(out) == 3:
-                f,c,p = out
-                if f != "": functions.append(f)
-                if c != "": callbacks.append(c)
-                if p != "": packages.append(p)
+                f, c, p = out
+                if f != "":
+                    functions.append(f)
+                if c != "":
+                    callbacks.append(c)
+                if p != "":
+                    packages.append(p)
 
         code += """
     <script type='text/javascript'>
         function pageLoad() {"""
         for c in callbacks:
-            code += """
-           %s();""" % c
+            code += (
+                """
+           %s();"""
+                % c
+            )
         code += """
         }
     </script>"""
 
         code += """
     <script type='text/javascript'>"""
         for f in functions:
             code += f
         code += """
     </script>"""
 
-        max_height = 280 # will be related to max column header length across all pages
+        max_height = 280  # will be related to max column header length across all pages
         code += """
     <style type="text/css">
       .container{
         display:inline;
       }
       .break{
         clear:left;
@@ -985,325 +1286,406 @@
           text-align: right;
       }
       td.divider {
           width: 0px;
           border: 0px solid #ccc;
           padding: 0px 0px
       }
-    </style>""" % (max_height,max_height/2-5)
+    </style>""" % (
+            max_height,
+            max_height / 2 - 5,
+        )
 
         code += """
   </head>
   <body onload="pageLoad()">"""
 
         ### loop over pages
-        for page in self.pages: code += "%s" % (page)
+        for page in self.pages:
+            code += "%s" % (page)
 
         code += """
   </body>
 </html>"""
         return code
 
+
 def RegisterCustomColormaps():
-    """Adds some new colormaps to matplotlib's database.
-    """
+    """Adds some new colormaps to matplotlib's database."""
     import colorsys as cs
 
     # score colormap
-    cm = LinearSegmentedColormap.from_list("score",[[0.84765625, 0.37109375, 0.0078125 ],
-                                                    [0.45703125, 0.4375    , 0.69921875],
-                                                    [0.10546875, 0.6171875 , 0.46484375]])
-    plt.register_cmap("score",cm)
+    cm = plt.get_cmap("plasma")
+    plt.register_cmap("score", cm)
 
     # bias colormap
     val = 0.8
-    per = 0.2 /2
-    Rd  = cs.rgb_to_hsv(1,0,0)
-    Rd  = cs.hsv_to_rgb(Rd[0],Rd[1],val)
-    Bl  = cs.rgb_to_hsv(0,0,1)
-    Bl  = cs.hsv_to_rgb(Bl[0],Bl[1],val)
-    RdBl = {'red':   ((0.0    , 0.0,   Bl[0]),
-                      (0.5-per, 1.0  , 1.0  ),
-                      (0.5+per, 1.0  , 1.0  ),
-                      (1.0    , Rd[0], 0.0  )),
-            'green': ((0.0    , 0.0,   Bl[1]),
-                      (0.5-per, 1.0  , 1.0  ),
-                      (0.5+per, 1.0  , 1.0  ),
-                      (1.0    , Rd[1], 0.0  )),
-            'blue':  ((0.0    , 0.0,   Bl[2]),
-                      (0.5-per, 1.0  , 1.0  ),
-                      (0.5+per, 1.0  , 1.0  ),
-                      (1.0    , Rd[2], 0.0  ))}
-    plt.register_cmap(cmap=LinearSegmentedColormap('bias',RdBl))
-
-    cm = LinearSegmentedColormap.from_list("wetdry",[[0.545882,0.400392,0.176078],
-                                                     [0.586667,0.440392,0.198824],
-                                                     [0.627451,0.480392,0.221569],
-                                                     [0.668235,0.520392,0.244314],
-                                                     [0.709020,0.560392,0.267059],
-                                                     [0.749804,0.600392,0.289804],
-                                                     [0.790588,0.640392,0.312549],
-                                                     [0.831373,0.680392,0.335294],
-                                                     [0.872157,0.720392,0.358039],
-                                                     [0.912941,0.760392,0.380784],
-                                                     [0.921961,0.788039,0.399020],
-                                                     [0.899216,0.803333,0.412745],
-                                                     [0.876471,0.818627,0.426471],
-                                                     [0.853725,0.833922,0.440196],
-                                                     [0.830980,0.849216,0.453922],
-                                                     [0.808235,0.864510,0.467647],
-                                                     [0.785490,0.879804,0.481373],
-                                                     [0.762745,0.895098,0.495098],
-                                                     [0.740000,0.910392,0.508824],
-                                                     [0.717255,0.925686,0.522549],
-                                                     [0.680392,0.933333,0.549020],
-                                                     [0.629412,0.933333,0.588235],
-                                                     [0.578431,0.933333,0.627451],
-                                                     [0.527451,0.933333,0.666667],
-                                                     [0.476471,0.933333,0.705882],
-                                                     [0.425490,0.933333,0.745098],
-                                                     [0.374510,0.933333,0.784314],
-                                                     [0.323529,0.933333,0.823529],
-                                                     [0.272549,0.933333,0.862745],
-                                                     [0.221569,0.933333,0.901961],
-                                                     [0.188627,0.910196,0.922157],
-                                                     [0.173725,0.863922,0.923333],
-                                                     [0.158824,0.817647,0.924510],
-                                                     [0.143922,0.771373,0.925686],
-                                                     [0.129020,0.725098,0.926863],
-                                                     [0.114118,0.678824,0.928039],
-                                                     [0.099216,0.632549,0.929216],
-                                                     [0.084314,0.586275,0.930392],
-                                                     [0.069412,0.540000,0.931569],
-                                                     [0.054510,0.493725,0.932745],
-                                                     [0.052157,0.447255,0.922549],
-                                                     [0.062353,0.400588,0.900980],
-                                                     [0.072549,0.353922,0.879412],
-                                                     [0.082745,0.307255,0.857843],
-                                                     [0.092941,0.260588,0.836275],
-                                                     [0.103137,0.213922,0.814706],
-                                                     [0.113333,0.167255,0.793137],
-                                                     [0.123529,0.120588,0.771569],
-                                                     [0.133725,0.073922,0.750000],
-                                                     [0.143922,0.027255,0.728431],
-                                                     [0.143137,0.013725,0.703922],
-                                                     [0.131373,0.033333,0.676471],
-                                                     [0.119608,0.052941,0.649020],
-                                                     [0.107843,0.072549,0.621569],
-                                                     [0.096078,0.092157,0.594118],
-                                                     [0.084314,0.111765,0.566667],
-                                                     [0.072549,0.131373,0.539216],
-                                                     [0.060784,0.150980,0.511765],
-                                                     [0.049020,0.170588,0.484314],
-                                                     [0.037255,0.190196,0.456863]])
-    plt.register_cmap("wetdry",cm)                                                    
+    per = 0.2 / 2
+    Rd = cs.rgb_to_hsv(1, 0, 0)
+    Rd = cs.hsv_to_rgb(Rd[0], Rd[1], val)
+    Bl = cs.rgb_to_hsv(0, 0, 1)
+    Bl = cs.hsv_to_rgb(Bl[0], Bl[1], val)
+    RdBl = {
+        "red": (
+            (0.0, 0.0, Bl[0]),
+            (0.5 - per, 1.0, 1.0),
+            (0.5 + per, 1.0, 1.0),
+            (1.0, Rd[0], 0.0),
+        ),
+        "green": (
+            (0.0, 0.0, Bl[1]),
+            (0.5 - per, 1.0, 1.0),
+            (0.5 + per, 1.0, 1.0),
+            (1.0, Rd[1], 0.0),
+        ),
+        "blue": (
+            (0.0, 0.0, Bl[2]),
+            (0.5 - per, 1.0, 1.0),
+            (0.5 + per, 1.0, 1.0),
+            (1.0, Rd[2], 0.0),
+        ),
+    }
+    plt.register_cmap(cmap=LinearSegmentedColormap("bias", RdBl))
+
+    cm = LinearSegmentedColormap.from_list(
+        "wetdry",
+        [
+            [0.545882, 0.400392, 0.176078],
+            [0.586667, 0.440392, 0.198824],
+            [0.627451, 0.480392, 0.221569],
+            [0.668235, 0.520392, 0.244314],
+            [0.709020, 0.560392, 0.267059],
+            [0.749804, 0.600392, 0.289804],
+            [0.790588, 0.640392, 0.312549],
+            [0.831373, 0.680392, 0.335294],
+            [0.872157, 0.720392, 0.358039],
+            [0.912941, 0.760392, 0.380784],
+            [0.921961, 0.788039, 0.399020],
+            [0.899216, 0.803333, 0.412745],
+            [0.876471, 0.818627, 0.426471],
+            [0.853725, 0.833922, 0.440196],
+            [0.830980, 0.849216, 0.453922],
+            [0.808235, 0.864510, 0.467647],
+            [0.785490, 0.879804, 0.481373],
+            [0.762745, 0.895098, 0.495098],
+            [0.740000, 0.910392, 0.508824],
+            [0.717255, 0.925686, 0.522549],
+            [0.680392, 0.933333, 0.549020],
+            [0.629412, 0.933333, 0.588235],
+            [0.578431, 0.933333, 0.627451],
+            [0.527451, 0.933333, 0.666667],
+            [0.476471, 0.933333, 0.705882],
+            [0.425490, 0.933333, 0.745098],
+            [0.374510, 0.933333, 0.784314],
+            [0.323529, 0.933333, 0.823529],
+            [0.272549, 0.933333, 0.862745],
+            [0.221569, 0.933333, 0.901961],
+            [0.188627, 0.910196, 0.922157],
+            [0.173725, 0.863922, 0.923333],
+            [0.158824, 0.817647, 0.924510],
+            [0.143922, 0.771373, 0.925686],
+            [0.129020, 0.725098, 0.926863],
+            [0.114118, 0.678824, 0.928039],
+            [0.099216, 0.632549, 0.929216],
+            [0.084314, 0.586275, 0.930392],
+            [0.069412, 0.540000, 0.931569],
+            [0.054510, 0.493725, 0.932745],
+            [0.052157, 0.447255, 0.922549],
+            [0.062353, 0.400588, 0.900980],
+            [0.072549, 0.353922, 0.879412],
+            [0.082745, 0.307255, 0.857843],
+            [0.092941, 0.260588, 0.836275],
+            [0.103137, 0.213922, 0.814706],
+            [0.113333, 0.167255, 0.793137],
+            [0.123529, 0.120588, 0.771569],
+            [0.133725, 0.073922, 0.750000],
+            [0.143922, 0.027255, 0.728431],
+            [0.143137, 0.013725, 0.703922],
+            [0.131373, 0.033333, 0.676471],
+            [0.119608, 0.052941, 0.649020],
+            [0.107843, 0.072549, 0.621569],
+            [0.096078, 0.092157, 0.594118],
+            [0.084314, 0.111765, 0.566667],
+            [0.072549, 0.131373, 0.539216],
+            [0.060784, 0.150980, 0.511765],
+            [0.049020, 0.170588, 0.484314],
+            [0.037255, 0.190196, 0.456863],
+        ],
+    )
+    plt.register_cmap("wetdry", cm)
+
 
-def HarvestScalarDatabase(build_dir,filename="scalar_database.csv"):
+def HarvestScalarDatabase(build_dir, filename="scalar_database.csv"):
     csv = '"Section","Variable","Source","Model","ScalarName","AnalysisType","Region","ScalarType","Units","Data","Weight"'
-    for root,subdirs,files in os.walk(build_dir):
+    for root, subdirs, files in os.walk(build_dir):
         for fname in files:
-            if not fname.endswith(".nc"): continue
-            if "Benchmark" in fname: continue
-            info = root.replace(build_dir,"")
-            if info.startswith("/"): info = info[1:].split("/")
+            if not fname.endswith(".nc"):
+                continue
+            if "Benchmark" in fname:
+                continue
+            info = root.replace(build_dir, "")
+            if info.startswith("/"):
+                info = info[1:].split("/")
             category = info[0]
-            varname  = info[1]
+            varname = info[1]
             provider = info[2]
-            with Dataset(os.path.join(root,fname)) as dset:
-                if dset.complete != 1: continue
+            with Dataset(os.path.join(root, fname)) as dset:
+                if dset.complete != 1:
+                    continue
                 model = dset.getncattr("name")
                 weight = dset.getncattr("weight")
                 for g1 in dset.groups:
                     for g2 in dset.groups[g1].groups:
                         grp = dset.groups[g1].groups[g2]
                         for vname in grp.variables:
                             stype = "score" if "Score" in vname else "scalar"
                             region = vname.split()[-1]
-                            var = vname.replace(" %s" % region,"")
+                            var = vname.replace(" %s" % region, "")
                             v = grp.variables[vname]
                             V = v[...]
                             s = "nan" if V.mask else "%g" % V
-                            csv += "\n" + ",".join(['"%s"' % v for v in (category,varname,provider,model,var,g1,region,stype,v.units,s,weight)])
-    with open(os.path.join(build_dir,filename),mode="w") as f: f.write(csv)
+                            csv += "\n" + ",".join(
+                                [
+                                    '"%s"' % v
+                                    for v in (
+                                        category,
+                                        varname,
+                                        provider,
+                                        model,
+                                        var,
+                                        g1,
+                                        region,
+                                        stype,
+                                        v.units,
+                                        s,
+                                        weight,
+                                    )
+                                ]
+                            )
+    with open(os.path.join(build_dir, filename), mode="w") as f:
+        f.write(csv)
 
-def CreateJSON(csv_file,M=None):
+
+def CreateJSON(csv_file, M=None):
     """Using the CSV scalar database, create a JSON following the CMEC standard.
 
     Parameters
     ----------
     csv_file : str
         the full path to the scalar database CSV file
     M : list of ILAMB.ModelResult, optional
         if not given, then the routine will attempt to load pickle
         files. If no models are given and they cannot be found in
         pickle files, then no description or source will be provided.
 
     """
-    def _unCamelCase(s): return re.sub("([a-z])([A-Z])","\g<1> \g<2>",s)
-    def _weightedMean(x): return (x.Data*x.Weight/x.Weight.sum()).sum()
-    def _meanScore(df_local,short,*args):
-        cols = ['Section','Variable','Source']
-        q = df_local.query(" & ".join(["(%s == '%s')" % (col,arg) for arg,col in zip(args,cols)]))
+
+    def _unCamelCase(s):
+        return re.sub("([a-z])([A-Z])", "\g<1> \g<2>", s)
+
+    def _weightedMean(x):
+        return (x.Data * x.Weight / x.Weight.sum()).sum()
+
+    def _meanScore(df_local, short, *args):
+        cols = ["Section", "Variable", "Source"]
+        q = df_local.query(
+            " & ".join(["(%s == '%s')" % (col, arg) for arg, col in zip(args, cols)])
+        )
         scores = {}
         for s in short:
             qs = q.query("ScalarName == '%s'" % s)
-            if qs.shape[0] > 0: scores[_unCamelCase(s)] = _weightedMean(qs)
+            if qs.shape[0] > 0:
+                scores[_unCamelCase(s)] = _weightedMean(qs)
         return scores
+
     def _parseConfig(f):
         lines = open(f).readlines()
         cfg = {}
         rel = {}
         h1 = h2 = v = None
         for line in lines:
             line = line.strip()
             if line.startswith("[h1:"):
                 h1 = line.strip("[h1:").strip("]").strip()
             elif line.startswith("[h2:"):
                 h2 = line.strip("[h2:").strip("]").strip()
             elif line.startswith("["):
                 v = line.strip("[").strip("]").strip()
-                if h1 not in cfg    : cfg[h1]     = {}
-                if h2 not in cfg[h1]: cfg[h1][h2] = []
+                if h1 not in cfg:
+                    cfg[h1] = {}
+                if h2 not in cfg[h1]:
+                    cfg[h1][h2] = []
                 cfg[h1][h2].append(v)
             elif line.startswith("relationships"):
-                line = line.replace('"','').replace("'","")
+                line = line.replace('"', "").replace("'", "")
                 line = (line.split("=")[1]).strip()
                 line = line.split(",")
-                r2 = "%s/%s" % (h2,v)
-                if h2 not in rel: rel[r2] = []
+                r2 = "%s/%s" % (h2, v)
+                if h2 not in rel:
+                    rel[r2] = []
                 for ind in line:
                     ind = ind.strip()
                     rel[r2].append(_unCamelCase(ind))
-        if rel: cfg["Relationships"] = rel
+        if rel:
+            cfg["Relationships"] = rel
         return cfg
-    
+
     # Drop nan's and we only need the scores from the database
     df = pd.read_csv(csv_file).dropna().query("ScalarType=='score'")
-    
+
     # Also drop 'Overall Score' for relationships, these mess up our
     # aggregation in this routine
     q = df.query("AnalysisType=='Relationships' & ScalarName=='Overall Score'")
     df = df.drop(q.index)
     if M is None:
         models = list(df.Model.unique())
     else:
         models = [m.name for m in M]
     r = Regions()
     regions = [n for n in df.Region.unique() if n in r.regions]
 
     out = {}
     # meta-data for which scheme and package have been used
-    out["SCHEMA"] = {"name": "CMEC","version": "v1","package": "ILAMB"}
+    out["SCHEMA"] = {"name": "CMEC", "version": "v1", "package": "ILAMB"}
 
     # what dimensions should we find?
-    out["DIMENSIONS"] = {"json_structure": ["region","model","metric","statistic"]}
+    out["DIMENSIONS"] = {"json_structure": ["region", "model", "metric", "statistic"]}
     out["DIMENSIONS"]["dimensions"] = {}
 
     # populate the regions
     nest = {}
     for region in regions:
         name = r.getRegionName(region)
         source = r.getRegionSource(region)
-        nest[region] = {"LongName":name,"Description":name,"Generator":source}
+        nest[region] = {"LongName": name, "Description": name, "Generator": source}
     out["DIMENSIONS"]["dimensions"]["region"] = nest
 
     # populate the models
     if M is None:
         M = []
-        for pkl_file in glob.glob(os.path.join(os.path.dirname(csv_file),"*.pkl")):
-            with open(pkl_file,'rb') as infile:
+        for pkl_file in glob.glob(os.path.join(os.path.dirname(csv_file), "*.pkl")):
+            with open(pkl_file, "rb") as infile:
                 M.append(pickle.load(infile))
     nest = {}
     for model in models:
         m = [m for m in M if m.name == model]
         if len(m) > 0:
-            nest[model] = {"Description":m[0].description,"Source":m[0].group}
+            nest[model] = {
+                "Description": m[0].description if hasattr(m[0], "description") else "",
+                "Source": m[0].group if hasattr(m[0], "group") else "",
+            }
         else:
-            nest[model] = {"Description":"","Source":""}
+            nest[model] = {"Description": "", "Source": ""}
     out["DIMENSIONS"]["dimensions"]["model"] = nest
 
     # populate the list of metrics
-    cfg = _parseConfig(os.path.join(os.path.dirname(csv_file),"ilamb.cfg"))
+    cfg = _parseConfig(os.path.join(os.path.dirname(csv_file), "ilamb.cfg"))
     nest = {}
-    base = {"URI":["https://www.osti.gov/biblio/1330803",
-                   "https://doi.org/10.1029/2018MS001354"],
-            "Contact": "forrest AT climatemodeling.org"}
+    base = {
+        "URI": [
+            "https://www.osti.gov/biblio/1330803",
+            "https://doi.org/10.1029/2018MS001354",
+        ],
+        "Contact": "forrest AT climatemodeling.org",
+    }
     S = list(cfg.keys())
     for s in S:
         s_json = s
-        s_csv  = s.replace(" ","")
-        nest[s_json] = {"Name":s_json,"Abstract":"composite score"}
+        s_csv = s.replace(" ", "")
+        nest[s_json] = {"Name": s_json, "Abstract": "composite score"}
         nest[s_json].update(base)
         V = list(cfg[s].keys())
         for v in V:
-            v_json = "%s::%s" % (s_json,v)
-            v_csv  = v.replace(" ","")
-            nest[v_json] = {"Name":v_json,"Abstract":"composite score"}
+            v_json = "%s::%s" % (s_json, v)
+            v_csv = v.replace(" ", "")
+            nest[v_json] = {"Name": v_json, "Abstract": "composite score"}
             nest[v_json].update(base)
             D = cfg[s][v]
             for d in D:
-                d_json = "%s!!%s" % (v_json,d)
-                d_csv  = d.replace(" ","")
-                nest[d_json] = {"Name":d_json,"Abstract":"benchmark score"}
-                nest[d_json].update(base)                
+                d_json = "%s!!%s" % (v_json, d)
+                d_csv = d.replace(" ", "")
+                nest[d_json] = {"Name": d_json, "Abstract": "benchmark score"}
+                nest[d_json].update(base)
     out["DIMENSIONS"]["dimensions"]["metric"] = nest
 
     # populate list of statistics, sorted so the common ones appear first
     def _priority(key):
         val = 1
         found = False
-        for i,word in enumerate(['overall','bias','rmse','cycle','interannual','spatial']):
+        for i, word in enumerate(
+            ["overall", "bias", "rmse", "cycle", "interannual", "spatial"]
+        ):
             if word in key.lower():
                 val *= 2**i
                 found = True
-        if not found: val = 2**6
+        if not found:
+            val = 2**6
         return val
-    short = list(df.query("AnalysisType=='MeanState' & ScalarType=='score'").ScalarName.unique())
-    short = sorted(short,key=_priority)
+
+    short = list(
+        df.query("AnalysisType=='MeanState' & ScalarType=='score'").ScalarName.unique()
+    )
+    short = sorted(short, key=_priority)
     index = [_unCamelCase(n) for n in short]
-    out["DIMENSIONS"]["dimensions"]["statistic"] = {"indices":index,"short_names":short}
+    out["DIMENSIONS"]["dimensions"]["statistic"] = {
+        "indices": index,
+        "short_names": short,
+    }
 
     # populate the statistics and their means
     nest = {}
     for region in regions:
         nest[region] = {}
         for m in models:
             nest[region][m] = {}
-            df_m = df.query("AnalysisType=='MeanState' & Region=='%s' & Model=='%s'" % (region,m))
+            df_m = df.query(
+                "AnalysisType=='MeanState' & Region=='%s' & Model=='%s'" % (region, m)
+            )
             for s in S:
                 s_json = s
-                s_csv  = s.replace(" ","")
-                t = _meanScore(df_m,short,s_csv)
-                if t: nest[region][m][s_json] = t
+                s_csv = s.replace(" ", "")
+                t = _meanScore(df_m, short, s_csv)
+                if t:
+                    nest[region][m][s_json] = t
                 V = list(cfg[s].keys())
                 for v in V:
-                    v_json = "%s::%s" % (s_json,v)
-                    v_csv  = v.replace(" ","")
-                    t = _meanScore(df_m,short,s_csv,v_csv)
-                    if t: nest[region][m][v_json] = t
+                    v_json = "%s::%s" % (s_json, v)
+                    v_csv = v.replace(" ", "")
+                    t = _meanScore(df_m, short, s_csv, v_csv)
+                    if t:
+                        nest[region][m][v_json] = t
                     D = cfg[s][v]
                     for d in D:
-                        d_json = "%s!!%s" % (v_json,d)
-                        d_csv  = d.replace(" ","")
-                        t = _meanScore(df_m,short,s_csv,v_csv,d_csv)
-                        if t: nest[region][m][d_json] = t
-                        
-            df_m = df.query("AnalysisType=='Relationships' & Region=='%s' & Model=='%s'" % (region,m))
+                        d_json = "%s!!%s" % (v_json, d)
+                        d_csv = d.replace(" ", "")
+                        t = _meanScore(df_m, short, s_csv, v_csv, d_csv)
+                        if t:
+                            nest[region][m][d_json] = t
+
+            df_m = df.query(
+                "AnalysisType=='Relationships' & Region=='%s' & Model=='%s'"
+                % (region, m)
+            )
             s_json = s_csv = "Relationships"
             if len(df_m):
-                nest[region][m][s_json] = {'Overall Score':_weightedMean(df_m)}
+                nest[region][m][s_json] = {"Overall Score": _weightedMean(df_m)}
                 V = list(cfg[s].keys())
                 for v in V:
-                    v_json = "%s::%s" % (s_json,v)
-                    v_csv  = v.replace(" ","").split("/")
-                    q = df_m.query("Variable=='%s' & Source=='%s'" % (v_csv[0],v_csv[1]))
-                    nest[region][m][v_json] = {'Overall Score':_weightedMean(q)}
+                    v_json = "%s::%s" % (s_json, v)
+                    v_csv = v.replace(" ", "").split("/")
+                    q = df_m.query(
+                        "Variable=='%s' & Source=='%s'" % (v_csv[0], v_csv[1])
+                    )
+                    nest[region][m][v_json] = {"Overall Score": _weightedMean(q)}
                     D = cfg[s][v]
                     for d in D:
-                        d_json = "%s!!%s" % (v_json,d)
-                        d_csv  = d.replace(" ","").replace("/","|")
-                        q = df_m.query("Variable=='%s' & Source=='%s' & ScalarName=='%s Score'" % (v_csv[0],v_csv[1],d_csv))
-                        nest[region][m][d_json] = {'Overall Score':_weightedMean(q)}
+                        d_json = "%s!!%s" % (v_json, d)
+                        d_csv = d.replace(" ", "").replace("/", "|")
+                        q = df_m.query(
+                            "Variable=='%s' & Source=='%s' & ScalarName=='%s Score'"
+                            % (v_csv[0], v_csv[1], d_csv)
+                        )
+                        nest[region][m][d_json] = {"Overall Score": _weightedMean(q)}
     out["RESULTS"] = nest
 
-    with open(csv_file.replace(".csv",".json"), 'w') as outfile:
-        json.dump(out,outfile)
+    with open(csv_file.replace(".csv", ".json"), "w") as outfile:
+        json.dump(out, outfile)
```

### Comparing `ILAMB-2.6/src/ILAMB/Relationship.py` & `ILAMB-2.7/src/ILAMB/Relationship.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-from .Regions import Regions
 import numpy as np
-import matplotlib.pyplot as plt
 from matplotlib.colors import LogNorm
 from mpl_toolkits.axes_grid1 import make_axes_locatable
-from .Post import UnitStringToMatplotlib
-    
+
+from ILAMB.Post import UnitStringToMatplotlib
+from ILAMB.Regions import Regions
+
+
 class Relationship(object):
-    
-    def __init__(self,ind,dep,ind_log=False,dep_log=False,order=None,color=None):
+    def __init__(self, ind, dep, ind_log=False, dep_log=False, order=None, color=None):
         """A class for investigating the relationship, dep = f(ind).
 
         Parameters
         ----------
         ind : ILAMB.Variable
             a ILAMB Variable which represents the independent variable
         dep : ILAMB.Variable
@@ -36,108 +36,128 @@
         self.checkConsistency()
         self.limits = self.computeLimits()
         self.dist = {}
         self.order = None
         if order is not None:
             order = int(order)
             self.order = order
-                        
+
     def __str__(self):
-        s  = "Relationship:\n"
+        s = "Relationship:\n"
         s += "-------------\n"
-        s += "{0:>20}: ".format("independent") + self.ind.name + "%s\n" % (" (log)" if self.ind_log else "")
-        s += "{0:>20}: ".format("ind limits")  + "(%+1.3e,%+1.3e) [%s]\n" % (self.limits[1][0],
-                                                                             self.limits[1][1],
-                                                                             self.ind.unit)
-        s += "{0:>20}: ".format("dependent")  + self.dep.name + "%s\n" % (" (log)" if self.dep_log else "")
-        s += "{0:>20}: ".format("dep limits") + "(%+1.3e,%+1.3e) [%s]\n" % (self.limits[0][0],
-                                                                            self.limits[0][1],
-                                                                            self.dep.unit)
+        s += (
+            "{0:>20}: ".format("independent")
+            + self.ind.name
+            + "%s\n" % (" (log)" if self.ind_log else "")
+        )
+        s += "{0:>20}: ".format("ind limits") + "(%+1.3e,%+1.3e) [%s]\n" % (
+            self.limits[1][0],
+            self.limits[1][1],
+            self.ind.unit,
+        )
+        s += (
+            "{0:>20}: ".format("dependent")
+            + self.dep.name
+            + "%s\n" % (" (log)" if self.dep_log else "")
+        )
+        s += "{0:>20}: ".format("dep limits") + "(%+1.3e,%+1.3e) [%s]\n" % (
+            self.limits[0][0],
+            self.limits[0][1],
+            self.dep.unit,
+        )
         if self.order is not None:
             s += "{0:>20}: ".format("polynomial") + "%d\n" % self.order
         return s
 
     def checkConsistency(self):
-        assert np.allclose(self.dep.data.shape,self.ind.data.shape)
-        if self.color: assert np.allclose(self.dep.data.shape,self.color.data.shape)
-        if self.ind_log: assert self.ind.data.min() > 0
-        if self.dep_log: assert self.dep.data.min() > 0
-        
-    def makeComparable(self,b,region=None):
+        assert np.allclose(self.dep.data.shape, self.ind.data.shape)
+        if self.color:
+            assert np.allclose(self.dep.data.shape, self.color.data.shape)
+        if self.ind_log:
+            assert self.ind.data.min() > 0
+        if self.dep_log:
+            assert self.dep.data.min() > 0
+
+    def makeComparable(self, b, region=None):
         """Ensures that relationships a and b are consistent on given region for scoring.
 
         Parameters
         ----------
         b : Relationship
             the relationships to consider
         region : string, optional
             the optional region on which to compare
         """
-        assert (type(b) == Relationship)
-        assert (self.ind_log == b.ind_log)*(self.dep_log == b.dep_log)
+        assert type(b) == Relationship
+        assert (self.ind_log == b.ind_log) * (self.dep_log == b.dep_log)
         key = "default" if region is None else region
         recompute_a = False
         recompute_b = False
-        
-        # Are the limits different? 
-        if not np.allclose(self.limits,b.limits):
+
+        # Are the limits different?
+        if not np.allclose(self.limits, b.limits):
             recompute_a = True
             recompute_b = True
-            limits = self.computeLimits(dep_lim=b.limits[0],
-                                        ind_lim=b.limits[1])
+            limits = self.computeLimits(dep_lim=b.limits[0], ind_lim=b.limits[1])
             self.limits = limits
             b.limits = limits
 
         # Have the distributions been tabulated?
-        if key not in self.dist.keys(): recompute_a = True
-        if key not in b.dist.keys(): recompute_b = True
-        
+        if key not in self.dist.keys():
+            recompute_a = True
+        if key not in b.dist.keys():
+            recompute_b = True
+
         # Are the distributions binned the same way?
         if (key in self.dist.keys()) and (key in b.dist.keys()):
-            if not np.allclose(self.dist[key][0].shape,b.dist[key][0].shape):
+            if not np.allclose(self.dist[key][0].shape, b.dist[key][0].shape):
                 recompute_a = True
                 recompute_b = True
-            if not np.allclose(self.dist[key][1],b.dist[key][1]):
+            if not np.allclose(self.dist[key][1], b.dist[key][1]):
                 recompute_a = True
                 recompute_b = True
-            if not np.allclose(self.dist[key][2],b.dist[key][2]):
+            if not np.allclose(self.dist[key][2], b.dist[key][2]):
                 recompute_a = True
                 recompute_b = True
 
         # Recompute if needed
-        if recompute_a: self.buildResponse(region=region)
-        if recompute_b: b.buildResponse(region=region)
+        if recompute_a:
+            self.buildResponse(region=region)
+        if recompute_b:
+            b.buildResponse(region=region)
 
-    def computeLimits(self,dep_lim=None,ind_lim=None):
-        """Computes the limits of the dependent and independent. 
+    def computeLimits(self, dep_lim=None, ind_lim=None):
+        """Computes the limits of the dependent and independent.
 
         Parameters
         ----------
         dep_lim : array-like of size 2, optional
             if specified, will return the most extensive limits of the
             dependent variable
         ind_lim : array-like of size 2, optional
             if specified, will return the most extensive limits of the
             independent variable
 
         """
-        def _singlelimit(var,limit=None):
-            lim     = [var.data.min(),var.data.max()]
-            delta   = 1e-8*(lim[1]-lim[0])
+
+        def _singlelimit(var, limit=None):
+            lim = [var.data.min(), var.data.max()]
+            delta = 1e-8 * (lim[1] - lim[0])
             lim[0] -= delta
             lim[1] += delta
             if limit is None:
                 limit = lim
             else:
-                limit[0] = min(limit[0],lim[0])
-                limit[1] = max(limit[1],lim[1])
+                limit[0] = min(limit[0], lim[0])
+                limit[1] = max(limit[1], lim[1])
             return limit
-        return _singlelimit(self.dep,dep_lim),_singlelimit(self.ind,ind_lim)
 
-    def buildResponse(self,region=None,nbin=25,eps=3e-3):
+        return _singlelimit(self.dep, dep_lim), _singlelimit(self.ind, ind_lim)
+
+    def buildResponse(self, region=None, nbin=25, eps=3e-3):
         """Creates a 2D distribution and a functional response.
 
         Also stores the values internally for later use.
 
         Parameters
         ----------
         region : str, optional
@@ -165,73 +185,88 @@
         p : numpy.ndarray, shape = (order+1)
             the polynomial coefficient array, last entry is the constant
         """
         dep = self.dep
         ind = self.ind
         dep_lim = self.limits[0]
         ind_lim = self.limits[1]
-        
+
         # Mask data
         mask = ind.data.mask + dep.data.mask
-        if region is not None: mask += Regions().getMask(region,ind)
-        x = ind.data[mask==False].flatten()
-        y = dep.data[mask==False].flatten()
+        if region is not None:
+            mask += Regions().getMask(region, ind)
+        x = ind.data[mask == False].flatten()
+        y = dep.data[mask == False].flatten()
         xedges = nbin
         yedges = nbin
         if self.ind_log:
-            xedges = 10**np.linspace(np.log10(ind_lim[ 0]),
-                                     np.log10(ind_lim[-1]),nbin+1)
+            xedges = 10 ** np.linspace(
+                np.log10(ind_lim[0]), np.log10(ind_lim[-1]), nbin + 1
+            )
         if self.dep_log:
-            yedges = 10**np.linspace(np.log10(dep_lim[ 0]),
-                                     np.log10(dep_lim[-1]),nbin+1)
-        
+            yedges = 10 ** np.linspace(
+                np.log10(dep_lim[0]), np.log10(dep_lim[-1]), nbin + 1
+            )
+
         # Compute normalized 2D distribution
-        dist,xedges,yedges = np.histogram2d(x,y,
-                                            bins  = [xedges,yedges],
-                                            range = [ind_lim,dep_lim])
-        dist  = np.ma.masked_values(dist.T,0).astype(float)
+        dist, xedges, yedges = np.histogram2d(
+            x, y, bins=[xedges, yedges], range=[ind_lim, dep_lim]
+        )
+        dist = np.ma.masked_values(dist.T, 0).astype(float)
         dist /= dist.sum()
 
         # Compute the functional response
-        which_bin = np.digitize(x,xedges).clip(1,xedges.size-1)-1
-        mean = np.ma.zeros(xedges.size-1)
-        std  = np.ma.zeros(xedges.size-1)
-        cnt  = np.ma.zeros(xedges.size-1)
-        with np.errstate(under='ignore'):
+        which_bin = np.digitize(x, xedges).clip(1, xedges.size - 1) - 1
+        mean = np.ma.zeros(xedges.size - 1)
+        std = np.ma.zeros(xedges.size - 1)
+        cnt = np.ma.zeros(xedges.size - 1)
+        with np.errstate(under="ignore"):
             for i in range(mean.size):
-                yi = y[which_bin==i]
-                cnt [i] = yi.size
+                yi = y[which_bin == i]
+                cnt[i] = yi.size
                 if self.dep_log:
                     yi = np.log10(yi)
-                    mean[i] = 10**yi.mean()
-                    std [i] = 10**yi.std()
+                    mean[i] = 10 ** yi.mean()
+                    std[i] = 10 ** yi.std()
                 else:
                     mean[i] = yi.mean()
-                    std [i] = yi.std()
-            mean = np.ma.masked_array(mean,mask = (cnt/cnt.sum()) < eps)
-            std  = np.ma.masked_array( std,mask = (cnt/cnt.sum()) < eps)
+                    std[i] = yi.std()
+            mean = np.ma.masked_array(mean, mask=(cnt / cnt.sum()) < eps)
+            std = np.ma.masked_array(std, mask=(cnt / cnt.sum()) < eps)
 
         # If there is a model order given, compute the regression and
         # the 50% prediction interval
         p = None
         i = None
         if self.order is not None:
-            gauss_critval =  0.674 # for 50%, could make more abstract
+            gauss_critval = 0.674  # for 50%, could make more abstract
             if self.dep_log:
-                p = np.polyfit(x,np.log10(y),self.order)
-                i = gauss_critval*(np.log10(y)-np.polyval(p,x)).std()
+                p = np.polyfit(x, np.log10(y), self.order)
+                with np.errstate(under="ignore"):
+                    i = gauss_critval * (np.log10(y) - np.polyval(p, x)).std()
             else:
-                p = np.polyfit(x,y,self.order)
-                i = gauss_critval*(y-np.polyval(p,x)).std()
-        
-        # Save the arrays
-        self.dist["default" if region is None else region] = dist,xedges,yedges,mean,std,p,i
-        return dist,xedges,yedges,mean,std,p,i
+                p = np.polyfit(x, y, self.order)
+                with np.errstate(under="ignore"):
+                    i = gauss_critval * (y - np.polyval(p, x)).std()
 
-    def plotPointCloud(self,ax,region=None,ms=1,color=None,vmin=None,vmax=None,cmap=None):
+        # Save the arrays
+        self.dist["default" if region is None else region] = (
+            dist,
+            xedges,
+            yedges,
+            mean,
+            std,
+            p,
+            i,
+        )
+        return dist, xedges, yedges, mean, std, p, i
+
+    def plotPointCloud(
+        self, ax, region=None, ms=1, color=None, vmin=None, vmax=None, cmap=None
+    ):
         """Plot the 2D point cloud.
 
         Parameters
         ----------
         ax : matplotlib axis
             the axis on which to plot the function
         region : str, optional
@@ -247,74 +282,89 @@
             plotted
         vmax : float
             if the relationship was initialized with a variable to
             color by, the maximum value of that variable that will be
             plotted
         cmap : str
             if the relationship was initialized with a variable to
-            color by, the colormap that will be used in plotting            
+            color by, the colormap that will be used in plotting
         """
         mask = self.ind.data.mask + self.dep.data.mask
-        if self.color is not None: mask += self.color.data.mask
-        if region is not None: mask += Regions().getMask(region,self.ind)
-        x = self.ind.data[mask==False].flatten()
-        y = self.dep.data[mask==False].flatten()
+        if self.color is not None:
+            mask += self.color.data.mask
+        if region is not None:
+            mask += Regions().getMask(region, self.ind)
+        x = self.ind.data[mask == False].flatten()
+        y = self.dep.data[mask == False].flatten()
         need_colorbar = False
         if self.color is not None and color is None:
-            color = self.color.data[mask==False].flatten()
+            color = self.color.data[mask == False].flatten()
             need_colorbar = True
-        sc = ax.scatter(x,y,c=color,s=ms,vmin=vmin,vmax=vmax,cmap=cmap)
+        sc = ax.scatter(x, y, c=color, s=ms, vmin=vmin, vmax=vmax, cmap=cmap)
         if need_colorbar:
             fig = ax.get_figure()
-            fig.colorbar(sc,orientation='horizontal',pad=0.15,
-                         label='%s [%s]'% (self.color.name,UnitStringToMatplotlib(self.color.unit)))
+            fig.colorbar(
+                sc,
+                orientation="horizontal",
+                pad=0.15,
+                label="%s [%s]"
+                % (self.color.name, UnitStringToMatplotlib(self.color.unit)),
+            )
         xlabel = self.ind.name + " [%s]" % (UnitStringToMatplotlib(self.ind.unit))
         ylabel = self.dep.name + " [%s]" % (UnitStringToMatplotlib(self.dep.unit))
-        ax.set_xlabel(xlabel,fontsize = 12)
-        ax.set_ylabel(ylabel,fontsize = 12 if len(ylabel) <= 60 else 10)
-        ax.set_xlim(self.limits[1][0],self.limits[1][1])
-        ax.set_ylim(self.limits[0][0],self.limits[0][1])
-        if self.dep_log: ax.set_yscale('log')
-        if self.ind_log: ax.set_xscale('log')
-        
-    def plotDistribution(self,ax,region=None):
+        ax.set_xlabel(xlabel, fontsize=12)
+        ax.set_ylabel(ylabel, fontsize=12 if len(ylabel) <= 60 else 10)
+        ax.set_xlim(self.limits[1][0], self.limits[1][1])
+        ax.set_ylim(self.limits[0][0], self.limits[0][1])
+        if self.dep_log:
+            ax.set_yscale("log")
+        if self.ind_log:
+            ax.set_xscale("log")
+
+    def plotDistribution(self, ax, region=None):
         """Plot the 2D histogram.
 
         Parameters
         ----------
         ax : matplotlib axis
             the axis on which to plot the function
         region : str, optional
             if the variables are spatial, restricts the response to
             cover only the cells defined by the given ILAMB Region
         """
         key = "default" if region is None else region
-        if key not in self.dist.keys(): self.buildResponse(region=region)
-        dist   = self.dist[key][0]
+        if key not in self.dist.keys():
+            self.buildResponse(region=region)
+        dist = self.dist[key][0]
         xedges = self.dist[key][1]
         yedges = self.dist[key][2]
         xlabel = self.ind.name + " [%s]" % (UnitStringToMatplotlib(self.ind.unit))
         ylabel = self.dep.name + " [%s]" % (UnitStringToMatplotlib(self.dep.unit))
         fig = ax.get_figure()
-        
-        pc = ax.pcolormesh(xedges, yedges, dist,
-                           norm = LogNorm(),
-                           cmap = 'plasma' if 'plasma' in plt.cm.cmap_d else 'summer',
-                           vmin = 1e-4, vmax = 1e-1)
+
+        pc = ax.pcolormesh(
+            xedges, yedges, dist, norm=LogNorm(vmin=1e-4, vmax=1e-1), cmap="plasma"
+        )
         div = make_axes_locatable(ax)
-        fig.colorbar(pc,cax=div.append_axes("right",size="5%",pad=0.05),
-                     orientation="vertical",label="Fraction of total datasites")
-        ax.set_xlabel(xlabel,fontsize = 12)
-        ax.set_ylabel(ylabel,fontsize = 12 if len(ylabel) <= 60 else 10)
-        ax.set_xlim(xedges[0],xedges[-1])
-        ax.set_ylim(yedges[0],yedges[-1])
-        if self.dep_log: ax.set_yscale('log')
-        if self.ind_log: ax.set_xscale('log')
-        
-    def plotFunction(self,ax,region=None,color='k',shift=0):
+        fig.colorbar(
+            pc,
+            cax=div.append_axes("right", size="5%", pad=0.05),
+            orientation="vertical",
+            label="Fraction of total datasites",
+        )
+        ax.set_xlabel(xlabel, fontsize=12)
+        ax.set_ylabel(ylabel, fontsize=12 if len(ylabel) <= 60 else 10)
+        ax.set_xlim(xedges[0], xedges[-1])
+        ax.set_ylim(yedges[0], yedges[-1])
+        if self.dep_log:
+            ax.set_yscale("log")
+        if self.ind_log:
+            ax.set_xscale("log")
+
+    def plotFunction(self, ax, region=None, color="k", shift=0):
         """Plot the mean response with standard deviation as error bars.
 
         Parameters
         ----------
         ax : matplotlib axis
             the axis on which to plot the function
         region : str, optional
@@ -324,38 +374,44 @@
             the color to use in plotting the line
         shift : float, optional
             shift expressed as a fraction on [-0.5,0.5] used to shift
             the plotting of the errorbars so that multiple functions do
             not overlab
         """
         key = "default" if region is None else region
-        if key not in self.dist.keys(): self.buildResponse(region=region)            
+        if key not in self.dist.keys():
+            self.buildResponse(region=region)
         y = self.dist[key][3]
         e = self.dist[key][4]
         xedges = self.dist[key][1]
         yedges = self.dist[key][2]
         xlabel = self.ind.name + " [%s]" % (UnitStringToMatplotlib(self.ind.unit))
         ylabel = self.dep.name + " [%s]" % (UnitStringToMatplotlib(self.dep.unit))
-        x = 0.5*(xedges[:-1]+xedges[1:]) + shift*np.diff(xedges).mean()
+        x = 0.5 * (xedges[:-1] + xedges[1:]) + shift * np.diff(xedges).mean()
         mask = y.mask
-        if type(mask) == np.bool_: mask = np.asarray([mask]*y.size)
-        x = x[mask==False]
-        y = y[mask==False]
-        e = e[mask==False]
+        if type(mask) == np.bool_:
+            mask = np.asarray([mask] * y.size)
+        x = x[mask == False]
+        y = y[mask == False]
+        e = e[mask == False]
         if self.dep_log:
-            e = np.asarray([10**(np.log10(y)-np.log10(e)),10**(np.log10(y)+np.log10(e))])
-        ax.errorbar(x,y,yerr=e,fmt='-o',color=color)
-        ax.set_xlabel(xlabel,fontsize = 12)
-        ax.set_ylabel(ylabel,fontsize = 12 if len(ylabel) <= 60 else 10)
-        ax.set_xlim(xedges[0],xedges[-1])
-        ax.set_ylim(yedges[0],yedges[-1])
-        if self.dep_log: ax.set_yscale('log')
-        if self.ind_log: ax.set_xscale('log')
+            e = np.asarray(
+                [10 ** (np.log10(y) - np.log10(e)), 10 ** (np.log10(y) + np.log10(e))]
+            )
+        ax.errorbar(x, y, yerr=e, fmt="-o", color=color)
+        ax.set_xlabel(xlabel, fontsize=12)
+        ax.set_ylabel(ylabel, fontsize=12 if len(ylabel) <= 60 else 10)
+        ax.set_xlim(xedges[0], xedges[-1])
+        ax.set_ylim(yedges[0], yedges[-1])
+        if self.dep_log:
+            ax.set_yscale("log")
+        if self.ind_log:
+            ax.set_xscale("log")
 
-    def plotModel(self,ax,region=None,color='k',prediction=False):
+    def plotModel(self, ax, region=None, color="k", prediction=False):
         """Plot the mean response with standard deviation as error bars.
 
         Parameters
         ----------
         ax : matplotlib axis
             the axis on which to plot the function
         region : str, optional
@@ -364,45 +420,49 @@
         color : str, optional
             the color to use in plotting the line
         shift : float, optional
             shift expressed as a fraction on [-0.5,0.5] used to shift
             the plotting of the errorbars so that multiple functions do
             not overlab
         """
-        key    = "default" if region is None else region
-        if key not in self.dist.keys(): self.buildResponse(region=region)
-        if self.dist[key][5] is None: return
-        p = self.dist[key][5]        
+        key = "default" if region is None else region
+        if key not in self.dist.keys():
+            self.buildResponse(region=region)
+        if self.dist[key][5] is None:
+            return
+        p = self.dist[key][5]
         i = self.dist[key][6]
         xedges = self.dist[key][1]
         yedges = self.dist[key][2]
-        x = np.linspace(xedges[0],xedges[-1],200)
-        y = np.polyval(p,x)
+        x = np.linspace(xedges[0], xedges[-1], 200)
+        y = np.polyval(p, x)
         if self.dep_log:
             y = 10**y
         xlabel = self.ind.name + " [%s]" % (UnitStringToMatplotlib(self.ind.unit))
         ylabel = self.dep.name + " [%s]" % (UnitStringToMatplotlib(self.dep.unit))
-        ax.plot(x,y,'-',color=color)
+        ax.plot(x, y, "-", color=color)
         if prediction:
             if self.dep_log:
-                yt = 10**(np.log10(y)-i)
-                ax.plot(x,10**(np.log10(y)-i),'--',color=color)
-                yt = 10**(np.log10(y)+i)
-                ax.plot(x,10**(np.log10(y)+i),'--',color=color)
+                yt = 10 ** (np.log10(y) - i)
+                ax.plot(x, 10 ** (np.log10(y) - i), "--", color=color)
+                yt = 10 ** (np.log10(y) + i)
+                ax.plot(x, 10 ** (np.log10(y) + i), "--", color=color)
             else:
-                ax.plot(x,y,'--',color=color)
-                ax.plot(x,y,'--',color=color)                
-        ax.set_xlabel(xlabel,fontsize = 12)
-        ax.set_ylabel(ylabel,fontsize = 12 if len(ylabel) <= 60 else 10)
-        ax.set_xlim(xedges[0],xedges[-1])
-        ax.set_ylim(yedges[0],yedges[-1])
-        if self.dep_log: ax.set_yscale('log')
-        if self.ind_log: ax.set_xscale('log')
-        
-    def scoreRMSE(self,r,region=None):
+                ax.plot(x, y, "--", color=color)
+                ax.plot(x, y, "--", color=color)
+        ax.set_xlabel(xlabel, fontsize=12)
+        ax.set_ylabel(ylabel, fontsize=12 if len(ylabel) <= 60 else 10)
+        ax.set_xlim(xedges[0], xedges[-1])
+        ax.set_ylim(yedges[0], yedges[-1])
+        if self.dep_log:
+            ax.set_yscale("log")
+        if self.ind_log:
+            ax.set_xscale("log")
+
+    def scoreRMSE(self, r, region=None):
         """Given another relationship, computes a RMSE score based on the mean functional responses.
 
         Parameter
         ---------
         r : Relationship
             a relationship to compare to this relationship
         region : str, optional
@@ -414,30 +474,30 @@
         S : float
             the relative RMSE error in the functional representations
             of the relationship, mapped to a score on the unit
             interval where higher values are better
 
         """
         key = "default" if region is None else region
-        self.makeComparable(r,region=region)
-        
+        self.makeComparable(r, region=region)
+
         # Compute the relative RMSE of the functions
         ref = self.dist[key][3].copy()
-        com = r   .dist[key][3].copy()
+        com = r.dist[key][3].copy()
         mask = ref.mask + com.mask
-        ref = np.ma.masked_array(ref.data,mask=mask).compressed()
-        com = np.ma.masked_array(com.data,mask=mask).compressed()
+        ref = np.ma.masked_array(ref.data, mask=mask).compressed()
+        com = np.ma.masked_array(com.data, mask=mask).compressed()
         if self.dep_log:
             ref = np.log10(ref)
             com = np.log10(com)
-        S = np.exp(-np.linalg.norm(ref-com)/np.linalg.norm(ref))
+        S = np.exp(-np.linalg.norm(ref - com) / np.linalg.norm(ref))
 
         return S
 
-    def scoreHellinger(self,r,region=None):
+    def scoreHellinger(self, r, region=None):
         """Given another relationship, computes the Hellenger score, which is 1 minus the Hellinger distance.
 
         Parameter
         ---------
         r : Relationship
             a relationship to compare to this relationship
         region : str, optional
@@ -449,19 +509,18 @@
         H : float
             the Hellinger distance, a measure of how well the
             relationship r approximates this relationship where small
             values are better
 
         """
         key = "default" if region is None else region
-        self.makeComparable(r,region=region)
-        
+        self.makeComparable(r, region=region)
+
         # Compute the Hellinger Distance
         ref = self.dist[key][0].copy()
-        com = r   .dist[key][0].copy()
+        com = r.dist[key][0].copy()
         mask = ref.mask + com.mask
-        ref = np.ma.masked_array(ref.data,mask=mask).compressed()
-        com = np.ma.masked_array(com.data,mask=mask).compressed()
-        H = 1-np.sqrt(((np.sqrt(ref)-np.sqrt(com))**2).sum())/np.sqrt(2)
+        ref = np.ma.masked_array(ref.data, mask=mask).compressed()
+        com = np.ma.masked_array(com.data, mask=mask).compressed()
+        H = 1 - np.sqrt(((np.sqrt(ref) - np.sqrt(com)) ** 2).sum()) / np.sqrt(2)
 
         return H
-
```

### Comparing `ILAMB-2.6/src/ILAMB/Scoreboard.py` & `ILAMB-2.7/src/ILAMB/Scoreboard.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,428 +1,574 @@
-from .Confrontation import Confrontation
-from .ConfNBP import ConfNBP
-from .ConfTWSA import ConfTWSA
-from .ConfRunoff import ConfRunoff
-from .ConfEvapFraction import ConfEvapFraction
-from .ConfIOMB import ConfIOMB
-from .ConfDiurnal import ConfDiurnal
-from .ConfPermafrost import ConfPermafrost
-from .ConfAlbedo import ConfAlbedo
-from .ConfSWE import ConfSWE
-from .ConfCO2 import ConfCO2
-from .ConfSoilCarbon import ConfSoilCarbon
-from .ConfUncertainty import ConfUncertainty
-from .ConfBurntArea import ConfBurntArea
-from .Regions import Regions
-import os,re
-from netCDF4 import Dataset
+import glob
+import importlib
+import json
+import os
+import re
+from copy import deepcopy
+
 import numpy as np
-from .Post import HarvestScalarDatabase,CreateJSON
-from .ilamblib import MisplacedData
-import glob,json
+from netCDF4 import Dataset
+
+import ILAMB
+from ILAMB.Confrontation import Confrontation
+from ILAMB.ilamblib import MisplacedData
+from ILAMB.Post import CreateJSON, HarvestScalarDatabase
+from ILAMB.Regions import Regions
+
+
+def get_confrontation_files():
+    """Return Confrontation child classes and their installed location."""
+    conf_files = {}
+    pkg_root = ILAMB.__path__[0]
+    for root, _, files in os.walk(pkg_root):
+        for file in files:
+            if not file.endswith(".py"):
+                continue
+            with open(os.path.join(root, file)) as fin:
+                match = re.search("class\s(.*)\(Confrontation\):", fin.read())
+                if match:
+                    conf_files[match.group(1)] = os.path.join(pkg_root, root, file)
+    return conf_files
+
+
+def dynamic_import(module_name, py_path):
+    """Import the modules given the full path."""
+    module_spec = importlib.util.spec_from_file_location(module_name, py_path)
+    module = importlib.util.module_from_spec(module_spec)
+    try:
+        module_spec.loader.exec_module(module)
+    except:
+        return None
+    return module.__dict__[module_name]
+
+
+# Dynamically import all confrontation types
+ConfrontationTypes = {
+    None: Confrontation,
+}
+for ctype, path in get_confrontation_files().items():
+    ConfrontationTypes[ctype] = dynamic_import(ctype, path)
 
-global_print_node_string  = ""
+global_print_node_string = ""
 global_confrontation_list = []
-global_model_list         = []
+global_model_list = []
 
-class Node(object):
 
+class Node(object):
     def __init__(self, name):
-        self.name                = name
-        self.children            = []
-        self.parent              = None
-        self.source              = None
-        self.cmap                = None
-        self.variable            = None
-        self.alternate_vars      = None
-        self.derived             = None
-        self.land                = False
-        self.confrontation       = None
-        self.output_path         = None
-        self.bgcolor             = "#EDEDED"
-        self.table_unit          = None
-        self.plot_unit           = None
-        self.space_mean          = True
-        self.relationships       = None
-        self.ctype               = None
-        self.regions             = None
-        self.skip_rmse           = False
-        self.skip_iav            = True
-        self.mass_weighting      = False
-        self.weight              = 1    # if a dataset has no weight specified, it is implicitly 1
-        self.sum_weight_children = 0    # what is the sum of the weights of my children?
-        self.normalize_weight    = 0    # my weight relative to my siblings
-        self.overall_weight      = 0    # the multiplication my normalized weight by all my parents' normalized weights
-        self.score               = 0    # placeholder
+        self.name = name
+        self.children = []
+        self.parent = None
+        self.source = None
+        self.cmap = None
+        self.variable = None
+        self.alternate_vars = None
+        self.derived = None
+        self.land = False
+        self.confrontation = None
+        self.output_path = None
+        self.bgcolor = "#EDEDED"
+        self.table_unit = None
+        self.plot_unit = None
+        self.space_mean = True
+        self.relationships = None
+        self.ctype = None
+        self.regions = None
+        self.skip_rmse = False
+        self.skip_iav = True
+        self.mass_weighting = False
+        self.weight = 1  # if a dataset has no weight specified, it is implicitly 1
+        self.sum_weight_children = 0  # what is the sum of the weights of my children?
+        self.normalize_weight = 0  # my weight relative to my siblings
+        self.overall_weight = 0  # the multiplication my normalized weight by all my parents' normalized weights
+        self.score = 0  # placeholder
 
     def __str__(self):
-        if self.parent is None: return ""
-        name   = self.name if self.name is not None else ""
+        if self.parent is None:
+            return ""
+        name = self.name if self.name is not None else ""
         weight = self.weight
+        depth = "%dm" % self.depth if "depth" in self.__dict__ else ""
         if self.isLeaf():
-            s = "%s%s %s" % ("   "*(self.getDepth()-1),name,self.score)
+            s = "%s%s %s" % ("   " * (self.getDepth() - 1), name, depth)
         else:
-            s = "%s%s %s" % ("   "*(self.getDepth()-1),name,self.score)
+            s = "%s%s %s" % ("   " * (self.getDepth() - 1), name, depth)
         return s
 
     def isLeaf(self):
-        if len(self.children) == 0: return True
+        if len(self.children) == 0:
+            return True
         return False
 
     def addChild(self, node):
         node.parent = self
         self.children.append(node)
 
     def getDepth(self):
-        depth  = 0
+        depth = 0
         parent = self.parent
         while parent is not None:
             depth += 1
             parent = parent.parent
         return depth
 
-def TraversePostorder(node,visit):
-    for child in node.children: TraversePostorder(child,visit)
+
+def TraversePostorder(node, visit):
+    for child in node.children:
+        TraversePostorder(child, visit)
     visit(node)
 
-def TraversePreorder(node,visit):
+
+def TraversePreorder(node, visit):
     visit(node)
-    for child in node.children: TraversePreorder(child,visit)
+    for child in node.children:
+        TraversePreorder(child, visit)
+
 
 def PrintNode(node):
     global global_print_node_string
     global_print_node_string += "%s\n" % (node)
 
+
 def ConvertTypes(node):
     def _to_bool(a):
-        if type(a) is type(True): return a
-        if type(a) is type("")  : return a.lower() == "true"
-    node.weight     = float(node.weight)
-    node.land       = _to_bool(node.land)
+        if type(a) is type(True):
+            return a
+        if type(a) is type(""):
+            return a.lower() == "true"
+
+    node.weight = float(node.weight)
+    node.land = _to_bool(node.land)
     node.space_mean = _to_bool(node.space_mean)
-    if node.regions        is not None: node.regions        = node.regions.split(",")
-    if node.relationships  is not None: node.relationships  = node.relationships.split(",")
+    if node.regions is not None:
+        node.regions = node.regions.split(",")
+    if node.relationships is not None:
+        node.relationships = node.relationships.split(",")
     if node.alternate_vars is not None:
         node.alternate_vars = node.alternate_vars.split(",")
     else:
         node.alternate_vars = []
 
+
 def SumWeightChildren(node):
-    for child in node.children: node.sum_weight_children += child.weight
+    for child in node.children:
+        node.sum_weight_children += child.weight
+
 
 def NormalizeWeights(node):
     if node.parent is not None:
-        sumw = 1.
-        if node.parent.sum_weight_children > 0: sumw = node.parent.sum_weight_children
-        node.normalize_weight = node.weight/sumw
+        sumw = 1.0
+        if node.parent.sum_weight_children > 0:
+            sumw = node.parent.sum_weight_children
+        node.normalize_weight = node.weight / sumw
+
 
 def OverallWeights(node):
     if node.isLeaf():
         node.overall_weight = node.normalize_weight
         parent = node.parent
         while parent.parent is not None:
             node.overall_weight *= parent.normalize_weight
             parent = parent.parent
 
+
 def InheritVariableNames(node):
-    if node.parent             is None: return
-    if node.variable           is None:  node.variable       = node.parent.variable
-    if node.derived            is None:  node.derived        = node.parent.derived
-    if node.cmap               is None:  node.cmap           = node.parent.cmap
-    if node.ctype              is None:  node.ctype          = node.parent.ctype
-    if node.skip_rmse          is False: node.skip_rmse      = node.parent.skip_rmse
-    if node.skip_iav           is False: node.skip_iav       = node.parent.skip_iav
-    if node.mass_weighting     is False: node.mass_weighting = node.parent.mass_weighting
+    if node.parent is None:
+        return
+    if node.variable is None:
+        node.variable = node.parent.variable
+    if node.derived is None:
+        node.derived = node.parent.derived
+    if node.cmap is None:
+        node.cmap = node.parent.cmap
+    if node.ctype is None:
+        node.ctype = node.parent.ctype
+    if node.skip_rmse is False:
+        node.skip_rmse = node.parent.skip_rmse
+    if node.skip_iav is False:
+        node.skip_iav = node.parent.skip_iav
+    if node.mass_weighting is False:
+        node.mass_weighting = node.parent.mass_weighting
     node.alternate_vars = node.parent.alternate_vars
 
+
+def ExpandDepths(node):
+    if node.getDepth() != 2:
+        return
+    if "depths" not in node.__dict__:
+        return
+    depths = [float(d) for d in node.__dict__["depths"].split(",")]
+
+    # we need to replace 'node' with a list of nodes
+    replace_node = []
+    for d in depths:
+        depth_node = deepcopy(node)
+        depth_node.__dict__.pop("depths")
+        depth_node.name += " %dm" % d
+        for c, _ in enumerate(depth_node.children):
+            depth_node.children[c].depth = d
+        replace_node.append(depth_node)
+
+    # now replace/expand
+    expansions = (replace_node if c == node else [c] for c in node.parent.children)
+    node.parent.children = [v for vals in expansions for v in vals]
+
+
 def ParseScoreboardConfigureFile(filename):
     root = Node(None)
     previous_node = root
     current_level = 0
     for line in open(filename).readlines():
         line = line.strip()
-        if line.startswith("#"): continue
-        m1 = re.search(r"\[h(\d):\s+(.*)\]",line)
-        m2 = re.search(r"\[(.*)\]",line)
-        m3 = re.search(r"(.*)=(.*)",line)
+        if line.startswith("#"):
+            continue
+        line = (
+            line[: line.index("#")] if ("#" in line and "bgcolor" not in line) else line
+        )
+        m1 = re.search(r"\[h(\d):\s+(.*)\]", line)
+        m2 = re.search(r"\[(.*)\]", line)
+        m3 = re.search(r"(.*)=(.*)", line)
         if m1:
             level = int(m1.group(1))
-            assert level-current_level<=1
-            name  = m1.group(2)
-            node  = Node(name)
-            if   level == current_level:
+            assert level - current_level <= 1
+            name = m1.group(2)
+            node = Node(name)
+            if level == current_level:
                 previous_node.parent.addChild(node)
-            elif level >  current_level:
+            elif level > current_level:
                 previous_node.addChild(node)
                 current_level = level
             else:
                 addto = root
-                for i in range(level-1): addto = addto.children[-1]
+                for i in range(level - 1):
+                    addto = addto.children[-1]
                 addto.addChild(node)
                 current_level = level
             previous_node = node
 
         if not m1 and m2:
-            node  = Node(m2.group(1))
+            node = Node(m2.group(1))
             previous_node.addChild(node)
 
         if m3:
             keyword = m3.group(1).strip()
-            value   = m3.group(2).strip().replace('"','')
+            value = m3.group(2).strip().replace('"', "")
             try:
                 node.__dict__[keyword] = value
             except:
                 pass
 
-    TraversePreorder (root,ConvertTypes)
-    TraversePostorder(root,SumWeightChildren)
-    TraversePreorder (root,NormalizeWeights)
-    TraversePreorder (root,OverallWeights)
-    TraversePostorder(root,InheritVariableNames)
+    TraversePreorder(root, ConvertTypes)
+    TraversePostorder(root, SumWeightChildren)
+    TraversePreorder(root, NormalizeWeights)
+    TraversePreorder(root, OverallWeights)
+    TraversePostorder(root, InheritVariableNames)
+    TraversePreorder(root, ExpandDepths)
     return root
 
-def getDict(node,scalars):
-    if node.name is None: return {}
+
+def getDict(node, scalars):
+    if node.name is None:
+        return {}
     n = node
     keys = []
     while n.parent is not None:
         keys.append(n.name)
         n = n.parent
     keys = keys[::-1]
     s = scalars
     for key in keys[:-1]:
-        s = s[key]['children']
+        s = s[key]["children"]
     return s[keys[-1]]
 
+
 def BuildDictionary(node):
     global scalars
-    if node.name is None: return
+    if node.name is None:
+        return
     n = node
     keys = []
     while n.parent is not None:
         keys.append(n.name)
         n = n.parent
     keys = keys[::-1]
     s = scalars
     for key in keys:
         if key not in s.keys():
             s[key] = {}
-            s[key]['children'] = {}
-        s = s[key]['children']
+            s[key]["children"] = {}
+        s = s[key]["children"]
+
 
 def BuildScalars(node):
-    if node.name is None: return
+    if node.name is None:
+        return
     global scalars
     global models
     global global_scores
     global section
-    s = getDict(node,scalars)
+    s = getDict(node, scalars)
     if node.isLeaf():
-        files = [f for f in glob.glob(os.path.join(node.output_path,"*.nc")) if "Benchmark" not in f]
+        files = [
+            f
+            for f in glob.glob(os.path.join(node.output_path, "*.nc"))
+            if "Benchmark" not in f
+        ]
         for fname in files:
             with Dataset(fname) as dset:
-                if dset.getncattr("name") not in models: continue
-                if section not in dset.groups: continue
+                if dset.getncattr("name") not in models:
+                    continue
+                if section not in dset.groups:
+                    continue
                 grp = dset.groups[section]["scalars"]
                 scores = [c for c in grp.variables.keys() if "Score" in c]
-                global_scores += [c for c in scores if ((c not in global_scores) and ("global" in c))]
+                global_scores += [
+                    c for c in scores if ((c not in global_scores) and ("global" in c))
+                ]
                 for c in scores:
                     if c not in s.keys():
-                        s[c] = np.ma.masked_array(np.zeros(len(models)),mask=np.ones(len(models),dtype=bool))
+                        s[c] = np.ma.masked_array(
+                            np.zeros(len(models)), mask=np.ones(len(models), dtype=bool)
+                        )
                     s[c][models.index(dset.getncattr("name"))] = grp[c][...]
     else:
         scores = None
         for child in node.children:
-            if scores is None: scores = [c for c in s['children'][child.name].keys() if "children" not in c]
+            if scores is None:
+                scores = [
+                    c for c in s["children"][child.name].keys() if "children" not in c
+                ]
             for c in scores:
                 if c not in s.keys():
-                    s[c] = np.ma.masked_array(np.zeros(len(models)),mask=np.zeros(len(models),dtype=bool))
-                if c in s['children'][child.name].keys():
-                    s[c] = s[c] + s['children'][child.name][c]*child.normalize_weight
+                    s[c] = np.ma.masked_array(
+                        np.zeros(len(models)), mask=np.zeros(len(models), dtype=bool)
+                    )
+                if c in s["children"][child.name].keys():
+                    s[c] = s[c] + s["children"][child.name][c] * child.normalize_weight
+
 
 def ConvertList(node):
-    if node.name is None: return
+    if node.name is None:
+        return
     global scalars
-    s = getDict(node,scalars)
+    s = getDict(node, scalars)
     for key in s.keys():
-        if key == "children": continue
+        if key == "children":
+            continue
         x = s[key]
         with np.errstate(under="ignore"):
-            x = (x-x.mean())/(x.std().clip(0.02) if x.std() > 1e-12 else 1)
+            x = (x - x.mean()) / (x.std().clip(0.02) if x.std() > 1e-12 else 1)
         x.data[x.mask] = -999
         s[key] = list(x.data)
 
-def CompositeScores(tree,M):
+
+def CompositeScores(tree, M):
     global global_model_list
     global_model_list = M
+
     def _loadScores(node):
         if node.isLeaf():
-            if node.confrontation is None: return
+            if node.confrontation is None:
+                return
             data = np.zeros(len(global_model_list))
-            mask = np.ones (len(global_model_list),dtype=bool)
-            for ind,m in enumerate(global_model_list):
-                fname = "%s/%s_%s.nc" % (node.confrontation.output_path,node.confrontation.name,m.name)
+            mask = np.ones(len(global_model_list), dtype=bool)
+            for ind, m in enumerate(global_model_list):
+                fname = "%s/%s_%s.nc" % (
+                    node.confrontation.output_path,
+                    node.confrontation.name,
+                    m.name,
+                )
                 if os.path.isfile(fname):
                     try:
                         dataset = Dataset(fname)
-                        grp     = dataset.groups["MeanState"].groups["scalars"]
+                        grp = dataset.groups["MeanState"].groups["scalars"]
                     except:
                         continue
                     if "Overall Score global" in grp.variables:
                         data[ind] = grp.variables["Overall Score global"][0]
                         mask[ind] = 0
                     else:
-                        data[ind] = -999.
+                        data[ind] = -999.0
                         mask[ind] = 1
-                    node.score = np.ma.masked_array(data,mask=mask)
+                    node.score = np.ma.masked_array(data, mask=mask)
         else:
-            node.score  = 0
+            node.score = 0
             sum_weights = 0
             for child in node.children:
-                node.score  += child.score*child.weight
+                node.score += child.score * child.weight
                 sum_weights += child.weight
-            np.seterr(over='ignore',under='ignore')
+            np.seterr(over="ignore", under="ignore")
             node.score /= sum_weights
-            np.seterr(over='raise',under='raise')
-    TraversePostorder(tree,_loadScores)
-    
-ConfrontationTypes = { None              : Confrontation,
-                       "ConfNBP"         : ConfNBP,
-                       "ConfTWSA"        : ConfTWSA,
-                       "ConfRunoff"      : ConfRunoff,
-                       "ConfEvapFraction": ConfEvapFraction,
-                       "ConfIOMB"        : ConfIOMB,
-                       "ConfDiurnal"     : ConfDiurnal,
-                       "ConfPermafrost"  : ConfPermafrost,
-                       "ConfAlbedo"      : ConfAlbedo,
-                       "ConfSWE"         : ConfSWE,
-                       "ConfCO2"         : ConfCO2,
-                       "ConfSoilCarbon"  : ConfSoilCarbon,
-                       "ConfUncertainty" : ConfUncertainty,
-                       "ConfBurntArea"   : ConfBurntArea}
+            np.seterr(over="raise", under="raise")
+
+    TraversePostorder(tree, _loadScores)
 
-class Scoreboard():
+
+class Scoreboard:
     """
     A class for managing confrontations
     """
-    def __init__(self,filename,regions=["global"],verbose=False,master=True,build_dir="./_build",extents=None,rel_only=False,mem_per_pair=100000.,run_title="ILAMB",rmse_score_basis="cycle"):
 
-        if 'ILAMB_ROOT' not in os.environ:
+    def __init__(
+        self,
+        filename,
+        regions=["global"],
+        verbose=False,
+        master=True,
+        build_dir="./_build",
+        extents=None,
+        rel_only=False,
+        mem_per_pair=100000.0,
+        run_title="ILAMB",
+        rmse_score_basis="cycle",
+        df_errs=None,
+    ):
+        if "ILAMB_ROOT" not in os.environ:
             raise ValueError("You must set the environment variable 'ILAMB_ROOT'")
         self.build_dir = build_dir
-        self.rel_only  = rel_only
+        self.rel_only = rel_only
         self.run_title = run_title
         self.regions = regions
         self.rmse_score_basis = rmse_score_basis
-        
-        if (master and not os.path.isdir(self.build_dir)): os.mkdir(self.build_dir)
+        self.df_errs = df_errs
+
+        if master and not os.path.isdir(self.build_dir):
+            os.mkdir(self.build_dir)
 
         self.tree = ParseScoreboardConfigureFile(filename)
         max_name_len = 45
 
         def _initConfrontation(node):
-            if not node.isLeaf(): return
-            
+            if not node.isLeaf():
+                return
+
             node.rmse_score_basis = self.rmse_score_basis
-            
+
             # if the user hasn't set regions, use the globally defined ones
-            if node.regions is None: node.regions = regions
+            if node.regions is None:
+                node.regions = regions
 
             # pick the confrontation to use, is it a built-in confrontation?
             if node.ctype in ConfrontationTypes:
                 Constructor = ConfrontationTypes[node.ctype]
+                if Constructor is None:
+                    raise ValueError(
+                        f"The confrontation {node.ctype} is nto available."
+                    )
             else:
                 # try importing the confrontation
                 conf = __import__(node.ctype)
                 Constructor = conf.__dict__[node.ctype]
 
             try:
-                if node.cmap is None: node.cmap = "jet"
-                node.source = os.path.join(os.environ["ILAMB_ROOT"],node.source if node.source else "")
-                node.mem_slab = mem_per_pair*0.5
+                if node.cmap is None:
+                    node.cmap = "jet"
+                node.source = os.path.join(
+                    os.environ["ILAMB_ROOT"], node.source if node.source else ""
+                )
+                node.mem_slab = mem_per_pair * 0.5
+                node.df_errs = self.df_errs
                 node.confrontation = Constructor(**(node.__dict__))
-                node.confrontation.cweight = node.weight*node.parent.weight
+                node.confrontation.cweight = node.weight * node.parent.weight
                 node.confrontation.extents = extents
 
-                if verbose and master: print(("    {0:>%d}\033[92m Initialized\033[0m" % max_name_len).format(node.confrontation.longname))
+                if verbose and master:
+                    print(
+                        (
+                            "    {0:>%d}\033[92m Initialized\033[0m" % max_name_len
+                        ).format(node.confrontation.longname)
+                    )
 
             except MisplacedData:
-
-                if (master and verbose):
+                if master and verbose:
                     longname = node.output_path
-                    longname = longname.replace("//","/").replace(self.build_dir,"")
-                    if longname[-1] == "/": longname = longname[:-1]
+                    longname = longname.replace("//", "/").replace(self.build_dir, "")
+                    if longname[-1] == "/":
+                        longname = longname[:-1]
                     longname = "/".join(longname.split("/")[1:])
-                    print(("    {0:>%d}\033[91m MisplacedData\033[0m" % max_name_len).format(longname))
+                    print(
+                        (
+                            "    {0:>%d}\033[91m MisplacedData\033[0m" % max_name_len
+                        ).format(longname)
+                    )
 
         def _buildDirectories(node):
-            if node.name is None: return
-            path   = ""
+            if node.name is None:
+                return
+            path = ""
             parent = node
             while parent.name is not None:
-                path   = os.path.join(parent.name.replace(" ",""),path)
+                path = os.path.join(parent.name.replace(" ", ""), path)
                 parent = parent.parent
-            path = os.path.join(self.build_dir,path)
-            if not os.path.isdir(path) and master: os.mkdir(path)
+            path = os.path.join(self.build_dir, path)
+            if not os.path.isdir(path) and master:
+                os.mkdir(path)
             node.output_path = path
 
-        TraversePreorder(self.tree,_buildDirectories)
-        TraversePreorder(self.tree,_initConfrontation)
+        TraversePreorder(self.tree, _buildDirectories)
+        TraversePreorder(self.tree, _initConfrontation)
 
     def __str__(self):
         global global_print_node_string
         global_print_node_string = ""
-        TraversePreorder(self.tree,PrintNode)
+        TraversePreorder(self.tree, PrintNode)
         return global_print_node_string
 
     def list(self):
         def _hasConfrontation(node):
             global global_confrontation_list
             if node.confrontation is not None:
                 global_confrontation_list.append(node.confrontation)
+
         global global_confrontation_list
         global_confrontation_list = []
-        TraversePreorder(self.tree,_hasConfrontation)
+        TraversePreorder(self.tree, _hasConfrontation)
         return global_confrontation_list
 
-    def createJSON(self,M,filename="scalars.json"):
-
+    def createJSON(self, M, filename="scalars.json"):
         global scalars
         global models
         global global_scores
         global section
-        rel_tree = GenerateRelationshipTree(self,M)
+        rel_tree = GenerateRelationshipTree(self, M)
         global_scores = []
-        models  = [m.name for m in M]
+        models = [m.name for m in M]
         scalars = {}
-        TraversePreorder (self.tree,BuildDictionary)
-        section = "MeanState"    ; TraversePostorder(self.tree,BuildScalars)
-        TraversePreorder (self.tree,ConvertList)
+        TraversePreorder(self.tree, BuildDictionary)
+        section = "MeanState"
+        TraversePostorder(self.tree, BuildScalars)
+        TraversePreorder(self.tree, ConvertList)
         check = rel_tree.children
-        if len(check) > 0: check = check[0]
+        if len(check) > 0:
+            check = check[0]
         if len(check.children) > 0:
-            TraversePreorder(rel_tree,BuildDictionary)
-            section = "Relationships"; TraversePostorder(rel_tree,BuildScalars)
-            TraversePreorder(rel_tree,ConvertList)
-        with open(os.path.join(self.build_dir,filename),mode='w') as f:
+            TraversePreorder(rel_tree, BuildDictionary)
+            section = "Relationships"
+            TraversePostorder(rel_tree, BuildScalars)
+            TraversePreorder(rel_tree, ConvertList)
+        with open(os.path.join(self.build_dir, filename), mode="w") as f:
             json.dump(scalars, f)
-        return global_scores,rel_tree
-        
-    def createHtml(self,M,filename="index.html"):
+        return global_scores, rel_tree
+
+    def createHtml(self, M, filename="index.html"):
         global models
         from ILAMB.generated_version import version as ilamb_version
+
         r = Regions()
-        run_title = "ILAMB Benchmarking" if self.run_title is None else self.run_title[0] 
+        run_title = (
+            "ILAMB Benchmarking" if self.run_title is None else self.run_title[0]
+        )
         models = [m.name for m in M]
         maxM = max([len(m) for m in models])
-        px = int(round(maxM*6.875))
-        if px % 2 == 1: px += 1
-        py = int(px/2)-5
-        scores,rel_tree = self.createJSON(M)
-        scores = [s.replace(" global","") for s in scores if " global" in s]
+        px = int(round(maxM * 6.875))
+        if px % 2 == 1:
+            px += 1
+        py = int(px / 2) - 5
+        scores, rel_tree = self.createJSON(M)
+        scores = [s.replace(" global", "") for s in scores if " global" in s]
         html = """
 <html>
   <head>
     <meta name="viewport" content="width=device-width, initial-scale=1">
     <link rel="stylesheet" href="https://code.jquery.com/mobile/1.4.5/jquery.mobile-1.4.5.min.css">
     <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
     <script src="https://code.jquery.com/mobile/1.4.5/jquery.mobile-1.4.5.min.js"></script>
@@ -451,45 +597,45 @@
 		      if(!($row.next().is(":hidden"))) children.push($row.next());
 		  }
 		  $row = $row.next();
 	      }
 	      return children;
 	  }
 	  $('.parent').on('click', function() {
-	      var children = getH1Children($(this));	 
+	      var children = getH1Children($(this));
 	      $.each(children, function() {
 		  $(this).toggle();
 	      })
 	  });
 	  $('.child_variable').on('click', function() {
 	      var children = getH2Children($(this));
 	      $.each(children, function() {
 		  $(this).toggle();
 	      })
 	  });
 	  $('.child_dataset').toggle();
       });
 
       function pageLoad() {
-	  
+
 	  $("table").delegate('td','mouseover mouseleave', function(e) {
 	      var table = document.getElementById("scoresTable");
 	      if (e.type == 'mouseover') {
 		  $(this).parent().addClass("hover");
 		  table.rows[0].cells[$(this).index()].style.fontWeight = "bolder";
 	      }
 	      else {
 		  $(this).parent().removeClass("hover");
 		  table.rows[0].cells[$(this).index()].style.fontWeight = "normal";
 	      }
 	  });
 
 	  colorTable();
       }
-      
+
       function printRow(table,row,array,cmap) {
 	  if(typeof array == 'undefined'){
 	      for(var i = 1, col; col = table.rows[row].cells[i]; i++) {
 		  col.style.backgroundColor = "#808080";
 	      }
 	      return;
 	  }
@@ -506,24 +652,24 @@
                   }
 		  ind = Math.min(Math.max(ind,0),nc-1);
 		  clr = cmap[ind];
 	      }
 	      table.rows[row].cells[col+1].style.backgroundColor = clr;
 	  }
       }
-      
+
       function colorTable() {
-	  	  
+
         $.getJSON("scalars.json", function(data) {
           var scalars = data;
 	  var scalar_option = document.getElementById("ScalarOption");
           var region_option = document.getElementById("RegionOption");
 	  var scalar_name   = scalar_option.options[scalar_option.selectedIndex].value;
 	  scalar_name  += " " + region_option.options[region_option.selectedIndex].value;
-	  
+
 	  var PuOr = ['#b35806','#e08214','#fdb863','#fee0b6','#f7f7f7','#d8daeb','#b2abd2','#8073ac','#542788'];
 	  var GnRd = ['#b2182b','#d6604d','#f4a582','#fddbc7','#f7f7f7','#d9f0d3','#a6dba0','#5aae61','#1b7837'];
 	  var cmap = GnRd;
 	  if(document.getElementById("colorblind").checked) cmap = PuOr;
 
 	  var row = 1;
 	  var tab = "";
@@ -535,29 +681,28 @@
 	      for(let h2 in H1){
 		  printRow(table,row,H1[h2][scalar_name],cmap);
 		  row += 1;
 		  H2 = H1[h2]["children"]
 		  for(let v in H2){
 	              var s_name = scalar_name;
                       if(h1 == "Relationships") {
-                        s_name = v.substring(0,v.indexOf("/")) + " Score " + region_option.options[region_option.selectedIndex].value;
-                      }else{
+                        s_name = v.replace("/","|") + " Score " + region_option.options[region_option.selectedIndex].value;
                       }
 		      printRow(table,row,H2[v][s_name],cmap);
 		      row += 1;
 		  }
 	      }
 	  }
 
 	  table = document.getElementById("scoresLegend");
 	  row = 0;
 	  for(var col=0;col<cmap.length;col++){
 	      table.rows[row].cells[col].style.backgroundColor = cmap[col];
 	  }
-	});  
+	});
       }
     </script>
     <style type="text/css">
       .parent{
       }
       .child_variable{
       }
@@ -590,132 +735,168 @@
 	  text-decoration: none;
       }
       .hover {
 	  font-weight: bold;
           border: 2px solid;
       }
     </style>
-    
+
   </head>
   <body onload="pageLoad()">
 
-    <div data-role="page" id="MeanState">      
+    <div data-role="page" id="MeanState">
       <div data-role="header" data-position="fixed" data-tap-toggle="false">
         <h1>%s</h1>
       </div>
 
-      <select id="ScalarOption" onchange="colorTable()">""" % (px,py,run_title)
+      <select id="ScalarOption" onchange="colorTable()">""" % (
+            px,
+            py,
+            run_title,
+        )
 
         for s in scores:
-            opts  = ' selected="selected"' if "Overall" in s else ''
+            opts = ' selected="selected"' if "Overall" in s else ""
             html += """
-        <option value="%s"%s>%s</option>""" % (s,opts,s)
+        <option value="%s"%s>%s</option>""" % (
+                s,
+                opts,
+                s,
+            )
         html += """
       </select>
       <select id="RegionOption" onchange="colorTable()">"""
 
-
         for region in self.regions:
             try:
                 rname = r.getRegionName(region)
             except:
                 rname = region
-            opts  = ''
+            opts = ""
             if region == "global" or len(self.regions) == 1:
-                opts  = ' selected="selected"'
+                opts = ' selected="selected"'
             html += """
-          <option value='%s'%s>%s</option>""" % (region,opts,rname)        
+          <option value='%s'%s>%s</option>""" % (
+                region,
+                opts,
+                rname,
+            )
         html += """
       </select>
 
       <form>
 	<fieldset data-role="controlgroup" data-type="horizontal" data-mini="True">
 	  <input type="checkbox" name="colorblind" id="colorblind" checked onchange="colorTable()">
 	  <label for="colorblind" >Colorblind colors</label>
 	</fieldset>
       </form>
-      
+
       <center>
 	<table class="table-header-rotated" id="scoresTable">
 	  <thead>
             <tr>
               <th></th>"""
-        
+
         for m in M:
             html += """
-              <th class="rotate"><div>%s</div></th>""" % (m.name)
+              <th class="rotate"><div>%s</div></th>""" % (
+                m.name
+            )
         html += """
             </tr>
 	  </thead>
 	  <tbody>"""
 
-
         global global_html
         global row_color
         global_html = ""
         row_color = ""
-            
+
         def GenRowHTML(node):
-            row_class = ['','parent','child_variable','child_dataset']
+            row_class = ["", "parent", "child_variable", "child_dataset"]
             global global_html
             global row_color
             global models
             global global_sb
             d = node.getDepth()
-            if d == 0: return
-            if d == 1: row_color = node.bgcolor
-            row_header = "%s" % (";".join(["&nbsp"]*(4*(d-1))))
-            if len(row_header) > 0: row_header += ";"
+            if d == 0:
+                return
+            if d == 1:
+                row_color = node.bgcolor
+            row_header = "%s" % (";".join(["&nbsp"] * (4 * (d - 1))))
+            if len(row_header) > 0:
+                row_header += ";"
             row_header += node.name
             if d == 3:
                 if node.parent.parent.name == "Relationships":
-                    path = node.output_path.replace(global_sb.build_dir,"")
-                    html = node.output_path.replace(global_sb.build_dir,"")
-                    if html.endswith("/"): html = html[:-1]
+                    path = node.output_path.replace(global_sb.build_dir, "")
+                    html = node.output_path.replace(global_sb.build_dir, "")
+                    if html.endswith("/"):
+                        html = html[:-1]
                     html = html.split("/")[-1]
-                    row_link = "./%s/%s.html#Relationships" % (path,html)
-                    row_link = row_link.replace("//","/")
+                    row_link = "./%s/%s.html#Relationships" % (path, html)
+                    row_link = row_link.replace("//", "/")
                 else:
-                    row_link = "./%s/%s/%s/%s.html" % (node.parent.parent.name.replace(" ",""),
-                                                       node.parent.name.replace(" ",""),
-                                                       node.name.replace(" ",""),
-                                                       node.name.replace(" ",""))
-                row_header = '<a href="%s" target="_blank">%s</a>' % (row_link,row_header)
-            
+                    row_link = "./%s/%s/%s/%s.html" % (
+                        node.parent.parent.name.replace(" ", ""),
+                        node.parent.name.replace(" ", ""),
+                        node.name.replace(" ", ""),
+                        node.name.replace(" ", ""),
+                    )
+                row_header = '<a href="%s" target="_blank">%s</a>' % (
+                    row_link,
+                    row_header,
+                )
+
             global_html += """
 	    <tr class="%s" bgcolor="%s">
-              <td class="row-label">%s</td>""" % (row_class[d],row_color,row_header)
+              <td class="row-label">%s</td>""" % (
+                row_class[d],
+                row_color,
+                row_header,
+            )
             for m in models:
                 if d < 3:
-                    href = ''
+                    href = ""
                 else:
-                    path = node.output_path.replace(global_sb.build_dir,"")
+                    path = node.output_path.replace(global_sb.build_dir, "")
                     if "/" in node.name:
-                        fname = node.output_path[:-1] if node.output_path.endswith("/") else node.output_path
+                        fname = (
+                            node.output_path[:-1]
+                            if node.output_path.endswith("/")
+                            else node.output_path
+                        )
                         fname = fname.split("/")[-1]
-                        path = os.path.join(path,"%s.html#Relationships" % (fname))                        
+                        path = os.path.join(path, "%s.html#Relationships" % (fname))
                     else:
-                        path = os.path.join(path,"%s.html" % (node.name))
-                    if path.startswith("/"): path = path[1:]
-                    href = '<a href="%s?model=%s" target="_blank">&nbsp;</a>' % (path,m)
-
-                global_html += """
-              <td>%s</td>""" % href
+                        path = os.path.join(path, "%s.html" % (node.name))
+                    if path.startswith("/"):
+                        path = path[1:]
+                    href = '<a href="%s?model=%s" target="_blank">&nbsp;</a>' % (
+                        path,
+                        m,
+                    )
+
+                global_html += (
+                    """
+              <td>%s</td>"""
+                    % href
+                )
 
         global global_sb
         global_sb = self
-        TraversePreorder(self.tree,GenRowHTML)
+        TraversePreorder(self.tree, GenRowHTML)
         if rel_tree.children[0].children:
-            TraversePreorder(rel_tree,GenRowHTML)
+            TraversePreorder(rel_tree, GenRowHTML)
         html += global_html
         html += """
 	  </tbody>
 	</table>
 
-	
+
 	<p>Relative Scale
 	<table class="table-header-rotated" id="scoresLegend">
 	  <tbody>
             <tr>
               <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
 	    </tr>
 	  </tbody>
@@ -730,35 +911,39 @@
 	</table>Missing Data or Error
       </center>
 
       <div data-role="footer">
         <center>ILAMB %s</center>
       </div>
     </body>
-</html>""" % (ilamb_version)
+</html>""" % (
+            ilamb_version
+        )
 
-        with open("%s/%s" % (self.build_dir,filename),"w") as f:
+        with open("%s/%s" % (self.build_dir, filename), "w") as f:
             f.write(html)
 
-    def dumpScores(self,M,filename):
-        CompositeScores(self.tree,M)
-        with open("%s/%s" % (self.build_dir,filename),"w") as out:
+    def dumpScores(self, M, filename):
+        CompositeScores(self.tree, M)
+        with open("%s/%s" % (self.build_dir, filename), "w") as out:
             out.write("Variables,%s\n" % (",".join([m.name for m in M])))
             for cat in self.tree.children:
                 for v in cat.children:
                     try:
-                        out.write("%s,%s\n" % (v.name,','.join([str(s) for s in v.score])))
+                        out.write(
+                            "%s,%s\n" % (v.name, ",".join([str(s) for s in v.score]))
+                        )
                     except:
-                        out.write("%s,%s\n" % (v.name,','.join(["~"]*len(M))))
+                        out.write("%s,%s\n" % (v.name, ",".join(["~"] * len(M))))
 
-    def harvestInformation(self,M):
+    def harvestInformation(self, M):
         HarvestScalarDatabase(self.build_dir)
-        CreateJSON(os.path.join(self.build_dir,"scalar_database.csv"),M)
+        CreateJSON(os.path.join(self.build_dir, "scalar_database.csv"), M)
 
-    def createUDDashboard(self,filename="dashboard.html"):
+    def createUDDashboard(self, filename="dashboard.html"):
         html = """
 <!DOCTYPE html>
 <html lang="en">
 <head>
    <meta charset="utf-8" />
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/climatemodeling/unified-dashboard@1.0.0/dist/js/lmtud_bundle.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/climatemodeling/unified-dashboard@1.0.0/dist/css/lmtud_bundle.min.css">
@@ -804,35 +989,35 @@
              <span class="el-checkbox-style  pull-right"></span>
           </label>
           <label class="el-checkbox el-checkbox-sm">
              <span class="margin-r">Column</span>
              <input type="checkbox" class="scacol" value='scacol' id="checkboxsca">
              <span class="el-checkbox-style  pull-right"></span>
           </label>
-          <select class="select-choice-sca" id="select-choice-mini-sca" style="width:75%"> 
+          <select class="select-choice-sca" id="select-choice-mini-sca" style="width:75%">
              <option value="0" selected> Not normalized </option>
              <option value="1"> Normalized [x-mean(x)]/sigma(x) </option>
              <option value="2"> Normalized [-1:1] </option>
              <option value="3"> Normalized [ 0:1] </option>
           </select>
-          <select class="select-choice-map" id="select-choice-mini-map" style="width:75%"> 
+          <select class="select-choice-map" id="select-choice-mini-map" style="width:75%">
              <option value="0" selected> ILAMB color mapping </option>
              <option value="1"> Linear color mapping </option>
              <option value="2"> Linear color mapping reverse </option>
           </select>
       </section>
       <hr>
       <section class="menu-section">
           <h3 class="menu-section-title">Examples</h3>
-          <select class="select-choice-ex" id="select-choice-mini-ex" style="width:75%"> 
+          <select class="select-choice-ex" id="select-choice-mini-ex" style="width:75%">
              <option value="cmec_ilamb_example_addsource.json"> CMEC ILAMB</option>
              <option value="pmp_enso_tel.json"> CMEC PMP</option>
           </select>
           <h3 class="menu-section-title">Logo</h3>
-          <select class="select-choice-logo" id="select-choice-mini-logo" style="width:75%"> 
+          <select class="select-choice-logo" id="select-choice-mini-logo" style="width:75%">
              <option value="rubisco_logo.png"> RUBISCO</option>
              <option value="cmec_logo.png"> CMEC</option>
              <option value="pmp_logo.png"> PMP</option>
              <option value="lmt-logo.png"> LMT</option>
           </select>
       </section>
       <section class="menu-section">
@@ -875,15 +1060,15 @@
     <main id="panel" class="panel">
       <header class="panel-header">
         <!--button class="btn-hamburger js-slideout-toggle"></button-->
         <span id="sidemenuicon" class="js-slideout-toggle">&#9776&nbsp;Menu</span>
         <h1 class="title">LMT Unified Dashboard</h1>
       </header>
       <section style="text-align:center">
-        <input name="file" id="file" type="file" onchange="loadlocJson()"/> 
+        <input name="file" id="file" type="file" onchange="loadlocJson()"/>
       </section>
       <section>
         <div class="tabDiv" id="mytab">
           <div id="dashboard-table"></div>
         </div>
         <center>
             <div class="legDiv">
@@ -905,35 +1090,36 @@
             </table>Missing Data or Error
             </div>
         </center>
       </section>
     </main>
 </body>
 </html>"""
-        with open(os.path.join(self.build_dir,filename           ),"w") as f: f.write(html)
-        with open(os.path.join(self.build_dir,"_lmtUDConfig.json"),'w') as f:
-            json.dump({
-                "udcJsonLoc": "scalar_database.json",
-                "udcDimSets": {
-                    "x_dim": "model",
-                    "y_dim": "metric",
-                    "fxdim": {
-                        "region": "global",
-                        "statistic": "Overall Score"
-                    }
+        with open(os.path.join(self.build_dir, filename), "w") as f:
+            f.write(html)
+        with open(os.path.join(self.build_dir, "_lmtUDConfig.json"), "w") as f:
+            json.dump(
+                {
+                    "udcJsonLoc": "scalar_database.json",
+                    "udcDimSets": {
+                        "x_dim": "model",
+                        "y_dim": "metric",
+                        "fxdim": {"region": "global", "statistic": "Overall Score"},
+                    },
+                    "udcScreenHeight": 0,
+                    "udcCellValue": 1,
+                    "udcNormType": "standarized",
+                    "udcNormAxis": "row",
+                    "logofile": "None",
                 },
-                "udcScreenHeight": 0,
-                "udcCellValue": 0,
-                "logofile": "None"
-            },f)
-        
+                f,
+            )
 
-        
-def GenerateRelationshipTree(S,M):
 
+def GenerateRelationshipTree(S, M):
     # Create a tree which mimics the scoreboard for relationships, but
     # we need
     #
     # root -> category -> datasets -> relationships
     #
     # instead of
     #
@@ -941,33 +1127,37 @@
     #
     rel_tree = Node(None)
     h1 = None
     for cat in S.tree.children:
         if h1 is None:
             h1 = Node("Relationships")
             h1.bgcolor = "#fff2e5"
-            h1.parent  = rel_tree
-            h1.normalize_weight = 1.
+            h1.parent = rel_tree
+            h1.normalize_weight = 1.0
             rel_tree.children.append(h1)
         for var in cat.children:
             for data in var.children:
-                if data               is None: continue
-                if data.relationships is None: continue
+                if data is None:
+                    continue
+                if data.confrontation is None:
+                    continue
+                if data.relationships is None:
+                    continue
 
                 # build tree
                 h2 = Node(data.confrontation.longname)
                 h1.children.append(h2)
                 h2.parent = h1
-                h2.normalize_weight = 1.
+                h2.normalize_weight = 1.0
                 h2.bgcolor = h1.bgcolor
                 for rel in data.relationships:
                     try:
                         longname = rel.longname
                     except:
                         longname = rel
                     v = Node(longname)
                     h2.children.append(v)
                     v.parent = h2
-                    v.normalize_weight = 1./len(data.relationships)
+                    v.normalize_weight = 1.0 / len(data.relationships)
                     v.bgcolor = h2.bgcolor
                     v.output_path = data.output_path
     return rel_tree
```

### Comparing `ILAMB-2.6/src/ILAMB/Variable.py` & `ILAMB-2.7/src/ILAMB/Variable.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,45 +1,53 @@
-from .constants import mid_months,bnd_months
-from .Regions import Regions
-import matplotlib.colors as colors
 import cartopy.crs as ccrs
 import cartopy.feature as cfeature
-from pylab import get_cmap
-from cf_units import Unit
-from . import ilamblib as il
-from . import Post as post
-import numpy as np
+import matplotlib.colors as colors
 import matplotlib.pyplot as plt
+import numpy as np
+from cf_units import Unit
+from pylab import get_cmap
+
+from ILAMB import ilamblib as il
+from ILAMB.constants import bnd_months, mid_months
+from ILAMB.Regions import Regions
+
 
 def _shiftLon(lon):
-    return (lon<=180)*lon + (lon>180)*(lon-360) + (lon<-180)*360
+    return (lon <= 180) * lon + (lon > 180) * (lon - 360) + (lon < -180) * 360
+
 
-def _shiftFirstColumnToDateline(lon,lon_bnds=None,data=None,area=None):
+def _shiftFirstColumnToDateline(lon, lon_bnds=None, data=None, area=None):
     shift = lon.argmin()
-    lon = np.roll(lon,-shift)
+    lon = np.roll(lon, -shift)
     if lon_bnds is not None:
-        lon_bnds = np.roll(lon_bnds,-shift,axis=0)
-        if lon_bnds[ 0,0] > lon_bnds[ 0,1]: lon_bnds[ 0,0] = max(lon_bnds[ 0,0]-360,-180)
-        if lon_bnds[-1,1] < lon_bnds[-1,0]: lon_bnds[-1,1] = min(lon_bnds[-1,1]+360,+180)
-    if data is not None: data = np.roll(data,-shift,axis=-1)
-    if area is not None: area = np.roll(area,-shift,axis=-1)
-    return lon,lon_bnds,data,area
+        lon_bnds = np.roll(lon_bnds, -shift, axis=0)
+        if lon_bnds[0, 0] > lon_bnds[0, 1]:
+            lon_bnds[0, 0] = max(lon_bnds[0, 0] - 360, -180)
+        if lon_bnds[-1, 1] < lon_bnds[-1, 0]:
+            lon_bnds[-1, 1] = min(lon_bnds[-1, 1] + 360, +180)
+    if data is not None:
+        data = np.roll(data, -shift, axis=-1)
+    if area is not None:
+        area = np.roll(area, -shift, axis=-1)
+    return lon, lon_bnds, data, area
+
 
 def _createBnds(x):
-    x      = np.asarray(x)
-    x_bnds = np.zeros((x.size,2))
-    x_bnds[+1:,0] = 0.5*(x[:-1]+x[+1:])
-    x_bnds[:-1,1] = 0.5*(x[:-1]+x[+1:])
+    x = np.asarray(x)
+    x_bnds = np.zeros((x.size, 2))
+    x_bnds[+1:, 0] = 0.5 * (x[:-1] + x[+1:])
+    x_bnds[:-1, 1] = 0.5 * (x[:-1] + x[+1:])
     if x.size == 1:
-        x_bnds[ ...] = x
+        x_bnds[...] = x
     else:
-        x_bnds[ 0,0] = x[ 0] - 0.5*(x[ 1]-x[ 0])
-        x_bnds[-1,1] = x[-1] + 0.5*(x[-1]-x[-2])
+        x_bnds[0, 0] = x[0] - 0.5 * (x[1] - x[0])
+        x_bnds[-1, 1] = x[-1] + 0.5 * (x[-1] - x[-2])
     return x_bnds
-        
+
+
 class Variable:
     r"""A class for managing variables and their analysis.
 
     There are two ways to create a Variable object. Because python
     does not support multiple constructors, we will use keyword
     arguments so that the users intent may be captured. The first way
     to specify a Variable is by loading a netCDF4 file. You can
@@ -91,193 +99,238 @@
     >>> v = Variable(name="some_variable",unit="some_unit",lat=lat,lon=lon,data=data)
 
     Or you can initiate a variable by extracting a specific field from a netCDF4 file.
 
     >>> v = Variable(filename="some_netcdf_file.nc",variable_name="name_of_var_to_extract")
 
     """
-    def __init__(self,**keywords):
-        r"""Constructor for the variable class by specifying the data arrays.
-        """
+
+    def __init__(self, **keywords):
+        r"""Constructor for the variable class by specifying the data arrays."""
         # See if the user specified a netCDF4 file and variable
-        filename       = keywords.get("filename"     ,None)
-        groupname      = keywords.get("groupname"    ,None)
-        variable_name  = keywords.get("variable_name",None)
-        alternate_vars = keywords.get("alternate_vars",[])
-        calendar       = "noleap"
-        if filename is None: # if not pull data from other arguments
-            data       = keywords.get("data"       ,None)
-            data_bnds  = keywords.get("data_bnds"  ,None)
-            unit       = keywords.get("unit"       ,None)
-            name       = keywords.get("name"       ,"unnamed")
-            time       = keywords.get("time"       ,None)
-            time_bnds  = keywords.get("time_bnds"  ,None)
-            lat        = keywords.get("lat"        ,None)
-            lat_bnds   = keywords.get("lat_bnds"   ,None)
-            lon        = keywords.get("lon"        ,None)
-            lon_bnds   = keywords.get("lon_bnds"   ,None)
-            depth      = keywords.get("depth"      ,None)
-            depth_bnds = keywords.get("depth_bnds" ,None)
-            ndata      = keywords.get("ndata"      ,None)
-            attr       = None
+        filename = keywords.get("filename", None)
+        groupname = keywords.get("groupname", None)
+        variable_name = keywords.get("variable_name", None)
+        alternate_vars = keywords.get("alternate_vars", [])
+        calendar = "noleap"
+        if filename is None:  # if not pull data from other arguments
+            data = keywords.get("data", None)
+            data_bnds = keywords.get("data_bnds", None)
+            unit = keywords.get("unit", None)
+            name = keywords.get("name", "unnamed")
+            time = keywords.get("time", None)
+            time_bnds = keywords.get("time_bnds", None)
+            lat = keywords.get("lat", None)
+            lat_bnds = keywords.get("lat_bnds", None)
+            lon = keywords.get("lon", None)
+            lon_bnds = keywords.get("lon_bnds", None)
+            depth = keywords.get("depth", None)
+            depth_bnds = keywords.get("depth_bnds", None)
+            ndata = keywords.get("ndata", None)
+            attr = None
             assert data is not None
             assert unit is not None
-            cbounds    = None
+            cbounds = None
             self.filename = ""
         else:
             self.filename = filename
             assert variable_name is not None
-            t0 = keywords.get("t0",None)
-            tf = keywords.get("tf",None)
-            convert_calendar = keywords.get("convert_calendar",True)
-            out = il.FromNetCDF4(filename,variable_name,alternate_vars,t0,tf,group=groupname,convert_calendar=convert_calendar)
-            data,data_bnds,unit,name,time,time_bnds,lat,lat_bnds,lon,lon_bnds,depth,depth_bnds,cbounds,ndata,calendar,attr = out
+            t0 = keywords.get("t0", None)
+            tf = keywords.get("tf", None)
+            convert_calendar = keywords.get("convert_calendar", True)
+            out = il.FromNetCDF4(
+                filename,
+                variable_name,
+                alternate_vars,
+                t0,
+                tf,
+                group=groupname,
+                convert_calendar=convert_calendar,
+            )
+            (
+                data,
+                data_bnds,
+                unit,
+                name,
+                time,
+                time_bnds,
+                lat,
+                lat_bnds,
+                lon,
+                lon_bnds,
+                depth,
+                depth_bnds,
+                cbounds,
+                ndata,
+                calendar,
+                attr,
+            ) = out
 
         # Add handling for some units which cf_units does not support
-        unit = unit.replace("psu","1e-3")
+        unit = unit.replace("psu", "1e-3")
 
-        if not np.ma.isMaskedArray(data): data = np.ma.masked_array(data)
-        self.data  = data
+        if not np.ma.isMaskedArray(data):
+            data = np.ma.masked_array(data)
+        self.data = data
         self.data_bnds = data_bnds
         self.ndata = ndata
-        self.unit  = unit
-        self.name  = name
+        self.unit = unit
+        self.name = name
         self.cbounds = cbounds
         self.calendar = calendar
         self.attr = attr
-        
+
         # Handle time data
-        self.time      = time      # time data
-        self.time_bnds = time_bnds # bounds on time
-        self.temporal  = False     # flag for temporal data
-        self.dt        = 0.        # mean temporal spacing
-        self.monthly   = False     # flag for monthly means
+        self.time = time  # time data
+        self.time_bnds = time_bnds  # bounds on time
+        self.temporal = False  # flag for temporal data
+        self.dt = 0.0  # mean temporal spacing
+        self.monthly = False  # flag for monthly means
         if time is not None:
             self.temporal = True
-            if self.time_bnds is None: self.time_bnds = _createBnds(self.time)
-            self.dt = (self.time_bnds[:,1]-self.time_bnds[:,0]).mean()
-            if np.allclose(self.dt,30,atol=3): self.monthly = True
-            assert (2*self.time.size) == (self.time_bnds.size)
+            if self.time_bnds is None:
+                self.time_bnds = _createBnds(self.time)
+            self.dt = (self.time_bnds[:, 1] - self.time_bnds[:, 0]).mean()
+            if np.allclose(self.dt, 30, atol=3):
+                self.monthly = True
+            assert (2 * self.time.size) == (self.time_bnds.size)
 
         # Handle space or multimember data
-        self.spatial  = False
-        self.lat      = lat
-        self.lon      = lon
+        self.spatial = False
+        self.lat = lat
+        self.lon = lon
         self.lat_bnds = lat_bnds
         self.lon_bnds = lon_bnds
-        self.area     = keywords.get("area",None)
+        self.area = keywords.get("area", None)
 
         # If the last dimensions are lat and lon, this is spatial data
         if lat is not None and lon is not None and data.ndim >= 2:
-            if (data.shape[-2] == lat.size and data.shape[-1] == lon.size): self.spatial = True
+            if data.shape[-2] == lat.size and data.shape[-1] == lon.size:
+                self.spatial = True
 
         # Exception for PEcAn
         if self.spatial is True:
-            if data.shape[-2:] == (1,1):
+            if data.shape[-2:] == (1, 1):
                 self.spatial = False
-                self.ndata   = 1
-                self.data    = self.data.reshape(self.data.shape[:-2]+(1,))
-        
+                self.ndata = 1
+                self.data = self.data.reshape(self.data.shape[:-2] + (1,))
+
         if self.spatial is False:
-            
             # Shift possible values on [0,360] to [-180,180]
-            if self.lon      is not None: self.lon      = _shiftLon(self.lon     )
-            if self.lon_bnds is not None: self.lon_bnds = _shiftLon(self.lon_bnds)
+            if self.lon is not None:
+                self.lon = _shiftLon(self.lon)
+            if self.lon_bnds is not None:
+                self.lon_bnds = _shiftLon(self.lon_bnds)
 
         else:
-            
             # Exception for CABLE, flips if monotonically decreasing
-            if np.all(np.diff(self.lat)<0):  
-                self.lat      = self.lat     [::-1     ]
-                self.data     = self.data[...,::-1,:   ]
-                if self.lat_bnds is not None: self.lat_bnds = self.lat_bnds[::-1,::-1]
-                if self.area     is not None: self.area     = self.area    [::-1,:]
+            if np.all(np.diff(self.lat) < 0):
+                self.lat = self.lat[::-1]
+                self.data = self.data[..., ::-1, :]
+                if self.lat_bnds is not None:
+                    self.lat_bnds = self.lat_bnds[::-1, ::-1]
+                if self.area is not None:
+                    self.area = self.area[::-1, :]
 
             # Shift possible values on [0,360] to [-180,180]
-            if self.lon      is not None: self.lon      = _shiftLon(self.lon     )
-            if self.lon_bnds is not None: self.lon_bnds = _shiftLon(self.lon_bnds)
+            if self.lon is not None:
+                self.lon = _shiftLon(self.lon)
+            if self.lon_bnds is not None:
+                self.lon_bnds = _shiftLon(self.lon_bnds)
 
             # Shift first column of data to the international dateline
-            out = _shiftFirstColumnToDateline(self.lon,
-                                              lon_bnds = self.lon_bnds,
-                                              data     = self.data,
-                                              area     = self.area)
-            self.lon,self.lon_bnds,self.data,self.area = out
-            
+            out = _shiftFirstColumnToDateline(
+                self.lon, lon_bnds=self.lon_bnds, data=self.data, area=self.area
+            )
+            self.lon, self.lon_bnds, self.data, self.area = out
+
             # Finally create bounds if they are not there
-            if self.lat_bnds is None: self.lat_bnds = _createBnds(self.lat)
-            if self.lon_bnds is None: self.lon_bnds = _createBnds(self.lon)
-            
+            if self.lat_bnds is None:
+                self.lat_bnds = _createBnds(self.lat)
+            if self.lon_bnds is None:
+                self.lon_bnds = _createBnds(self.lon)
+
             # Fix potential problems with rolling the axes of the lon_bnds
-            self.lat_bnds = self.lat_bnds.clip(- 90,+ 90)
-            self.lon_bnds = self.lon_bnds.clip(-180,+180)
-            
+            self.lat_bnds = self.lat_bnds.clip(-90, +90)
+            self.lon_bnds = self.lon_bnds.clip(-180, +180)
+
             # Make sure that the value lies within the bounds
-            assert np.all((self.lat>=self.lat_bnds[:,0])*(self.lat<=self.lat_bnds[:,1]))
-            assert np.all((self.lon>=self.lon_bnds[:,0])*(self.lon<=self.lon_bnds[:,1]))
-            if self.area is None: self.area = il.CellAreas(None,None,
-                                                           lat_bnds=self.lat_bnds,
-                                                           lon_bnds=self.lon_bnds)
+            assert np.all(
+                (self.lat >= self.lat_bnds[:, 0]) * (self.lat <= self.lat_bnds[:, 1])
+            )
+            assert np.all(
+                (self.lon >= self.lon_bnds[:, 0]) * (self.lon <= self.lon_bnds[:, 1])
+            )
+            if self.area is None:
+                self.area = il.CellAreas(
+                    None, None, lat_bnds=self.lat_bnds, lon_bnds=self.lon_bnds
+                )
 
         # Is the data layered
-        self.layered    = False
-        self.depth      = depth
+        self.layered = False
+        self.depth = depth
         self.depth_bnds = depth_bnds
-        if (data.ndim > (self.temporal + 2*self.spatial + (self.ndata is not None))) and depth is not None:
-            self.layered    = True
-            if depth_bnds is None: self.depth_bnds = _createBnds(self.depth)
+        if (
+            data.ndim > (self.temporal + 2 * self.spatial + (self.ndata is not None))
+        ) and depth is not None:
+            self.layered = True
+            if depth_bnds is None:
+                self.depth_bnds = _createBnds(self.depth)
 
     def __str__(self):
-        if self.data  is None: return "Uninitialized Variable"
+        if self.data is None:
+            return "Uninitialized Variable"
         if self.ndata is None:
             ndata = "N/A"
         else:
             ndata = str(self.ndata)
         if not self.temporal:
             time = ""
         else:
             time = " (%d)" % self.time.size
         if not self.spatial:
             space = ""
         else:
-            space = " (%d,%d)" % (self.lat.size,self.lon.size)
+            space = " (%d,%d)" % (self.lat.size, self.lon.size)
         if not self.layered:
             layer = ""
         else:
             layer = " (%d)" % (self.depth.size)
-        s  = "Variable: %s\n" % self.name
-        s += "-"*(len(self.name)+10) + "\n"
-        s += "{0:>20}: ".format("unit")       + self.unit          + "\n"
-        s += "{0:>20}: ".format("isTemporal") + str(self.temporal) + time  + "\n"
-        s += "{0:>20}: ".format("isSpatial")  + str(self.spatial)  + space + "\n"
-        s += "{0:>20}: ".format("isLayered")  + str(self.layered)  + layer + "\n"
-        s += "{0:>20}: ".format("nDatasites") + ndata              + "\n"
-        s += "{0:>20}: ".format("dataShape")  + "%s\n" % (self.data.shape,)
-        np.seterr(over='ignore',under='ignore')
-        s += "{0:>20}: ".format("dataMax")    + "%e\n" % self.data.max()
-        s += "{0:>20}: ".format("dataMin")    + "%e\n" % self.data.min()
-        s += "{0:>20}: ".format("dataMean")   + "%e\n" % self.data.mean()
-        np.seterr(over='warn',under='warn')
+        s = "Variable: %s\n" % self.name
+        s += "-" * (len(self.name) + 10) + "\n"
+        s += "{0:>20}: ".format("unit") + self.unit + "\n"
+        s += "{0:>20}: ".format("isTemporal") + str(self.temporal) + time + "\n"
+        s += "{0:>20}: ".format("isSpatial") + str(self.spatial) + space + "\n"
+        s += "{0:>20}: ".format("isLayered") + str(self.layered) + layer + "\n"
+        s += "{0:>20}: ".format("nDatasites") + ndata + "\n"
+        s += "{0:>20}: ".format("dataShape") + "%s\n" % (self.data.shape,)
+        np.seterr(over="ignore", under="ignore")
+        s += "{0:>20}: ".format("dataMax") + "%e\n" % self.data.max()
+        s += "{0:>20}: ".format("dataMin") + "%e\n" % self.data.min()
+        s += "{0:>20}: ".format("dataMean") + "%e\n" % self.data.mean()
+        np.seterr(over="warn", under="warn")
         if self.cbounds is not None:
-            s += "{0:>20}: ".format("climatology")   + "%d thru %d\n" % (self.cbounds[0],self.cbounds[1])
+            s += "{0:>20}: ".format("climatology") + "%d thru %d\n" % (
+                self.cbounds[0],
+                self.cbounds[1],
+            )
 
         return s
 
     def nbytes(self):
-        r"""Estimate the memory usage of a variable in bytes.
-        """
-        nbytes = 0.
+        r"""Estimate the memory usage of a variable in bytes."""
+        nbytes = 0.0
         for key in self.__dict__.keys():
             try:
                 nbytes += self.__dict__[key].nbytes
             except:
                 pass
         return nbytes
 
-    def integrateInTime(self,**keywords):
+    def integrateInTime(self, **keywords):
         r"""Integrates the variable over a given time period.
 
         Uses nodal integration to integrate to approximate
 
         .. math:: \int_{t_0}^{t_f} v(t,\dots)\ dt
 
         The arguments of the integrand reflect that while it must be
@@ -307,98 +360,155 @@
         Returns
         -------
         integral : ILAMB.Variable.Variable
             a Variable instance with the integrated value along with the
             appropriate name and unit change
 
         """
-        if not self.temporal: raise il.NotTemporalVariable()
-        t0   = keywords.get("t0",self.time_bnds[:,0].min())
-        tf   = keywords.get("tf",self.time_bnds[:,1].max())
-        mean = keywords.get("mean",False)
+        if not self.temporal:
+            raise il.NotTemporalVariable()
+        t0 = keywords.get("t0", self.time_bnds[:, 0].min())
+        tf = keywords.get("tf", self.time_bnds[:, 1].max())
+        mean = keywords.get("mean", False)
 
         # find which time bounds are included even partially in the interval [t0,tf]
         time_bnds = np.copy(self.time_bnds)
-        ind       = np.where((t0<time_bnds[:,1])*(tf>time_bnds[:,0]))[0]
-        time_bnds[(t0>time_bnds[:,0])*(t0<time_bnds[:,1]),0] = t0
-        time_bnds[(tf>time_bnds[:,0])*(tf<time_bnds[:,1]),1] = tf
-        time_bnds = time_bnds[ind,:]
-        dt        = (time_bnds[:,1]-time_bnds[:,0])
+        ind = np.where((t0 < time_bnds[:, 1]) * (tf > time_bnds[:, 0]))[0]
+        time_bnds[(t0 > time_bnds[:, 0]) * (t0 < time_bnds[:, 1]), 0] = t0
+        time_bnds[(tf > time_bnds[:, 0]) * (tf < time_bnds[:, 1]), 1] = tf
+        time_bnds = time_bnds[ind, :]
+        dt = time_bnds[:, 1] - time_bnds[:, 0]
 
         # now expand this dt to the other dimensions of the data array (i.e. space or datasites)
-        for i in range(self.data.ndim-1): dt = np.expand_dims(dt,axis=-1)
+        for i in range(self.data.ndim - 1):
+            dt = np.expand_dims(dt, axis=-1)
 
         # approximate the integral by nodal integration (rectangle rule)
-        np.seterr(over='ignore',under='ignore')
-        integral = (self.data[ind]*dt).sum(axis=0)
+        np.seterr(over="ignore", under="ignore")
+        integral = (self.data[ind] * dt).sum(axis=0)
         integral_bnd = None
         if self.data_bnds is not None:
-            integral_bnd = np.ma.asarray([((((self.data_bnds[...,0])[ind])*dt).sum(axis=0)).T,
-                                          ((((self.data_bnds[...,1])[ind])*dt).sum(axis=0)).T]).T
-        np.seterr(over='raise',under='raise')
+            integral_bnd = np.ma.asarray(
+                [
+                    ((((self.data_bnds[..., 0])[ind]) * dt).sum(axis=0)).T,
+                    ((((self.data_bnds[..., 1])[ind]) * dt).sum(axis=0)).T,
+                ]
+            ).T
+        np.seterr(over="raise", under="raise")
 
         # the integrated array should be masked where *all* data in time was previously masked
         mask = False
         if self.data.ndim > 1 and self.data.mask.size > 1:
-            mask = np.apply_along_axis(np.all,0,self.data.mask[ind])
-        integral = np.ma.masked_array(integral,mask=mask,copy=False)
-            
+            mask = np.apply_along_axis(np.all, 0, self.data.mask[ind])
+        integral = np.ma.masked_array(integral, mask=mask, copy=False)
+
         # handle units
         unit = Unit(self.unit)
         name = self.name + "_integrated_over_time"
 
         if mean:
-
             # divide thru by the non-masked amount of time, the units
             # can remain as input because we integrate over time and
             # then divide by the time interval in the same units
-            name     += "_and_divided_by_time_period"
+            name += "_and_divided_by_time_period"
             if self.data.mask.size > 1:
-                dt = (dt*(self.data.mask[ind]==0)).sum(axis=0)
+                dt = (dt * (self.data.mask[ind] == 0)).sum(axis=0)
             else:
                 dt = dt.sum(axis=0)
-            np.seterr(over='ignore',under='ignore')
+            np.seterr(over="ignore", under="ignore")
             integral = integral / dt
             if integral_bnd is not None:
-                integral_bnd[...,0] = integral_bnd[...,0] / dt
-                integral_bnd[...,1] = integral_bnd[...,1] / dt
-            np.seterr(over='raise' ,under='raise' )
+                integral_bnd[..., 0] = integral_bnd[..., 0] / dt
+                integral_bnd[..., 1] = integral_bnd[..., 1] / dt
+            np.seterr(over="raise", under="raise")
 
         else:
-
             # if not a mean, we need to potentially handle unit conversions
-            unit0 = Unit("d")*unit
-            unit  = Unit(unit0.format().split()[-1])
-
+            unit0 = Unit("d") * unit
+            unit = Unit(unit0.format().split()[-1])
 
             if not isinstance(integral.mask, np.ndarray):
-               if integral.mask == True:
-                  integral=np.ma.masked_array(data=integral.data, mask=np.ones(integral.shape,dtype='bool'))
-               else:
-                  integral=np.ma.masked_array(data=integral.data, mask=np.zeros(integral.shape,dtype='bool'))
+                if integral.mask == True:
+                    integral = np.ma.masked_array(
+                        data=integral.data, mask=np.ones(integral.shape, dtype="bool")
+                    )
+                else:
+                    integral = np.ma.masked_array(
+                        data=integral.data, mask=np.zeros(integral.shape, dtype="bool")
+                    )
 
-            unit0.convert(integral,unit,inplace=True)
+            unit0.convert(integral, unit, inplace=True)
             if integral_bnd is not None:
-                unit0.convert(integral_bnd,unit,inplace=True)
-                
-                
-        return Variable(data       = integral,
-                        data_bnds   = integral_bnd,
-                        unit       = "%s" % unit,
-                        name       = name,
-                        lat        = self.lat,
-                        lat_bnds   = self.lat_bnds,
-                        lon        = self.lon,
-                        lon_bnds   = self.lon_bnds,
-                        depth      = self.depth,
-                        depth_bnds = self.depth_bnds,
-                        area       = self.area,
-                        ndata      = self.ndata)
+                unit0.convert(integral_bnd, unit, inplace=True)
 
-    def integrateInDepth(self,**keywords):
+        return Variable(
+            data=integral,
+            data_bnds=integral_bnd,
+            unit="%s" % unit,
+            name=name,
+            lat=self.lat,
+            lat_bnds=self.lat_bnds,
+            lon=self.lon,
+            lon_bnds=self.lon_bnds,
+            depth=self.depth,
+            depth_bnds=self.depth_bnds,
+            area=self.area,
+            ndata=self.ndata,
+        )
+
+    def applyOverTimeInterval(self, op, **keywords):
+        r"""Apply operator over time intervals.
+
+        Parameters
+        ----------
+        op : function
+            operator to apply over the time intervals
+        t0 : float, optional
+            initial time in days since 1/1/1850
+        tf : float, optional
+            final time in days since 1/1/1850
+        intervals : array of shape (n,2), optional
+            An array of n intervals where the first entry is the
+            beginning and the second entry is the end of the interval
+
+        """
+        if not self.temporal:
+            raise il.NotTemporalVariable()
+        t0 = keywords.get("t0", self.time_bnds[:, 0].min())
+        tf = keywords.get("tf", self.time_bnds[:, 1].max())
+        intervals = keywords.get("intervals", None)
+        if intervals is None:
+            intervals = np.asarray([[t0, tf]])
+        assert intervals.ndim == 2
+        n = intervals.shape[0]
+        shp = (n,) + self.data.shape[1:]
+        time = np.zeros(n)
+        data = np.ma.zeros(shp)
+        for i in range(n):
+            t0 = intervals[i, 0]
+            tf = intervals[i, 1]
+            time[i] = 0.5 * (t0 + tf)
+            ind = np.where((t0 <= self.time_bnds[:, 1]) * (tf >= self.time_bnds[:, 0]))[
+                0
+            ]
+            data[i] = op(self.data[ind, ...], axis=0)
+        return Variable(
+            name="%s_%s" % (op.__name__, self.name),
+            unit=self.unit,
+            time=time,
+            time_bnds=intervals,
+            data=data,
+            ndata=self.ndata,
+            lat=self.lat,
+            lon=self.lon,
+            area=self.area,
+            depth_bnds=self.depth_bnds,
+        )
+
+    def integrateInDepth(self, **keywords):
         r"""Integrates the variable over a given layer limits.
 
         Uses nodal integration to integrate to approximate
 
         .. math:: \int_{z_0}^{z_f} v(z,\dots)\ dz
 
         The arguments of the integrand reflect that while it must be
@@ -428,94 +538,96 @@
         Returns
         -------
         integral : ILAMB.Variable.Variable
             a Variable instance with the integrated value along with the
             appropriate name and unit change
 
         """
-        if not self.layered: raise il.NotLayeredVariable()
-        z0   = keywords.get("z0",self.depth_bnds[:,0].min())
-        zf   = keywords.get("zf",self.depth_bnds[:,1].max())
-        mean = keywords.get("mean",False)
+        if not self.layered:
+            raise il.NotLayeredVariable()
+        z0 = keywords.get("z0", self.depth_bnds[:, 0].min())
+        zf = keywords.get("zf", self.depth_bnds[:, 1].max())
+        mean = keywords.get("mean", False)
 
         # find which time bounds are included even partially in the interval [z0,zf]
         depth_bnds = np.copy(self.depth_bnds)
-        ind        = np.where((z0<depth_bnds[:,1])*(zf>depth_bnds[:,0]))[0]
-        depth_bnds[(z0>depth_bnds[:,0])*(z0<depth_bnds[:,1]),0] = z0
-        depth_bnds[(zf>depth_bnds[:,0])*(zf<depth_bnds[:,1]),1] = zf
-        depth_bnds = depth_bnds[ind,:]
-        dz         = (depth_bnds[:,1]-depth_bnds[:,0])
+        ind = np.where((z0 < depth_bnds[:, 1]) * (zf > depth_bnds[:, 0]))[0]
+        depth_bnds[(z0 > depth_bnds[:, 0]) * (z0 < depth_bnds[:, 1]), 0] = z0
+        depth_bnds[(zf > depth_bnds[:, 0]) * (zf < depth_bnds[:, 1]), 1] = zf
+        depth_bnds = depth_bnds[ind, :]
+        dz = depth_bnds[:, 1] - depth_bnds[:, 0]
 
         args = []
         axis = 0
         if self.temporal:
             axis += 1
-            dz = np.expand_dims(dz,axis=0)
+            dz = np.expand_dims(dz, axis=0)
             args.append(range(self.time.size))
-        if self.layered: args.append(ind)
+        if self.layered:
+            args.append(ind)
         if self.ndata:
-            dz = np.expand_dims(dz,axis=-1)
+            dz = np.expand_dims(dz, axis=-1)
             args.append(range(self.ndata))
         if self.spatial:
-            dz = np.expand_dims(dz,axis=-1)
-            dz = np.expand_dims(dz,axis=-1)
+            dz = np.expand_dims(dz, axis=-1)
+            dz = np.expand_dims(dz, axis=-1)
             args.append(range(self.lat.size))
             args.append(range(self.lon.size))
         ind = np.ix_(*args)
 
         # approximate the integral by nodal integration (rectangle rule)
         shp = self.data[ind].shape
-        np.seterr(over='ignore',under='ignore')
-        integral = (self.data[ind]*dz).sum(axis=axis)
-        np.seterr(over='raise',under='raise')
+        np.seterr(over="ignore", under="ignore")
+        integral = (self.data[ind] * dz).sum(axis=axis)
+        np.seterr(over="raise", under="raise")
 
         # the integrated array should be masked where *all* data in depth was previously masked
         mask = False
         if self.data.ndim > 1 and self.data.mask.size > 1:
-            mask = np.apply_along_axis(np.all,axis,self.data.mask[ind])
-        integral = np.ma.masked_array(integral,mask=mask,copy=False)
+            mask = np.apply_along_axis(np.all, axis, self.data.mask[ind])
+        integral = np.ma.masked_array(integral, mask=mask, copy=False)
 
         # handle units
         unit = Unit(self.unit)
         name = self.name + "_integrated_over_depth"
 
         if mean:
-
             # divide thru by the non-masked amount of time, the units
             # can remain as input because we integrate over time and
             # then divide by the time interval in the same units
-            name     += "_and_divided_by_depth"
+            name += "_and_divided_by_depth"
             if self.data.mask.size > 1:
-                dz = (dz*(self.data.mask[ind]==0)).sum(axis=axis)
+                dz = (dz * (self.data.mask[ind] == 0)).sum(axis=axis)
             else:
                 dz = dz.sum(axis=axis)
-            np.seterr(over='ignore',under='ignore')
+            np.seterr(over="ignore", under="ignore")
             integral = integral / dz
-            np.seterr(over='raise' ,under='raise' )
+            np.seterr(over="raise", under="raise")
 
         else:
-
             # if not a mean, we need to potentially handle unit conversions
-            unit0    = Unit("m")*unit
-            unit     = Unit(unit0.format().split()[-1])
-            unit0.convert(integral,unit,inplace=True)
-
-        return Variable(data       = integral,
-                        unit       = "%s" % unit,
-                        name       = name,
-                        time       = self.time,
-                        time_bnds  = self.time_bnds,
-                        lat        = self.lat,
-                        lat_bnds   = self.lat_bnds,
-                        lon        = self.lon,
-                        lon_bnds   = self.lon_bnds,
-                        area       = self.area,
-                        ndata      = self.ndata)
+            unit0 = Unit("m") * unit
+            unit = Unit(unit0.format().split()[-1])
+            unit0.convert(integral, unit, inplace=True)
+
+        return Variable(
+            data=integral,
+            unit="%s" % unit,
+            name=name,
+            time=self.time,
+            time_bnds=self.time_bnds,
+            lat=self.lat,
+            lat_bnds=self.lat_bnds,
+            lon=self.lon,
+            lon_bnds=self.lon_bnds,
+            area=self.area,
+            ndata=self.ndata,
+        )
 
-    def integrateInSpace(self,region=None,mean=False,weight=None,intabs=False):
+    def integrateInSpace(self, region=None, mean=False, weight=None, intabs=False):
         r"""Integrates the variable over a given region.
 
         Uses nodal integration to integrate to approximate
 
         .. math:: \int_{\Omega} v(\mathbf{x},\dots)\ d\Omega
 
         The arguments of the integrand reflect that while it must be
@@ -563,167 +675,187 @@
         Returns
         -------
         integral : ILAMB.Variable.Variable
             a Variable instace with the integrated value along with the
             appropriate name and unit change.
 
         """
-        def _integrate(var,areas):
-            op = lambda x : x
-            if intabs: op = np.abs
+
+        def _integrate(var, areas):
+            op = lambda x: x
+            if intabs:
+                op = np.abs
             assert var.shape[-2:] == areas.shape
-            np.seterr(over='ignore',under='ignore')
-            vbar = (op(var)*areas).sum(axis=-1).sum(axis=-1)
-            np.seterr(over='raise',under='raise')
+            np.seterr(over="ignore", under="ignore")
+            vbar = (op(var) * areas).sum(axis=-1).sum(axis=-1)
+            np.seterr(over="raise", under="raise")
             return vbar
 
-        if not self.spatial: raise il.NotSpatialVariable()
+        if not self.spatial:
+            raise il.NotSpatialVariable()
 
         # determine the measure
         mask = self.data.mask
-        while mask.ndim > 2: mask = np.all(mask,axis=0)
-        measure = np.ma.masked_array(self.area,mask=mask,copy=True)
-        if weight is not None: measure *= weight
+        while mask.ndim > 2:
+            mask = np.all(mask, axis=0)
+        measure = np.ma.masked_array(self.area, mask=mask, copy=True)
+        if weight is not None:
+            measure *= weight
 
         # if we want to integrate over a region, we need add to the
         # measure's mask
         r = Regions()
-        if region is not None: measure.mask += r.getMask(region,self)
+        if region is not None:
+            measure.mask += r.getMask(region, self)
 
         # approximate the integral
-        integral = _integrate(self.data,measure)
+        integral = _integrate(self.data, measure)
         if mean:
-            np.seterr(under='ignore',invalid='ignore')
+            np.seterr(under="ignore", invalid="ignore")
             integral = integral / measure.sum()
-            np.seterr(under='raise',invalid='warn')
+            np.seterr(under="raise", invalid="warn")
 
         # handle the name and unit
         name = self.name + "_integrated_over_space"
-        if region is not None: name = name.replace("space",region)
+        if region is not None:
+            name = name.replace("space", region)
         unit = Unit(self.unit)
         if mean:
-
             # we have already divided thru by the non-masked area in
             # units of m^2, which are the same units of the integrand.
             name += "_and_divided_by_area"
         else:
-
             # if not a mean, we need to potentially handle unit conversions
             unit *= Unit("m2")
 
-        return Variable(data       = np.ma.masked_array(integral),
-                        unit       = "%s" % unit,
-                        time       = self.time,
-                        time_bnds  = self.time_bnds,
-                        depth      = self.depth,
-                        depth_bnds = self.depth_bnds,
-                        name       = name)
+        return Variable(
+            data=np.ma.masked_array(integral),
+            unit="%s" % unit,
+            time=self.time,
+            time_bnds=self.time_bnds,
+            depth=self.depth,
+            depth_bnds=self.depth_bnds,
+            name=name,
+        )
 
-    def siteStats(self,region=None,weight=None,intabs=False):
+    def siteStats(self, region=None, weight=None, intabs=False):
         """Computes the mean and standard deviation of the variable over all data sites.
 
         Parameters
         ----------
         region : str, optional
             name of the region overwhich you wish to include stats.
 
         Returns
         -------
         mean : ILAMB.Variable.Variable
             a Variable instace with the mean values
 
         """
-        if self.ndata is None: raise il.NotDatasiteVariable()
-        op = lambda x : x
-        if intabs: op = np.abs
+        if self.ndata is None:
+            raise il.NotDatasiteVariable()
+        op = lambda x: x
+        if intabs:
+            op = np.abs
         rem_mask = np.copy(self.data.mask)
         rname = ""
         r = Regions()
         if region is not None:
-            self.data.mask += r.getMask(region,self)
+            self.data.mask += r.getMask(region, self)
             rname = "_over_%s" % region
-        np.seterr(over='ignore',under='ignore')
-        mean = np.ma.average(op(self.data),axis=-1,weights=weight)
-        np.seterr(over='raise',under='raise')
+        np.seterr(over="ignore", under="ignore")
+        mean = np.ma.average(op(self.data), axis=-1, weights=weight)
+        np.seterr(over="raise", under="raise")
         self.data.mask = rem_mask
-        return Variable(data       = mean,
-                        unit       = self.unit,
-                        time       = self.time,
-                        time_bnds  = self.time_bnds,
-                        depth      = self.depth,
-                        depth_bnds = self.depth_bnds,
-                        name       = "mean_%s%s" % (self.name,rname))
+        return Variable(
+            data=mean,
+            unit=self.unit,
+            time=self.time,
+            time_bnds=self.time_bnds,
+            depth=self.depth,
+            depth_bnds=self.depth_bnds,
+            name="mean_%s%s" % (self.name, rname),
+        )
 
     def annualCycle(self):
         """Computes mean annual cycle information (climatology) for the variable.
 
         For each site/cell/depth in the variable, compute the mean annual cycle.
 
         Returns
         -------
         mean : ILAMB.Variable.Variable
             The annual cycle mean values
         """
-        if not self.temporal: raise il.NotTemporalVariable()
+        if not self.temporal:
+            raise il.NotTemporalVariable()
         assert self.monthly
         assert self.time.size > 11
-        begin = np.argmin(self.time[:11]%365)
-        end   = begin+int(self.time[begin:].size/12.)*12
-        shp   = (-1,12) + self.data.shape[1:]
-        v     = self.data[begin:end,...].reshape(shp)
-        np.seterr(over='ignore',under='ignore')
-        mean  = v.mean(axis=0)
-        np.seterr(over='raise',under='raise')
-        return Variable(data       = mean,
-                        unit       = self.unit,
-                        name       = "annual_cycle_mean_of_%s" % self.name,
-                        time       = mid_months,
-                        time_bnds  = np.asarray([bnd_months[:-1],bnd_months[1:]]).T,
-                        lat        = self.lat,
-                        lat_bnds   = self.lat_bnds,
-                        lon        = self.lon,
-                        lon_bnds   = self.lon_bnds,
-                        area       = self.area,
-                        depth      = self.depth,
-                        depth_bnds = self.depth_bnds,
-                        ndata      = self.ndata)
+        begin = np.argmin(self.time[:11] % 365)
+        end = begin + int(self.time[begin:].size / 12.0) * 12
+        shp = (-1, 12) + self.data.shape[1:]
+        v = self.data[begin:end, ...].reshape(shp)
+        np.seterr(over="ignore", under="ignore")
+        mean = v.mean(axis=0)
+        np.seterr(over="raise", under="raise")
+        return Variable(
+            data=mean,
+            unit=self.unit,
+            name="annual_cycle_mean_of_%s" % self.name,
+            time=mid_months,
+            time_bnds=np.asarray([bnd_months[:-1], bnd_months[1:]]).T,
+            lat=self.lat,
+            lat_bnds=self.lat_bnds,
+            lon=self.lon,
+            lon_bnds=self.lon_bnds,
+            area=self.area,
+            depth=self.depth,
+            depth_bnds=self.depth_bnds,
+            ndata=self.ndata,
+        )
 
-    def timeOfExtrema(self,etype="max"):
+    def timeOfExtrema(self, etype="max"):
         """Returns the time of the specified extrema.
 
         Parameters
         ----------
         etype : str, optional
             The type of extrema to compute, either 'max' or 'min'
 
         Returns
         -------
         extrema : ILAMB.Variable.Variable
             The times of the extrema computed
         """
-        if not self.temporal: raise il.NotTemporalVariable()
-        fcn = {"max":np.argmax,"min":np.argmin}
+        if not self.temporal:
+            raise il.NotTemporalVariable()
+        fcn = {"max": np.argmax, "min": np.argmin}
         assert etype in fcn.keys()
-        tid  = np.apply_along_axis(fcn[etype],0,self.data)
+        tid = np.apply_along_axis(fcn[etype], 0, self.data)
         mask = False
-        if self.data.ndim > 1 and self.data.mask.ndim > 0: mask = np.apply_along_axis(np.all,0,self.data.mask) # mask cells where all data is masked
-        data = np.ma.masked_array(self.time[tid],mask=mask)
-        return Variable(data       = data,
-                        unit       = "d",
-                        name       = "time_of_%s_%s" % (etype,self.name),
-                        lat        = self.lat,
-                        lat_bnds   = self.lat_bnds,
-                        lon        = self.lon,
-                        lon_bnds   = self.lon_bnds,
-                        area       = self.area,
-                        depth      = self.depth,
-                        depth_bnds = self.depth_bnds,
-                        ndata      = self.ndata)
+        if self.data.ndim > 1 and self.data.mask.ndim > 0:
+            mask = np.apply_along_axis(
+                np.all, 0, self.data.mask
+            )  # mask cells where all data is masked
+        data = np.ma.masked_array(self.time[tid], mask=mask)
+        return Variable(
+            data=data,
+            unit="d",
+            name="time_of_%s_%s" % (etype, self.name),
+            lat=self.lat,
+            lat_bnds=self.lat_bnds,
+            lon=self.lon,
+            lon_bnds=self.lon_bnds,
+            area=self.area,
+            depth=self.depth,
+            depth_bnds=self.depth_bnds,
+            ndata=self.ndata,
+        )
 
-    def extractDatasites(self,lat,lon):
+    def extractDatasites(self, lat, lon):
         """Extracts a variable at sites defined by a set of latitude and longitude.
 
         Parameters
         ----------
         lat : numpy.ndarray
             an array with the latitude values, must be same size as the longitude values
         lon : numpy.ndarray
@@ -731,34 +863,37 @@
 
         Returns
         -------
         extracted : ILAMB.Variable.Variable
             The extracted variables
         """
         assert lat.size == lon.size
-        if not self.spatial: raise il.NotSpatialVariable()
-        ilat = np.apply_along_axis(np.argmin,1,np.abs(lat[:,np.newaxis]-self.lat))
-        ilon = np.apply_along_axis(np.argmin,1,np.abs(lon[:,np.newaxis]-self.lon))
+        if not self.spatial:
+            raise il.NotSpatialVariable()
+        ilat = np.apply_along_axis(np.argmin, 1, np.abs(lat[:, np.newaxis] - self.lat))
+        ilon = np.apply_along_axis(np.argmin, 1, np.abs(lon[:, np.newaxis] - self.lon))
         ndata = lat.size
         if self.data.ndim == 2:
-            data  = self.data[    ilat,ilon]
+            data = self.data[ilat, ilon]
         else:
-            data  = self.data[...,ilat,ilon]
-        return Variable(data       = data,
-                        unit       = self.unit,
-                        name       = self.name,
-                        lat        = lat,
-                        lon        = lon,
-                        ndata      = ndata,
-                        depth      = self.depth,
-                        depth_bnds = self.depth_bnds,
-                        time       = self.time,
-                        time_bnds  = self.time_bnds)
+            data = self.data[..., ilat, ilon]
+        return Variable(
+            data=data,
+            unit=self.unit,
+            name=self.name,
+            lat=lat,
+            lon=lon,
+            ndata=ndata,
+            depth=self.depth,
+            depth_bnds=self.depth_bnds,
+            time=self.time,
+            time_bnds=self.time_bnds,
+        )
 
-    def spatialDifference(self,var):
+    def spatialDifference(self, var):
         """Computes the point-wise difference of two spatially defined variables.
 
         If the variable is spatial or site data and is defined on the
         same grid, this routine will simply compute the difference in
         the data arrays. If the variables are spatial but defined on
         separate grids, the routine will interpolate both variables to
         a composed grid via nearest-neighbor interpolation and then
@@ -770,60 +905,79 @@
             The variable we wish to compare against this variable
 
         Returns
         -------
         diff : ILAMB.Variable.Variable
             A new variable object representing the difference
         """
+
         def _make_bnds(x):
-            bnds       = np.zeros(x.size+1)
-            bnds[1:-1] = 0.5*(x[1:]+x[:-1])
-            bnds[0]    = max(x[0] -0.5*(x[ 1]-x[ 0]),-180)
-            bnds[-1]   = min(x[-1]+0.5*(x[-1]-x[-2]),+180)
+            bnds = np.zeros(x.size + 1)
+            bnds[1:-1] = 0.5 * (x[1:] + x[:-1])
+            bnds[0] = max(x[0] - 0.5 * (x[1] - x[0]), -180)
+            bnds[-1] = min(x[-1] + 0.5 * (x[-1] - x[-2]), +180)
             return bnds
+
         assert Unit(var.unit) == Unit(self.unit)
         assert self.temporal == False
-        assert self.ndata    == var.ndata
-        assert self.layered  == False
+        assert self.ndata == var.ndata
+        assert self.layered == False
         # Perform a check on the spatial grid. If it is the exact same
         # grid, there is no need to interpolate.
         same_grid = False
         try:
-            same_grid = np.allclose(self.lat,var.lat)*np.allclose(self.lon,var.lon)
+            same_grid = np.allclose(self.lat, var.lat) * np.allclose(self.lon, var.lon)
         except:
             pass
 
         if same_grid:
-            error = np.ma.masked_array(var.data-self.data,mask=self.data.mask+var.data.mask)
-            diff  = Variable(data      = error,
-                             unit      = var.unit,
-                             lat       = var.lat,
-                             lat_bnds  = var.lat_bnds,
-                             lon       = var.lon,
-                             lon_bnds  = var.lon_bnds,
-                             ndata     = var.ndata,
-                             name      = "%s_minus_%s" % (var.name,self.name))
+            error = np.ma.masked_array(
+                var.data - self.data, mask=self.data.mask + var.data.mask
+            )
+            diff = Variable(
+                data=error,
+                unit=var.unit,
+                lat=var.lat,
+                lat_bnds=var.lat_bnds,
+                lon=var.lon,
+                lon_bnds=var.lon_bnds,
+                ndata=var.ndata,
+                name="%s_minus_%s" % (var.name, self.name),
+            )
         else:
-            if not self.spatial: raise il.NotSpatialVariable()
+            if not self.spatial:
+                raise il.NotSpatialVariable()
             lat_bnd1 = _make_bnds(self.lat)
             lon_bnd1 = _make_bnds(self.lon)
-            lat_bnd2 = _make_bnds( var.lat)
-            lon_bnd2 = _make_bnds( var.lon)
-            lat_bnd,lon_bnd,lat,lon,error = il.TrueError(lat_bnd1,lon_bnd1,self.lat,self.lon,self.data,
-                                                         lat_bnd2,lon_bnd2, var.lat, var.lon, var.data)
-            diff = Variable(data      = error,
-                            unit      = var.unit,
-                            lat       = lat,
-                            lat_bnd   = lat_bnd,
-                            lon       = lon,
-                            lon_bnd   = lon_bnd,
-                            name      = "%s_minus_%s" % (var.name,self.name))
+            lat_bnd2 = _make_bnds(var.lat)
+            lon_bnd2 = _make_bnds(var.lon)
+            lat_bnd, lon_bnd, lat, lon, error = il.TrueError(
+                lat_bnd1,
+                lon_bnd1,
+                self.lat,
+                self.lon,
+                self.data,
+                lat_bnd2,
+                lon_bnd2,
+                var.lat,
+                var.lon,
+                var.data,
+            )
+            diff = Variable(
+                data=error,
+                unit=var.unit,
+                lat=lat,
+                lat_bnd=lat_bnd,
+                lon=lon,
+                lon_bnd=lon_bnd,
+                name="%s_minus_%s" % (var.name, self.name),
+            )
         return diff
 
-    def convert(self,unit,density=998.2,molar_mass=12.0107):
+    def convert(self, unit, density=998.2, molar_mass=12.0107):
         """Convert the variable to a given unit.
 
         We use the UDUNITS library via the cf_units python interface to
         convert the variable's unit. Additional support is provided
         for unit conversions in which substance information is
         required. For example, in quantities such as precipitation it
         is common to have data in the form of a mass rate per unit
@@ -846,260 +1000,277 @@
 
         Returns
         -------
         self : ILAMB.Variable.Variable
             this object with its unit converted
 
         """
-        if unit is None: return self
+        if unit is None:
+            return self
         src_unit = Unit(self.unit)
-        tar_unit = Unit(     unit)
-        mask = self.data.mask        
+        tar_unit = Unit(unit)
+        mask = self.data.mask
         mass_density = Unit("kg m-3")
         molar_density = Unit("g mol-1")
-        if ((src_unit/tar_unit)/mass_density).is_dimensionless():
-            with np.errstate(all='ignore'):
+        if ((src_unit / tar_unit) / mass_density).is_dimensionless():
+            with np.errstate(all="ignore"):
                 self.data /= density
             src_unit /= mass_density
-        elif ((tar_unit/src_unit)/mass_density).is_dimensionless():
-            with np.errstate(all='ignore'):
+        elif ((tar_unit / src_unit) / mass_density).is_dimensionless():
+            with np.errstate(all="ignore"):
                 self.data *= density
             src_unit *= mass_density
-        if ((src_unit/tar_unit)/molar_density).is_dimensionless():
-            with np.errstate(all='ignore'):
+        if ((src_unit / tar_unit) / molar_density).is_dimensionless():
+            with np.errstate(all="ignore"):
                 self.data /= molar_mass
             src_unit /= molar_density
-        elif ((tar_unit/src_unit)/molar_density).is_dimensionless():
-            with np.errstate(all='ignore'):
+        elif ((tar_unit / src_unit) / molar_density).is_dimensionless():
+            with np.errstate(all="ignore"):
                 self.data *= molar_mass
             src_unit *= molar_density
         try:
-            with np.errstate(all='ignore'):
-                src_unit.convert(self.data,tar_unit,inplace=True)
+            with np.errstate(all="ignore"):
+                src_unit.convert(self.data, tar_unit, inplace=True)
                 if self.data_bnds is not None:
-                    src_unit.convert(self.data_bnds.data,tar_unit,inplace=True)
+                    src_unit.convert(self.data_bnds.data, tar_unit, inplace=True)
             self.unit = unit
         except:
             raise il.UnitConversionError()
         return self
 
-    def toNetCDF4(self,dataset,attributes=None,group=None):
+    def toNetCDF4(self, dataset, attributes=None, group=None):
         """Adds the variable to the specified netCDF4 dataset.
 
         Parameters
         ----------
         dataset : netCDF4.Dataset
             a dataset into which you wish to save this variable
         attributes : dict of scalars, optional
             a dictionary of additional scalars to encode as ncattrs
         group : str, optional
             the name of the netCDF4 group to to which we add this variable
         """
-        def _checkTime(t,dset):
+
+        def _checkTime(t, dset):
             """A local function for ensuring the time dimension is saved in the dataset."""
             time_name = "time"
             while True:
                 if time_name in dset.dimensions.keys():
-                    if (t.shape    == dset.variables[time_name][...].shape and
-                        np.allclose(t,dset.variables[time_name][...],atol=0.5*self.dt)):
+                    if t.shape == dset.variables[time_name][...].shape and np.allclose(
+                        t, dset.variables[time_name][...], atol=0.5 * self.dt
+                    ):
                         return time_name
                     else:
                         time_name += "_"
                 else:
                     dset.createDimension(time_name)
-                    T = dset.createVariable(time_name,"double",(time_name))
-                    T.setncattr("units","days since 1850-01-01 00:00:00")
-                    T.setncattr("calendar","noleap")
-                    T.setncattr("axis","T")
-                    T.setncattr("long_name","time")
-                    T.setncattr("standard_name","time")
+                    T = dset.createVariable(time_name, "double", (time_name))
+                    T.setncattr("units", "days since 1850-01-01 00:00:00")
+                    T.setncattr("calendar", "noleap")
+                    T.setncattr("axis", "T")
+                    T.setncattr("long_name", "time")
+                    T.setncattr("standard_name", "time")
                     T[...] = t
                     if self.time_bnds is not None:
-                        bnd_name = time_name.replace("time","time_bnds")
-                        T.setncattr("bounds",bnd_name)
+                        bnd_name = time_name.replace("time", "time_bnds")
+                        T.setncattr("bounds", bnd_name)
                         if "nb" not in dset.dimensions.keys():
-                            D = dset.createDimension("nb",size=2)
+                            D = dset.createDimension("nb", size=2)
                         if bnd_name not in dset.variables.keys():
-                            B = dset.createVariable(bnd_name,"double",(time_name,"nb"))
-                            B.setncattr("units","days since 1850-01-01 00:00:00")
+                            B = dset.createVariable(
+                                bnd_name, "double", (time_name, "nb")
+                            )
+                            B.setncattr("units", "days since 1850-01-01 00:00:00")
                             B[...] = self.time_bnds
                     return time_name
 
-        def _checkLat(lat,dset):
+        def _checkLat(lat, dset):
             """A local function for ensuring the lat dimension is saved in the dataset."""
             lat_name = "lat"
             while True:
                 if lat_name in dset.dimensions.keys():
-                    if (lat.shape == dset.variables[lat_name][...].shape and
-                        np.allclose(lat,dset.variables[lat_name][...])):
+                    if lat.shape == dset.variables[lat_name][...].shape and np.allclose(
+                        lat, dset.variables[lat_name][...]
+                    ):
                         return lat_name
                     else:
                         lat_name += "_"
                 else:
-                    dset.createDimension(lat_name,size=lat.size)
-                    Y = dset.createVariable(lat_name,"double",(lat_name))
-                    Y.setncattr("units","degrees_north")
-                    Y.setncattr("axis","Y")
-                    Y.setncattr("long_name","latitude")
-                    Y.setncattr("standard_name","latitude")
+                    dset.createDimension(lat_name, size=lat.size)
+                    Y = dset.createVariable(lat_name, "double", (lat_name))
+                    Y.setncattr("units", "degrees_north")
+                    Y.setncattr("axis", "Y")
+                    Y.setncattr("long_name", "latitude")
+                    Y.setncattr("standard_name", "latitude")
                     Y[...] = lat
                     if self.lat_bnds is not None:
-                        bnd_name = lat_name.replace("lat","lat_bnds")
-                        Y.setncattr("bounds",bnd_name)
+                        bnd_name = lat_name.replace("lat", "lat_bnds")
+                        Y.setncattr("bounds", bnd_name)
                         if "nb" not in dset.dimensions.keys():
-                            D = dset.createDimension("nb",size=2)
+                            D = dset.createDimension("nb", size=2)
                         if bnd_name not in dset.variables.keys():
-                            B = dset.createVariable(bnd_name,"double",(lat_name,"nb"))
-                            B.setncattr("units","degrees_north")
+                            B = dset.createVariable(
+                                bnd_name, "double", (lat_name, "nb")
+                            )
+                            B.setncattr("units", "degrees_north")
                             B[...] = self.lat_bnds
                     return lat_name
 
-        def _checkLon(lon,dset):
+        def _checkLon(lon, dset):
             """A local function for ensuring the lon dimension is saved in the dataset."""
             lon_name = "lon"
             while True:
                 if lon_name in dset.dimensions.keys():
-                    if (lon.shape == dset.variables[lon_name][...].shape and
-                    np.allclose(lon,dset.variables[lon_name][...])):
+                    if lon.shape == dset.variables[lon_name][...].shape and np.allclose(
+                        lon, dset.variables[lon_name][...]
+                    ):
                         return lon_name
                     else:
                         lon_name += "_"
                 else:
-                    dset.createDimension(lon_name,size=lon.size)
-                    X = dset.createVariable(lon_name,"double",(lon_name))
-                    X.setncattr("units","degrees_east")
-                    X.setncattr("axis","X")
-                    X.setncattr("long_name","longitude")
-                    X.setncattr("standard_name","longitude")
+                    dset.createDimension(lon_name, size=lon.size)
+                    X = dset.createVariable(lon_name, "double", (lon_name))
+                    X.setncattr("units", "degrees_east")
+                    X.setncattr("axis", "X")
+                    X.setncattr("long_name", "longitude")
+                    X.setncattr("standard_name", "longitude")
                     X[...] = lon
                     if self.lon_bnds is not None:
-                        bnd_name = lon_name.replace("lon","lon_bnds")
-                        X.setncattr("bounds",bnd_name)
+                        bnd_name = lon_name.replace("lon", "lon_bnds")
+                        X.setncattr("bounds", bnd_name)
                         if "nb" not in dset.dimensions.keys():
-                            D = dset.createDimension("nb",size=2)
+                            D = dset.createDimension("nb", size=2)
                         if bnd_name not in dset.variables.keys():
-                            B = dset.createVariable(bnd_name,"double",(lon_name,"nb"))
-                            B.setncattr("units","degrees_east")
+                            B = dset.createVariable(
+                                bnd_name, "double", (lon_name, "nb")
+                            )
+                            B.setncattr("units", "degrees_east")
                             B[...] = self.lon_bnds
                     return lon_name
 
-        def _checkData(ndata,dset):
+        def _checkData(ndata, dset):
             """A local function for ensuring the data dimension is saved in the dataset."""
             data_name = "data"
             while True:
                 if data_name in dset.dimensions.keys():
-                    if (ndata == len(dset.dimensions[data_name])):
+                    if ndata == len(dset.dimensions[data_name]):
                         return data_name
                     else:
                         data_name += "_"
                 else:
-                    dset.createDimension(data_name,size=ndata)
+                    dset.createDimension(data_name, size=ndata)
                     return data_name
 
-        def _checkLayer(layer,dataset):
+        def _checkLayer(layer, dataset):
             """A local function for ensuring the layer dimension is saved in the dataset."""
             layer_name = "layer"
             while True:
                 if layer_name in dataset.dimensions.keys():
-                    if (layer.shape == dataset.variables[layer_name][...].shape and
-                    np.allclose(layer,dataset.variables[layer_name][...])):
+                    if layer.shape == dataset.variables[layer_name][
+                        ...
+                    ].shape and np.allclose(layer, dataset.variables[layer_name][...]):
                         return layer_name
                     else:
                         layer_name += "_"
                 else:
-                    dataset.createDimension(layer_name,size=layer.size)
-                    Z = dataset.createVariable(layer_name,"double",(layer_name))
-                    Z.setncattr("units","m")
-                    Z.setncattr("axis","Z")
-                    Z.setncattr("long_name","depth")
-                    Z.setncattr("standard_name","depth")
+                    dataset.createDimension(layer_name, size=layer.size)
+                    Z = dataset.createVariable(layer_name, "double", (layer_name))
+                    Z.setncattr("units", "m")
+                    Z.setncattr("axis", "Z")
+                    Z.setncattr("long_name", "depth")
+                    Z.setncattr("standard_name", "depth")
                     Z[...] = layer
                     if self.depth_bnds is not None:
-                        bnd_name = layer_name.replace("layer","layer_bnds")
-                        Z.setncattr("bounds",bnd_name)
+                        bnd_name = layer_name.replace("layer", "layer_bnds")
+                        Z.setncattr("bounds", bnd_name)
                         if "nb" not in dataset.dimensions.keys():
-                            D = dataset.createDimension("nb",size=2)
+                            D = dataset.createDimension("nb", size=2)
                         if bnd_name not in dataset.variables.keys():
-                            B = dataset.createVariable(bnd_name,"double",(layer_name,"nb"))
-                            B.setncattr("units","m")
+                            B = dataset.createVariable(
+                                bnd_name, "double", (layer_name, "nb")
+                            )
+                            B.setncattr("units", "m")
                             B[...] = self.depth_bnds
                     return layer_name
 
         # if not group is desired, just write to the dataset...
         if group is None:
             dset = dataset
         else:
             # if a group is desired, check to see it exists and write into group
             if group not in dataset.groups:
                 dset = dataset.createGroup(group)
             else:
                 dset = dataset.groups[group]
 
         dim = []
-        if self.temporal: dim.append(_checkTime (self.time ,dset))
-        if self.layered:  dim.append(_checkLayer(self.depth,dset))
+        if self.temporal:
+            dim.append(_checkTime(self.time, dset))
+        if self.layered:
+            dim.append(_checkLayer(self.depth, dset))
         if self.ndata is not None:
-            dim.append(_checkData (self.ndata,dset))
-            _checkLat(self.lat,dset)
-            _checkLon(self.lon,dset)
+            dim.append(_checkData(self.ndata, dset))
+            _checkLat(self.lat, dset)
+            _checkLon(self.lon, dset)
         else:
-            if self.lat is not None: dim.append(_checkLat  (self.lat  ,dset))
-            if self.lon is not None: dim.append(_checkLon  (self.lon  ,dset))
+            if self.lat is not None:
+                dim.append(_checkLat(self.lat, dset))
+            if self.lon is not None:
+                dim.append(_checkLon(self.lon, dset))
 
         grp = dset
         if self.data.size == 1:
-            
             # we are writing out a scalar so we add it into a special
             # subgroup for scalars
             if "scalars" not in dset.groups:
                 grp = dset.createGroup("scalars")
             else:
                 grp = dset.groups["scalars"]
-            V = grp.createVariable(self.name,"double",dim,zlib=True)
+            V = grp.createVariable(self.name, "double", dim, zlib=True)
             V.units = self.unit
             if type(self.data) is np.ma.core.MaskedConstant:
                 V[...] = np.nan
             else:
                 V[...] = self.data
 
         else:
             # not a scalar, compute the min/max and middle 98
             # percentiles
             try:
-                per = np.percentile(self.data.compressed(),[0,1,99,100])
+                per = np.percentile(self.data.compressed(), [0, 1, 99, 100])
             except:
-                per = [0,0,0,0]
-            V = grp.createVariable(self.name,"double",dim,zlib=True)
+                per = [0, 0, 0, 0]
+            V = grp.createVariable(self.name, "double", dim, zlib=True)
             V.units = self.unit
             V.min = per[0]
             V.dn99 = per[1]
             V.up99 = per[2]
             V.max = per[3]
             V[...] = self.data
             if self.data_bnds is not None:
                 bnd_name = "%s_bnds" % (self.name)
                 V.bounds = bnd_name
-                VB = grp.createVariable(bnd_name,"double",dim+['nb'],zlib=True)
+                VB = grp.createVariable(bnd_name, "double", dim + ["nb"], zlib=True)
                 try:
-                    per = np.percentile(self.data_bnds.compressed(),[0,1,99,100])
+                    per = np.percentile(self.data_bnds.compressed(), [0, 1, 99, 100])
                 except:
-                    per = [0,0,0,0]
+                    per = [0, 0, 0, 0]
                 VB.units = self.unit
                 VB.min = per[0]
                 VB.dn99 = per[1]
                 VB.up99 = per[2]
                 VB.max = per[3]
                 VB[...] = self.data_bnds
-                
+
         # optionally write out more attributes
         if attributes:
             for key in attributes.keys():
-                V.setncattr(key,attributes[key])
+                V.setncattr(key, attributes[key])
 
-    def plot(self,ax,**keywords):
+    def plot(self, ax, **keywords):
         """Plots the variable on the given matplotlib axis.
 
         The behavior of this routine depends on the type of variable
         specified. If the data is purely temporal, then the plot will
         be a scatter plot versus time of the data. If it is purely
         spatial, then the plot will be a global plot of the data. The
         routine supports multiple keywords although some may not apply
@@ -1127,142 +1298,192 @@
             The name of the colormap to be used in plotting the spatial variable
         ticks : array of floats, optional
             Defines the locations of xtick
         ticklabels : array of strings, optional
             Defines the labels of the xticks
         """
         data = self.data if self.data_bnds is None else self.data_bnds
-        pad = 0.05*(data.max()-data.min())
-        lw     = keywords.get("lw"    ,1.0)
-        alpha  = keywords.get("alpha" ,1.0)
-        color  = keywords.get("color" ,"k")
-        label  = keywords.get("label" ,None)
-        vmin   = keywords.get("vmin"  ,data.min()-pad)
-        vmax   = keywords.get("vmax"  ,data.max()+pad)
-        region = keywords.get("region","global")
-        cmap   = keywords.get("cmap"  ,"jet")
-        land   = keywords.get("land"  ,0.875)
-        water  = keywords.get("water" ,0.750)
-        pad    = keywords.get("pad"   ,5.0)
-        cbar   = keywords.get("cbar"  ,False)
+        pad = 0.05 * (data.max() - data.min())
+        lw = keywords.get("lw", 1.0)
+        alpha = keywords.get("alpha", 1.0)
+        color = keywords.get("color", "k")
+        label = keywords.get("label", None)
+        vmin = keywords.get("vmin", data.min() - pad)
+        vmax = keywords.get("vmax", data.max() + pad)
+        region = keywords.get("region", "global")
+        cmap = keywords.get("cmap", "jet")
+        land = keywords.get("land", 0.875)
+        water = keywords.get("water", 0.750)
+        pad = keywords.get("pad", 5.0)
+        cbar = keywords.get("cbar", False)
 
         rem_mask = None
         r = Regions()
         if self.temporal and not self.spatial:
-
-            ticks      = keywords.get("ticks",None)
-            ticklabels = keywords.get("ticklabels",None)
-            t          = self.time/365.+1850
-            ax.plot(t,self.data,'-',
-                    color = color,
-                    lw    = lw,
-                    alpha = alpha,
-                    label = label)
+            ticks = keywords.get("ticks", None)
+            ticklabels = keywords.get("ticklabels", None)
+            t = self.time / 365.0 + 1850
+            ax.plot(t, self.data, "-", color=color, lw=lw, alpha=alpha, label=label)
             if self.data_bnds is not None:
-                ax.fill_between(t,self.data_bnds[:,0],self.data_bnds[:,1],
-                                lw = 0,
-                                color = color,
-                                alpha = 0.25*alpha)
-            if ticks      is not None: ax.set_xticks(ticks)
-            if ticklabels is not None: ax.set_xticklabels(ticklabels)
+                ax.fill_between(
+                    t,
+                    self.data_bnds[:, 0],
+                    self.data_bnds[:, 1],
+                    lw=0,
+                    color=color,
+                    alpha=0.25 * alpha,
+                )
+            if ticks is not None:
+                ax.set_xticks(ticks)
+            if ticklabels is not None:
+                ax.set_xticklabels(ticklabels)
             ax.grid(True)
 
-            if(abs(vmax-vmin) > 1e-14): ax.set_ylim(vmin,vmax)
-            if (vmax-vmin) > 1e-12: ax.set_ylim(vmin,vmax)
+            if abs(vmax - vmin) > 1e-14:
+                ax.set_ylim(vmin, vmax)
+            if (vmax - vmin) > 1e-12:
+                ax.set_ylim(vmin, vmax)
 
         elif not self.temporal:
-
             # mask out areas outside our region
-            rem_mask  = np.copy(self.data.mask)
-            self.data.mask += r.getMask(region,self)
+            rem_mask = np.copy(self.data.mask)
+            self.data.mask += r.getMask(region, self)
 
             # determine the plotting extents
             percent_pad = 0.2
             if self.ndata is None:
-                
-                lat_empty = np.where(self.data.mask.all(axis=-1)==False)[0]
-                lon_empty = np.where(self.data.mask.all(axis=-2)==False)[0]        
-                extents = [self.lon_bnds[lon_empty[ 0],0],
-                           self.lon_bnds[lon_empty[-1],1],
-                           self.lat_bnds[lat_empty[ 0],0],
-                           self.lat_bnds[lat_empty[-1],1]]
-                dx = percent_pad*(extents[1]-extents[0])
-                dy = percent_pad*(extents[3]-extents[2])
-                extents[0] = max(extents[0]-dx,-180); extents[1] = min(extents[1]+dx,+180)
-                extents[2] = max(extents[2]-dy,- 90); extents[3] = min(extents[3]+dy,+ 90)
-                lon_mid    = 0.5*(extents[0]+extents[1])
-                
+                lat_empty = np.where(self.data.mask.all(axis=-1) == False)[0]
+                lon_empty = np.where(self.data.mask.all(axis=-2) == False)[0]
+                extents = [
+                    self.lon_bnds[lon_empty[0], 0],
+                    self.lon_bnds[lon_empty[-1], 1],
+                    self.lat_bnds[lat_empty[0], 0],
+                    self.lat_bnds[lat_empty[-1], 1],
+                ]
+                dx = percent_pad * (extents[1] - extents[0])
+                dy = percent_pad * (extents[3] - extents[2])
+                extents[0] = max(extents[0] - dx, -180)
+                extents[1] = min(extents[1] + dx, +180)
+                extents[2] = max(extents[2] - dy, -90)
+                extents[3] = min(extents[3] + dy, +90)
+                lon_mid = 0.5 * (extents[0] + extents[1])
+
                 # ...but the data might cross the dateline, but not be global
-                if(lon_empty[ 0]== 0 and
-                   lon_empty[-1]==(self.lon.size-1) and
-                   np.diff(lon_empty).max() > 0.5*self.lon.size):
-                    wrap_lon  = self.lon[lon_empty]
-                    wrap_lon += (wrap_lon<0)*360
+                if (
+                    lon_empty[0] == 0
+                    and lon_empty[-1] == (self.lon.size - 1)
+                    and np.diff(lon_empty).max() > 0.5 * self.lon.size
+                ):
+                    wrap_lon = self.lon[lon_empty]
+                    wrap_lon += (wrap_lon < 0) * 360
                     extents[0] = wrap_lon.min()
                     extents[1] = wrap_lon.max()
-                    dx = percent_pad*(extents[1]-extents[0])
-                    extents[0] -= dx; extents[1] += dx
-                    
-                    # find the middle centroid by mean angle 
-                    lons = self.lon[np.where(self.data.mask.all(axis=-2)==False)[0]]
-                    lons = lons/360*2*np.pi
-                    lon_mid = np.arctan2(np.sin(lons).mean(),np.cos(lons).mean())/2/np.pi*360
-                
+                    dx = percent_pad * (extents[1] - extents[0])
+                    extents[0] -= dx
+                    extents[1] += dx
+
+                    # find the middle centroid by mean angle
+                    lons = self.lon[np.where(self.data.mask.all(axis=-2) == False)[0]]
+                    lons = lons / 360 * 2 * np.pi
+                    lon_mid = (
+                        np.arctan2(np.sin(lons).mean(), np.cos(lons).mean())
+                        / 2
+                        / np.pi
+                        * 360
+                    )
+
             else:
-                extents = [self.lon.min(),self.lon.max(),
-                           self.lat.min(),self.lat.max()]
-                dx = percent_pad*(extents[1]-extents[0])
-                dy = percent_pad*(extents[3]-extents[2])
-                extents[0] = max(extents[0]-dx,-180); extents[1] = min(extents[1]+dx,+180)
-                extents[2] = max(extents[2]-dy,- 90); extents[3] = min(extents[3]+dy,+ 90)
-                lon_mid = 0.5*(extents[0]+extents[1])
+                extents = [
+                    self.lon.min(),
+                    self.lon.max(),
+                    self.lat.min(),
+                    self.lat.max(),
+                ]
+                dx = percent_pad * (extents[1] - extents[0])
+                dy = percent_pad * (extents[3] - extents[2])
+                extents[0] = max(extents[0] - dx, -180)
+                extents[1] = min(extents[1] + dx, +180)
+                extents[2] = max(extents[2] - dy, -90)
+                extents[3] = min(extents[3] + dy, +90)
+                lon_mid = 0.5 * (extents[0] + extents[1])
 
             # choose a projection based on the non-masked data
             proj = ccrs.PlateCarree(central_longitude=lon_mid)
-            aspect_ratio = (extents[3]-extents[2])/(extents[1]-extents[0])
-            if (extents[1]-extents[0]) > 320:
-                if np.allclose(extents[2],-90) and extents[3] <= 0:
-                    proj = ccrs.Orthographic(central_latitude=-90,central_longitude=0)
-                    aspect_ratio = 1.
-                elif extents[3]>75 and extents[2] >= 0:
-                    proj = ccrs.Orthographic(central_latitude=+90,central_longitude=0)
-                    aspect_ratio = 1.
-                elif (extents[3]-extents[2]) > 140:
+            aspect_ratio = (extents[3] - extents[2]) / (extents[1] - extents[0])
+            if (extents[1] - extents[0]) > 320:
+                if np.allclose(extents[2], -90) and extents[3] <= 0:
+                    proj = ccrs.Orthographic(central_latitude=-90, central_longitude=0)
+                    aspect_ratio = 1.0
+                elif extents[3] > 75 and extents[2] >= 0:
+                    proj = ccrs.Orthographic(central_latitude=+90, central_longitude=0)
+                    aspect_ratio = 1.0
+                elif (extents[3] - extents[2]) > 140:
                     proj = ccrs.Robinson(central_longitude=0)
-                    extents = [-180,180,-90,90]
+                    extents = [-180, 180, -90, 90]
                     aspect_ratio = 0.5
-                    lon_mid = 0.
-                    
+                    lon_mid = 0.0
+
             # make the plot
-            w = 7.5; h = w*aspect_ratio
-            fig,ax = plt.subplots(figsize=(w,h),
-                                  subplot_kw={'projection':proj})
+            w = 7.5
+            h = w * aspect_ratio
+            fig, ax = plt.subplots(figsize=(w, h), subplot_kw={"projection": proj})
             if self.ndata is None:
-                lat = np.hstack([self.lat_bnds[:,0],self.lat_bnds[-1,-1]])
-                lon = np.hstack([self.lon_bnds[:,0],self.lon_bnds[-1,-1]])
-                p = ax.pcolormesh(lon,lat,self.data,cmap=cmap,vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())
+                lat = np.hstack([self.lat_bnds[:, 0], self.lat_bnds[-1, -1]])
+                lon = np.hstack([self.lon_bnds[:, 0], self.lon_bnds[-1, -1]])
+                p = ax.pcolormesh(
+                    lon,
+                    lat,
+                    self.data,
+                    cmap=cmap,
+                    vmin=vmin,
+                    vmax=vmax,
+                    transform=ccrs.PlateCarree(),
+                )
             else:
-                norm = colors.Normalize(vmin,vmax)
+                norm = colors.Normalize(vmin, vmax)
                 cmap = get_cmap(cmap)
                 clrs = cmap(norm(self.data))
-                p = ax.scatter(self.lon,self.lat,s=35,color=clrs,cmap=cmap,linewidths=0,
-                               transform=ccrs.PlateCarree())
-            ax.add_feature(cfeature.NaturalEarthFeature('physical','land','110m',
-                                                        edgecolor='face',
-                                                        facecolor='0.875'),zorder=-1)
-            ax.add_feature(cfeature.NaturalEarthFeature('physical','ocean','110m',
-                                                        edgecolor='face',
-                                                        facecolor='0.750'),zorder=-1)
-            ax.set_extent(extents,ccrs.PlateCarree())
-            if cbar: fig.colorbar(p,orientation='horizontal',pad=0.05,label=label)
-            if rem_mask is not None: self.data.mask = rem_mask
-            
+                p = ax.scatter(
+                    self.lon,
+                    self.lat,
+                    s=35,
+                    color=clrs,
+                    cmap=cmap,
+                    linewidths=0,
+                    transform=ccrs.PlateCarree(),
+                )
+            ax.add_feature(
+                cfeature.NaturalEarthFeature(
+                    "physical", "land", "110m", edgecolor="face", facecolor="0.875"
+                ),
+                zorder=-1,
+            )
+            ax.add_feature(
+                cfeature.NaturalEarthFeature(
+                    "physical", "ocean", "110m", edgecolor="face", facecolor="0.750"
+                ),
+                zorder=-1,
+            )
+            ax.set_extent(extents, ccrs.PlateCarree())
+            if cbar:
+                fig.colorbar(p, orientation="horizontal", pad=0.05, label=label)
+            if rem_mask is not None:
+                self.data.mask = rem_mask
+
         return ax
-    
-    def interpolate(self,time=None,lat=None,lon=None,lat_bnds=None,lon_bnds=None,itype='nearestneighbor'):
+
+    def interpolate(
+        self,
+        time=None,
+        lat=None,
+        lon=None,
+        lat_bnds=None,
+        lon_bnds=None,
+        itype="nearestneighbor",
+    ):
         """Use nearest-neighbor interpolation to interpolate time and/or space at given values.
 
         Parameters
         ----------
         time : numpy.ndarray, optional
             Array of times at which to interpolate the variable
         lat : numpy.ndarray, optional
@@ -1271,82 +1492,103 @@
             Array of longitudes at which to interpolate the variable
 
         Returns
         -------
         var : ILAMB.Variable.Variable
             The interpolated variable
         """
-        if time is None and lat is None and lon is None: return self
-        output_time = self.time      if (time is None) else time
+        if time is None and lat is None and lon is None:
+            return self
+        output_time = self.time if (time is None) else time
         output_tbnd = self.time_bnds if (time is None) else None
-        output_lat  = self.lat       if (lat  is None) else lat
-        output_lon  = self.lon       if (lon  is None) else lon
-        output_area = self.area      if (lat  is None and lon is None) else None
+        output_lat = self.lat if (lat is None) else lat
+        output_lon = self.lon if (lon is None) else lon
+        output_area = self.area if (lat is None and lon is None) else None
 
         data = self.data
         data_bnds = None
         if self.spatial and (lat is not None or lon is not None):
-            if lat is None: lat = self.lat
-            if lon is None: lon = self.lon
-            if itype == 'nearestneighbor':
-                rows  = (np.abs(lat[:,np.newaxis]-self.lat)).argmin(axis=1)
-                cols  = (np.abs(lon[:,np.newaxis]-self.lon)).argmin(axis=1)
-                args  = []
-                if self.temporal: args.append(range(self.time.size))
-                if self.layered:  args.append(range(self.depth.size))
+            if lat is None:
+                lat = self.lat
+            if lon is None:
+                lon = self.lon
+            if itype == "nearestneighbor":
+                rows = (np.abs(lat[:, np.newaxis] - self.lat)).argmin(axis=1)
+                cols = (np.abs(lon[:, np.newaxis] - self.lon)).argmin(axis=1)
+                args = []
+                if self.temporal:
+                    args.append(range(self.time.size))
+                if self.layered:
+                    args.append(range(self.depth.size))
                 args.append(rows)
                 args.append(cols)
-                ind   = np.ix_(*args)
-                mask  = data.mask
-                if data.mask.size > 1: mask = data.mask[ind]
-                data  = data.data[ind]
-                data  = np.ma.masked_array(data,mask=mask)
+                ind = np.ix_(*args)
+                mask = data.mask
+                if data.mask.size > 1:
+                    mask = data.mask[ind]
+                data = data.data[ind]
+                data = np.ma.masked_array(data, mask=mask)
                 if self.data_bnds is not None:
-                    args.append([0,1])
+                    args.append([0, 1])
                     ind = np.ix_(*args)
                     data_bnds = self.data_bnds[ind]
-                np.seterr(under='ignore',over='ignore')
-                frac  = self.area / il.CellAreas(self.lat,self.lon,self.lat_bnds,self.lon_bnds).clip(1e-12)
-                np.seterr(under='raise',over='raise')
-                frac  = frac.clip(0,1)
-                frac  = frac[np.ix_(rows,cols)]
-                output_area = frac * il.CellAreas(lat,lon,lat_bnds,lon_bnds)
-            elif itype == 'bilinear':
+                np.seterr(under="ignore", over="ignore")
+                frac = self.area / il.CellAreas(
+                    self.lat, self.lon, self.lat_bnds, self.lon_bnds
+                ).clip(1e-12)
+                np.seterr(under="raise", over="raise")
+                frac = frac.clip(0, 1)
+                frac = frac[np.ix_(rows, cols)]
+                output_area = frac * il.CellAreas(lat, lon, lat_bnds, lon_bnds)
+            elif itype == "bilinear":
                 from scipy.interpolate import RectBivariateSpline
+
                 if self.data.ndim == 3:
                     halo = il.LandLinInterMissingValues(self.data)
-                    data = np.ma.zeros((self.data.shape[:-2]+(lat.size,lon.size)))
+                    data = np.ma.zeros((self.data.shape[:-2] + (lat.size, lon.size)))
                     for i in range(self.data.shape[0]):
-                        dint = RectBivariateSpline(self.lat,self.lon,     halo[i,...],     kx=1,ky=1)
-                        mint = RectBivariateSpline(self.lat,self.lon,self.data[i,...].mask,kx=1,ky=1)
-                        data[i,...] = np.ma.masked_array(dint(lat,lon,grid=True),
-                                                         mint(lat,lon,grid=True)>0.5)
-                frac  = self.area / il.CellAreas(self.lat,self.lon).clip(1e-12)
-                frac  = frac.clip(0,1)
-                frac  = RectBivariateSpline(self.lat,self.lon,frac,kx=1,ky=1)
-                output_area = frac(lat,lon,grid=True) * il.CellAreas(lat,lon)
+                        dint = RectBivariateSpline(
+                            self.lat, self.lon, halo[i, ...], kx=1, ky=1
+                        )
+                        mint = RectBivariateSpline(
+                            self.lat, self.lon, self.data[i, ...].mask, kx=1, ky=1
+                        )
+                        data[i, ...] = np.ma.masked_array(
+                            dint(lat, lon, grid=True), mint(lat, lon, grid=True) > 0.5
+                        )
+                frac = self.area / il.CellAreas(self.lat, self.lon).clip(1e-12)
+                frac = frac.clip(0, 1)
+                frac = RectBivariateSpline(self.lat, self.lon, frac, kx=1, ky=1)
+                output_area = frac(lat, lon, grid=True) * il.CellAreas(lat, lon)
             else:
                 raise ValueError("Uknown interpolation type: %s" % itype)
         if self.temporal and time is not None:
-            times = np.apply_along_axis(np.argmin,1,np.abs(time[:,np.newaxis]-self.time))
-            mask  = data.mask
-            if mask.size > 1: mask = data.mask[times,...]
-            data  = data.data[times,...]
-            data  = np.ma.masked_array(data,mask=mask)
+            times = np.apply_along_axis(
+                np.argmin, 1, np.abs(time[:, np.newaxis] - self.time)
+            )
+            mask = data.mask
+            if mask.size > 1:
+                mask = data.mask[times, ...]
+            data = data.data[times, ...]
+            data = np.ma.masked_array(data, mask=mask)
             output_tbnd = self.time_bnds[times]
-        return Variable(data = data,
-                        data_bnds = data_bnds,
-                        unit = self.unit, name = self.name, ndata = self.ndata,
-                        lat  = output_lat,
-                        lon  = output_lon,
-                        area = output_area,
-                        time = output_time,
-                        time_bnds = output_tbnd)
+        return Variable(
+            data=data,
+            data_bnds=data_bnds,
+            unit=self.unit,
+            name=self.name,
+            ndata=self.ndata,
+            lat=output_lat,
+            lon=output_lon,
+            area=output_area,
+            time=output_time,
+            time_bnds=output_tbnd,
+        )
 
-    def phaseShift(self,var,method="max_of_annual_cycle"):
+    def phaseShift(self, var, method="max_of_annual_cycle"):
         """Computes the phase shift between a variable and this variable.
 
         Finds the phase shift as the time between extrema of the
         annual cycles of the variables. Note that if this var and/or
         the given variable are not already annual cycles, they will be
         computed but not returned.
 
@@ -1354,46 +1596,52 @@
         ----------
         var : ILAMB.Variable.Variable
             The variable with which we will measure phase shift
         method : str, optional
             The name of the method used to compute the phase shift
 
         """
-        assert method in ["max_of_annual_cycle","min_of_annual_cycle"]
+        assert method in ["max_of_annual_cycle", "min_of_annual_cycle"]
         assert self.temporal == var.temporal
-        v1 = self; v2 = var
+        v1 = self
+        v2 = var
         if not self.temporal:
             # If the data is not temporal, then the user may have
             # already found the extrema. If the units of the input
             # variable are days, then set the extrema to this data.
-            if not (self.unit == "d" and var.unit == "d"): raise il.NotTemporalVariable
+            if not (self.unit == "d" and var.unit == "d"):
+                raise il.NotTemporalVariable
             e1 = v1
             e2 = v2
         else:
             # While temporal, the user may have passed in the mean
             # annual cycle as the variable. So if the leading
             # dimension is 12 we assume the variables are already the
             # annual cycles. If not, we compute the cycles and then
             # compute the extrema.
-            if self.time.size != 12: v1 = self.annualCycle()
-            if  var.time.size != 12: v2 = var .annualCycle()
+            if self.time.size != 12:
+                v1 = self.annualCycle()
+            if var.time.size != 12:
+                v2 = var.annualCycle()
             e1 = v1.timeOfExtrema(etype=method[:3])
             e2 = v2.timeOfExtrema(etype=method[:3])
         if e1.spatial:
             shift = e1.spatialDifference(e2)
         else:
-            data  = e2.data      - e1.data
-            mask  = e1.data.mask + e2.data.mask
-            shift = Variable(data=data,unit=e1.unit,ndata=e1.ndata,lat=e1.lat,lon=e1.lon)
+            data = e2.data - e1.data
+            mask = e1.data.mask + e2.data.mask
+            shift = Variable(
+                data=data, unit=e1.unit, ndata=e1.ndata, lat=e1.lat, lon=e1.lon
+            )
         shift.name = "phase_shift_of_%s" % e1.name
-        shift.data += (shift.data < -0.5*365.)*365.
-        shift.data -= (shift.data > +0.5*365.)*365.
+        shift.data += (shift.data < -0.5 * 365.0) * 365.0
+        shift.data -= (shift.data > +0.5 * 365.0) * 365.0
         return shift
 
-    def correlation(self,var,ctype,region=None):
+    def correlation(self, var, ctype, region=None):
         """Computes the correlation between two variables.
 
         Parameters
         ----------
         var : ILAMB.Variable.Variable
             The variable with which we will compute a correlation
         ctype : str
@@ -1405,122 +1653,152 @@
         -----
         Need to better think about what correlation means when data
         are masked. The sums ignore the data but then the number of
         items *n* is not constant and should be reduced for masked
         values.
 
         """
-        def _correlation(x,y,axes=None):
-            if axes is None: axes = range(x.ndim)
-            if type(axes) == int: axes = (int(axes),)
+
+        def _correlation(x, y, axes=None):
+            if axes is None:
+                axes = range(x.ndim)
+            if type(axes) == int:
+                axes = (int(axes),)
             axes = tuple(axes)
-            n    = 1
-            for ax in axes: n *= x.shape[ax]
-            xbar = x.sum(axis=axes)/n # because np.mean() doesn't take axes which are tuples
-            ybar = y.sum(axis=axes)/n
-            xy   = (x*y).sum(axis=axes)
-            x2   = (x*x).sum(axis=axes)
-            y2   = (y*y).sum(axis=axes)
+            n = 1
+            for ax in axes:
+                n *= x.shape[ax]
+            xbar = (
+                x.sum(axis=axes) / n
+            )  # because np.mean() doesn't take axes which are tuples
+            ybar = y.sum(axis=axes) / n
+            xy = (x * y).sum(axis=axes)
+            x2 = (x * x).sum(axis=axes)
+            y2 = (y * y).sum(axis=axes)
             try:
-                np.seterr(under='ignore',invalid='ignore')
-                r = (xy-n*xbar*ybar)/(np.sqrt(x2-n*xbar*xbar)*np.sqrt(y2-n*ybar*ybar))
-                np.seterr(under='raise',invalid='warn')
+                np.seterr(under="ignore", invalid="ignore")
+                r = (xy - n * xbar * ybar) / (
+                    np.sqrt(x2 - n * xbar * xbar) * np.sqrt(y2 - n * ybar * ybar)
+                )
+                np.seterr(under="raise", invalid="warn")
             except:
                 r = np.nan
             return r
 
         # checks on data consistency
         assert region is None
         assert self.data.shape == var.data.shape
-        assert ctype in ["spatial","temporal","spatiotemporal"]
+        assert ctype in ["spatial", "temporal", "spatiotemporal"]
 
         # determine arguments for functions
-        axes      = None
-        out_time  = None
-        out_lat   = None
-        out_lon   = None
-        out_area  = None
+        axes = None
+        out_time = None
+        out_lat = None
+        out_lon = None
+        out_area = None
         out_ndata = None
         if ctype == "temporal":
             axes = 0
             if self.spatial:
-                out_lat   = self.lat
-                out_lon   = self.lon
-                out_area  = self.area
+                out_lat = self.lat
+                out_lon = self.lon
+                out_area = self.area
             elif self.ndata:
                 out_ndata = self.ndata
         elif ctype == "spatial":
-            if self.spatial:  axes     = range(self.data.ndim)[-2:]
-            if self.ndata:    axes     = self.data.ndim-1
-            if self.temporal: out_time = self.time
+            if self.spatial:
+                axes = range(self.data.ndim)[-2:]
+            if self.ndata:
+                axes = self.data.ndim - 1
+            if self.temporal:
+                out_time = self.time
         out_time_bnds = None
-        if out_time is not None: out_time_bnds = self.time_bnds
-        r = _correlation(self.data,var.data,axes=axes)
-        return Variable(data=r,unit="1",
-                        name="%s_correlation_of_%s" % (ctype,self.name),
-                        time=out_time,time_bnds=out_time_bnds,ndata=out_ndata,
-                        lat=out_lat,lon=out_lon,area=out_area)
+        if out_time is not None:
+            out_time_bnds = self.time_bnds
+        r = _correlation(self.data, var.data, axes=axes)
+        return Variable(
+            data=r,
+            unit="1",
+            name="%s_correlation_of_%s" % (ctype, self.name),
+            time=out_time,
+            time_bnds=out_time_bnds,
+            ndata=out_ndata,
+            lat=out_lat,
+            lon=out_lon,
+            area=out_area,
+        )
 
-    def bias(self,var):
+    def bias(self, var):
         """Computes the bias between a given variable and this variable.
 
         Parameters
         ----------
         var : ILAMB.Variable.Variable
             The variable with which we will measure bias
 
         Returns
         -------
         bias : ILAMB.Variable.Variable
             the bias
         """
         # If not a temporal variable, then we assume that the user is
         # passing in mean data and return the difference.
-        lat,lon,area = self.lat,self.lon,self.area
+        lat, lon, area = self.lat, self.lon, self.area
         if not self.temporal:
             assert self.temporal == var.temporal
             bias = self.spatialDifference(var)
             bias.name = "bias_of_%s" % self.name
             return bias
         if self.spatial:
             # If the data is spatial, then we interpolate it on a
             # common grid and take the difference.
 
             same_grid = False
             try:
-                same_grid = np.allclose(self.lat,var.lat)*np.allclose(self.lon,var.lon)
+                same_grid = np.allclose(self.lat, var.lat) * np.allclose(
+                    self.lon, var.lon
+                )
             except:
                 pass
             if not same_grid:
-                lat,lon  = il.ComposeSpatialGrids(self,var)
-                area     = None
-                self_int = self.interpolate(lat=lat,lon=lon)
-                var_int  = var .interpolate(lat=lat,lon=lon)
-                data     = var_int.data-self_int.data
-                mask     = var_int.data.mask+self_int.data.mask
+                lat, lon = il.ComposeSpatialGrids(self, var)
+                area = None
+                self_int = self.interpolate(lat=lat, lon=lon)
+                var_int = var.interpolate(lat=lat, lon=lon)
+                data = var_int.data - self_int.data
+                mask = var_int.data.mask + self_int.data.mask
             else:
-                data     = var.data     -self.data
-                mask     = var.data.mask+self.data.mask
+                data = var.data - self.data
+                mask = var.data.mask + self.data.mask
 
-        elif (self.ndata or self.time.size == self.data.size):
+        elif self.ndata or self.time.size == self.data.size:
             # If the data are at sites, then take the difference
-            data = var.data.data-self.data.data
-            mask = var.data.mask+self.data.mask
+            data = var.data.data - self.data.data
+            mask = var.data.mask + self.data.mask
         else:
             raise il.NotSpatialVariable("Cannot take bias of scalars")
         # Finally we return the temporal mean of the difference
-        bias = Variable(data=np.ma.masked_array(data,mask=mask),
-                        name="bias_of_%s" % self.name,time=self.time,time_bnds=self.time_bnds,
-                        unit=self.unit,ndata=self.ndata,
-                        lat=lat,lon=lon,area=area,
-                        depth_bnds = self.depth_bnds).integrateInTime(mean=True)
-        bias.name = bias.name.replace("_integrated_over_time_and_divided_by_time_period","")
+        bias = Variable(
+            data=np.ma.masked_array(data, mask=mask),
+            name="bias_of_%s" % self.name,
+            time=self.time,
+            time_bnds=self.time_bnds,
+            unit=self.unit,
+            ndata=self.ndata,
+            lat=lat,
+            lon=lon,
+            area=area,
+            depth_bnds=self.depth_bnds,
+        ).integrateInTime(mean=True)
+        bias.name = bias.name.replace(
+            "_integrated_over_time_and_divided_by_time_period", ""
+        )
         return bias
 
-    def rmse(self,var):
+    def rmse(self, var):
         """Computes the RMSE between a given variable and this variable.
 
         Parameters
         ----------
         var : ILAMB.Variable.Variable
             The variable with which we will measure RMSE
 
@@ -1528,123 +1806,149 @@
         -------
         RMSE : ILAMB.Variable.Variable
             the RMSE
 
         """
         # If not a temporal variable, then we assume that the user is
         # passing in mean data and return the difference.
-        lat,lon,area = self.lat,self.lon,self.area
+        lat, lon, area = self.lat, self.lon, self.area
         if not self.temporal:
             assert self.temporal == var.temporal
             rmse = self.spatialDifference(var)
             rmse.name = "rmse_of_%s" % self.name
             return rmse
         if self.spatial:
             # If the data is spatial, then we interpolate it on a
             # common grid and take the difference.
             same_grid = False
             try:
-                same_grid = np.allclose(self.lat,var.lat)*np.allclose(self.lon,var.lon)
+                same_grid = np.allclose(self.lat, var.lat) * np.allclose(
+                    self.lon, var.lon
+                )
             except:
                 pass
             if not same_grid:
-                lat,lon  = il.ComposeSpatialGrids(self,var)
-                area     = None
-                self_int = self.interpolate(lat=lat,lon=lon)
-                var_int  = var .interpolate(lat=lat,lon=lon)
-                data     = var_int.data-self_int.data
-                mask     = var_int.data.mask+self_int.data.mask
+                lat, lon = il.ComposeSpatialGrids(self, var)
+                area = None
+                self_int = self.interpolate(lat=lat, lon=lon)
+                var_int = var.interpolate(lat=lat, lon=lon)
+                data = var_int.data - self_int.data
+                mask = var_int.data.mask + self_int.data.mask
             else:
-                data     = var.data     -self.data
-                mask     = var.data.mask+self.data.mask
-        elif (self.ndata or self.time.size == self.data.size):
+                data = var.data - self.data
+                mask = var.data.mask + self.data.mask
+        elif self.ndata or self.time.size == self.data.size:
             # If the data are at sites, then take the difference
-            data = var.data.data-self.data.data
-            mask = var.data.mask+self.data.mask
+            data = var.data.data - self.data.data
+            mask = var.data.mask + self.data.mask
         else:
             raise il.NotSpatialVariable("Cannot take rmse of scalars")
         # Finally we return the temporal mean of the difference squared
-        np.seterr(over='ignore',under='ignore')
+        np.seterr(over="ignore", under="ignore")
         data *= data
-        np.seterr(over='raise',under='raise')
-        rmse = Variable(data=np.ma.masked_array(data,mask=mask),
-                        name="rmse_of_%s" % self.name,time=self.time,time_bnds=self.time_bnds,
-                        unit=self.unit,ndata=self.ndata,
-                        lat=lat,lon=lon,area=area,
-                        depth_bnds = self.depth_bnds).integrateInTime(mean=True)
-        rmse.name = rmse.name.replace("_integrated_over_time_and_divided_by_time_period","")
+        np.seterr(over="raise", under="raise")
+        rmse = Variable(
+            data=np.ma.masked_array(data, mask=mask),
+            name="rmse_of_%s" % self.name,
+            time=self.time,
+            time_bnds=self.time_bnds,
+            unit=self.unit,
+            ndata=self.ndata,
+            lat=lat,
+            lon=lon,
+            area=area,
+            depth_bnds=self.depth_bnds,
+        ).integrateInTime(mean=True)
+        rmse.name = rmse.name.replace(
+            "_integrated_over_time_and_divided_by_time_period", ""
+        )
         rmse.data = np.sqrt(rmse.data)
         return rmse
 
     def rms(self):
         """Computes the RMS of this variable.
 
         Returns
         -------
         RMS : ILAMB.Variable.Variable
             the RMS
 
         """
-        if not self.temporal: raise il.NotTemporalVariable()
+        if not self.temporal:
+            raise il.NotTemporalVariable()
         unit = self.unit
-        np.seterr(over='ignore',under='ignore')
+        np.seterr(over="ignore", under="ignore")
         data = self.data**2
-        np.seterr(over='raise',under='raise')
-        rms  = Variable(data  = data,
-                        unit  = "1",    # will change later
-                        name  = "tmp",  # will change later
-                        ndata = self.ndata,
-                        lat   = self.lat,
-                        lon   = self.lon,
-                        area  = self.area,
-                        time  = self.time).integrateInTime(mean=True)
-        np.seterr(over='ignore',under='ignore')
+        np.seterr(over="raise", under="raise")
+        rms = Variable(
+            data=data,
+            unit="1",  # will change later
+            name="tmp",  # will change later
+            ndata=self.ndata,
+            lat=self.lat,
+            lon=self.lon,
+            area=self.area,
+            time=self.time,
+        ).integrateInTime(mean=True)
+        np.seterr(over="ignore", under="ignore")
         rms.data = np.sqrt(rms.data)
-        np.seterr(over='raise',under='raise')
+        np.seterr(over="raise", under="raise")
         rms.unit = unit
         rms.name = "rms_of_%s" % self.name
         return rms
 
-    def variability(self,mean=None):
-        """Computes the variability of the variable.
-        """
-        if not self.temporal: raise il.NotTemporalVariable
-        if mean is None: mean = self.integrateInTime(mean=True)
-        var = Variable(unit = self.unit,
-                       data = self.data - mean.data[np.newaxis,...],
-                       time = self.time, time_bnds = self.time_bnds,
-                       lat  = self.lat, lat_bnds = self.lat_bnds,
-                       lon  = self.lon, lon_bnds = self.lon_bnds,
-                       area = self.area,
-                       ndata = self.ndata).rms()
+    def variability(self, mean=None):
+        """Computes the variability of the variable."""
+        if not self.temporal:
+            raise il.NotTemporalVariable
+        if mean is None:
+            mean = self.integrateInTime(mean=True)
+        var = Variable(
+            unit=self.unit,
+            data=self.data - mean.data[np.newaxis, ...],
+            time=self.time,
+            time_bnds=self.time_bnds,
+            lat=self.lat,
+            lat_bnds=self.lat_bnds,
+            lon=self.lon,
+            lon_bnds=self.lon_bnds,
+            area=self.area,
+            ndata=self.ndata,
+        ).rms()
         var.name = "var_of_%s" % self.name
         return var
-        
+
     def interannualVariability(self):
         """Computes the interannual variability.
 
         The internannual variability in this case is defined as the
         standard deviation of the data in the temporal dimension.
 
         Returns
         -------
         iav : ILAMB.Variable.Variable
             the interannual variability variable
         """
-        if not self.temporal: raise il.NotTemporalVariable
-        np.seterr(over='ignore',under='ignore')
+        if not self.temporal:
+            raise il.NotTemporalVariable
+        np.seterr(over="ignore", under="ignore")
         data = self.data.std(axis=0)
-        np.seterr(over='raise',under='raise')
-        return Variable(data=data,
-                        name="iav_of_%s" % self.name,
-                        unit=self.unit,ndata=self.ndata,
-                        lat=self.lat,lon=self.lon,area=self.area,
-                        depth_bnds = self.depth_bnds)
+        np.seterr(over="raise", under="raise")
+        return Variable(
+            data=data,
+            name="iav_of_%s" % self.name,
+            unit=self.unit,
+            ndata=self.ndata,
+            lat=self.lat,
+            lon=self.lon,
+            area=self.area,
+            depth_bnds=self.depth_bnds,
+        )
 
-    def spatialDistribution(self,var,region="global"):
+    def spatialDistribution(self, var, region="global"):
         r"""Evaluates how well the input variable is spatially distributed relative to this variable.
 
         This routine returns the normalized standard deviation and
         correlation (needed for a Taylor plot) as well as a score
         given as
 
         .. math:: \frac{4(1+R)}{((\sigma+\frac{1}{\sigma})^2 (1+R_0))}
@@ -1671,61 +1975,70 @@
 
         """
         assert self.temporal == var.temporal == False
 
         r = Regions()
 
         # First compute the observational spatial/site standard deviation
-        rem_mask0  = np.copy(self.data.mask)
-        self.data.mask += r.getMask(region,self)
+        rem_mask0 = np.copy(self.data.mask)
+        self.data.mask += r.getMask(region, self)
 
-        np.seterr(over='ignore',under='ignore')
+        np.seterr(over="ignore", under="ignore")
         std0 = self.data.std()
-        np.seterr(over='raise',under='raise')
+        np.seterr(over="raise", under="raise")
 
         # Next compute the model spatial/site standard deviation
-        rem_mask  = np.copy(var.data.mask)
-        var.data.mask += r.getMask(region,var)
+        rem_mask = np.copy(var.data.mask)
+        var.data.mask += r.getMask(region, var)
 
-        np.seterr(over='ignore',under='ignore')
+        np.seterr(over="ignore", under="ignore")
         std = var.data.std()
-        np.seterr(over='raise',under='raise')
+        np.seterr(over="raise", under="raise")
 
         # Interpolate to new grid for correlation
         if self.spatial:
-            lat,lon  = il.ComposeSpatialGrids(self,var)
-            self_int = self.interpolate(lat=lat,lon=lon)
-            var_int  = var .interpolate(lat=lat,lon=lon)
+            lat, lon = il.ComposeSpatialGrids(self, var)
+            self_int = self.interpolate(lat=lat, lon=lon)
+            var_int = var.interpolate(lat=lat, lon=lon)
         else:
             self_int = self
-            var_int  = var
-        R   = self_int.correlation(var_int,ctype="spatial") # add regions
-        if type(R.data) is np.ma.core.MaskedConstant: R.data = 0.
+            var_int = var
+        R = self_int.correlation(var_int, ctype="spatial")  # add regions
+        if type(R.data) is np.ma.core.MaskedConstant:
+            R.data = 0.0
 
         # Restore masks
         self.data.mask = rem_mask0
-        var.data.mask  = rem_mask
+        var.data.mask = rem_mask
 
         # Put together scores, we clip the standard deviation of both
         # variables at the same small amount, meant to avoid division
         # by zero errors.
         try:
-            R0    = 1.0
-            std0  = std0.clip(1e-12)
-            std   = std .clip(1e-12)
-            std   = std/std0
-            score = 4.0*(1.0+R.data)/((std+1.0/std)**2 *(1.0+R0))
+            R0 = 1.0
+            std0 = std0.clip(1e-12)
+            std = std.clip(1e-12)
+            std = std / std0
+            score = 4.0 * (1.0 + R.data) / ((std + 1.0 / std) ** 2 * (1.0 + R0))
         except:
-            std   = np.asarray([0.0])
+            std = np.asarray([0.0])
             score = np.asarray([0.0])
-        std   = Variable(data=std  ,name="normalized_spatial_std_of_%s_over_%s" % (self.name,region),unit="1")
-        score = Variable(data=score,name="spatial_distribution_score_of_%s_over_%s" % (self.name,region),unit="1")
-        return std,R,score
+        std = Variable(
+            data=std,
+            name="normalized_spatial_std_of_%s_over_%s" % (self.name, region),
+            unit="1",
+        )
+        score = Variable(
+            data=score,
+            name="spatial_distribution_score_of_%s_over_%s" % (self.name, region),
+            unit="1",
+        )
+        return std, R, score
 
-    def coarsenInTime(self,intervals,window=0.):
+    def coarsenInTime(self, intervals, window=0.0):
         """Compute the mean function value in each of the input intervals.
 
         Parameters
         ----------
         intervals : array of shape (n,2)
             An array of n intervals where the first entry is the
             beginning and the second entry is the end of the interval
@@ -1733,36 +2046,39 @@
             Extend each interval before and after by this amount of time
 
         Returns
         -------
         coarse : ILAMB.Variable.Variable
             The coarsened variable
         """
-        if not self.temporal: raise il.NotTemporalVariable
+        if not self.temporal:
+            raise il.NotTemporalVariable
         assert intervals.ndim == 2
-        n    = intervals.shape[0]
-        shp  = (n,)+self.data.shape[1:]
+        n = intervals.shape[0]
+        shp = (n,) + self.data.shape[1:]
         time = np.zeros(n)
         data = np.ma.zeros(shp)
         for i in range(n):
-            t0          = intervals[i,0]-window
-            tf          = intervals[i,1]+window
-            time[i]     = 0.5*(t0+tf)
-            mean        = self.integrateInTime(mean=True,t0=t0,tf=tf).convert(self.unit)
-            data[i,...] = mean.data
-        return Variable(name       = "coarsened_%s" % self.name,
-                        unit       = self.unit,
-                        time       = time,
-                        time_bnds  = intervals,
-                        data       = data,
-                        ndata      = self.ndata,
-                        lat        = self.lat,
-                        lon        = self.lon,
-                        area       = self.area,
-                        depth_bnds = self.depth_bnds)
+            t0 = intervals[i, 0] - window
+            tf = intervals[i, 1] + window
+            time[i] = 0.5 * (t0 + tf)
+            mean = self.integrateInTime(mean=True, t0=t0, tf=tf).convert(self.unit)
+            data[i, ...] = mean.data
+        return Variable(
+            name="coarsened_%s" % self.name,
+            unit=self.unit,
+            time=time,
+            time_bnds=intervals,
+            data=data,
+            ndata=self.ndata,
+            lat=self.lat,
+            lon=self.lon,
+            area=self.area,
+            depth_bnds=self.depth_bnds,
+        )
 
     def accumulateInTime(self):
         r"""For each time interval, accumulate variable from the beginning.
 
         For each time interval :math:`i` in the variable, defined by
         :math:`[t_0^i,t_f^i]`, compute
 
@@ -1774,84 +2090,93 @@
 
         Returns
         -------
         sum : ILAMB.Variable.Variable
             The cumulative sum of this variable
 
         """
-        if not self.temporal: raise il.NotTemporalVariable
-        n       = self.time.size
-        shp     = (n+1,) + self.data.shape[1:]
-        time    = np.zeros(n+1)
-        data    = np.ma.zeros(shp)
-        data_bnds = None if self.data_bnds is None else np.ma.zeros(shp+(2,))
-        time[0] = self.time_bnds[0,0]
+        if not self.temporal:
+            raise il.NotTemporalVariable
+        n = self.time.size
+        shp = (n + 1,) + self.data.shape[1:]
+        time = np.zeros(n + 1)
+        data = np.ma.zeros(shp)
+        data_bnds = None if self.data_bnds is None else np.ma.zeros(shp + (2,))
+        time[0] = self.time_bnds[0, 0]
         for i in range(n):
-            t0   = self.time_bnds[i,0]
-            tf   = self.time_bnds[i,1]
-            isum = self.integrateInTime(t0=t0,tf=tf)
-            time[i+1]     = tf
-            data[i+1,...] = data[i,...] + isum.data
+            t0 = self.time_bnds[i, 0]
+            tf = self.time_bnds[i, 1]
+            isum = self.integrateInTime(t0=t0, tf=tf)
+            time[i + 1] = tf
+            data[i + 1, ...] = data[i, ...] + isum.data
             if data_bnds is not None:
-                data_bnds[i+1,...] = data_bnds[i,...] + isum.data_bnds
-        return Variable(name      = "cumulative_sum_%s" % self.name,
-                        unit      = isum.unit,
-                        time      = time,
-                        data      = data,
-                        data_bnds  = data_bnds,
-                        lat       = self.lat,
-                        lon       = self.lon,
-                        area      = self.area)
+                data_bnds[i + 1, ...] = data_bnds[i, ...] + isum.data_bnds
+        return Variable(
+            name="cumulative_sum_%s" % self.name,
+            unit=isum.unit,
+            time=time,
+            data=data,
+            data_bnds=data_bnds,
+            lat=self.lat,
+            lon=self.lon,
+            area=self.area,
+        )
 
-
-    def trim(self,lat=None,lon=None,t=None,d=None):
+    def trim(self, lat=None, lon=None, t=None, d=None):
         """Trim away a variable in space/time in place.
 
         Parameters
         ----------
         lat,lon,t,d : tuple or list
             a 2-tuple containing the lower and upper limits beyond which we trim
         """
-        def _whichInterval(val,bnds):
-            if val < bnds.min(): val = bnds.min()
-            if val > bnds.max(): val = bnds.max()            
-            ind = np.where((val>=bnds[:,0])*(val<=bnds[:,1]))[0]
+
+        def _whichInterval(val, bnds):
+            if val < bnds.min():
+                val = bnds.min()
+            if val > bnds.max():
+                val = bnds.max()
+            ind = np.where((val >= bnds[:, 0]) * (val <= bnds[:, 1]))[0]
             assert ind.size <= 2
             ind = ind[0]
             return ind
 
         if lat is not None:
             assert len(lat) == 2
-            if not self.spatial: raise il.NotSpatialVariable
-            i = _whichInterval(lat[0],self.lat_bnds)
-            j = _whichInterval(lat[1],self.lat_bnds)+1
-            self.lat      = self.lat     [i:j]
+            if not self.spatial:
+                raise il.NotSpatialVariable
+            i = _whichInterval(lat[0], self.lat_bnds)
+            j = _whichInterval(lat[1], self.lat_bnds) + 1
+            self.lat = self.lat[i:j]
             self.lat_bnds = self.lat_bnds[i:j]
-            self.data     = self.data[...,i:j,:]
+            self.data = self.data[..., i:j, :]
             if self.data_bnds is not None:
-                self.data_bnds = self.data_bnds[...,i:j,:,:]                
-            self.area     = self.area[    i:j,:]
+                self.data_bnds = self.data_bnds[..., i:j, :, :]
+            self.area = self.area[i:j, :]
         if lon is not None:
             assert len(lon) == 2
-            if not self.spatial: raise il.NotSpatialVariable
-            i = _whichInterval(lon[0],self.lon_bnds)
-            j = _whichInterval(lon[1],self.lon_bnds)+1
-            self.lon      = self.lon     [i:j]
+            if not self.spatial:
+                raise il.NotSpatialVariable
+            i = _whichInterval(lon[0], self.lon_bnds)
+            j = _whichInterval(lon[1], self.lon_bnds) + 1
+            self.lon = self.lon[i:j]
             self.lon_bnds = self.lon_bnds[i:j]
-            self.data     = self.data[...,i:j]
+            self.data = self.data[..., i:j]
             if self.data_bnds is not None:
-                self.data_bnds = self.data_bnds[...,i:j,:]                
-            self.area     = self.area[  :,i:j]
+                self.data_bnds = self.data_bnds[..., i:j, :]
+            self.area = self.area[:, i:j]
         if t is not None:
             assert len(t) == 2
-            if not self.temporal: raise il.NotTemporalVariable
-            self = il.ClipTime(self,t[0],t[1])
+            if not self.temporal:
+                raise il.NotTemporalVariable
+            self = il.ClipTime(self, t[0], t[1])
         if d is not None:
             assert len(d) == 2
-            if self.depth_bnds is None: raise ValueError
-            keep = (self.depth_bnds[:,1] >= d[0])*(self.depth_bnds[:,0] <= d[1])
-            ind  = np.where(keep)[0]
-            self.depth_bnds = self.depth_bnds[ind,:]
-            self.depth      = self.depth     [ind  ]
-            self.data       = self.data[...,ind,:,:]
+            if self.depth_bnds is None:
+                raise ValueError
+            keep = (self.depth_bnds[:, 1] >= d[0]) * (self.depth_bnds[:, 0] <= d[1])
+            ind = np.where(keep)[0]
+            self.depth_bnds = self.depth_bnds[ind, :]
+            self.depth = self.depth[ind]
+            self.data = self.data[..., ind, :, :]
 
         return self
```

### Comparing `ILAMB-2.6/src/ILAMB/ccgfilt.py` & `ILAMB-2.7/src/ILAMB/ccgfilt.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 """
 Class for computing the curve fitting/smoothing technique used by Thoning et al 1989.
 
 This technique uses the following step:
 
    1 - Fit a function consisting of a polynomial and harmonics to the data
    2 - Smooth the residuals from the function fit with a low-pass filter using
@@ -21,88 +20,92 @@
 from scipy import optimize
 from scipy import stats
 from scipy import interpolate
 from scipy import fftpack
 import numpy
 import copy
 
-#--------------------------------------------------
+# --------------------------------------------------
 # Define the function we are trying to fit
 # This is a combination of a polynomial and harmonic function
-#--------------------------------------------------
+# --------------------------------------------------
 def fitFunc(params, x, numpoly, numharm):
-    """ Calculate the function at time x with coefficients given in params.
+    """Calculate the function at time x with coefficients given in params.
     This is a combination of a polynomial with numpoly coefficients, and
     a sin/cosine harmonic with numharm coefficients.
     e.g., with numpoly=3 and numharm=2:
 
         y = a + b*x + c*x^2 + d*sin(2*pi*x) + e*cos(2*pi*x) + f*sin(4*pi*x) + g*cos(4*pi*x) ...
 
     where a = params[0], b = params[1], c = params[2], d = params[3] ...
     """
 
     # polynomial part
     # we need to reverse the order of the polynomial coefficients for input into polyval
-    p = numpy.polyval(params[numpoly-1::-1], x)
+    p = numpy.polyval(params[numpoly - 1 :: -1], x)
 
     # get harmonic part of function
     s = harmonics(params, x, numpoly, numharm)
 
-    return p+s
+    return p + s
+
 
-#--------------------------------------------------
+# --------------------------------------------------
 def harmonics(params, x, numpoly, numharm):
-    """ calculate the harmonic part of the function at time x """
+    """calculate the harmonic part of the function at time x"""
 
     # harmonic part
-    pi2 = 2*pi*x
+    pi2 = 2 * pi * x
     if numharm > 0:
         # create an array s of correct size by explicitly evaluating first harmonic
-        s = params[numpoly]*numpy.sin(pi2) + params[numpoly+1]*numpy.cos(pi2)
+        s = params[numpoly] * numpy.sin(pi2) + params[numpoly + 1] * numpy.cos(pi2)
 
         # do additional harmonics (nharm > 1)
         for i in range(1, numharm):
-            ix = 2*i + numpoly    # index into params for harmonic coefficients
-            s += params[ix]*numpy.sin((i+1)*pi2) + params[ix+1]*numpy.cos((i+1)*pi2)
+            ix = 2 * i + numpoly  # index into params for harmonic coefficients
+            s += params[ix] * numpy.sin((i + 1) * pi2) + params[ix + 1] * numpy.cos(
+                (i + 1) * pi2
+            )
 
-        n = numpoly + 2*numharm
+        n = numpoly + 2 * numharm
         if n < len(params):
-            s = (1+params[n]*x) * s        # amplitude gain factor
+            s = (1 + params[n] * x) * s  # amplitude gain factor
 
         return s
     else:
         return 0
 
 
-#--------------------------------------------------
+# --------------------------------------------------
 def errfunc(p, x, y, numpoly, numharm):
-    """ function to calc the difference between input values and function """
+    """function to calc the difference between input values and function"""
     return y - fitFunc(p, x, numpoly, numharm)
 
-#--------------------------------------------------
+
+# --------------------------------------------------
 def partial(n, x, numpoly):
-    """ calculate partial derivative of function with respect to parameter n at time x """
+    """calculate partial derivative of function with respect to parameter n at time x"""
 
     if n < numpoly:
         if n == 0:
             p = 1.0
         else:
             p = pow(x, float(n))
     else:
-        ix = (n-numpoly)/2 + 1
+        ix = (n - numpoly) / 2 + 1
         xx = ix * 2 * pi * x
-        if (n-numpoly) % 2 == 0:
+        if (n - numpoly) % 2 == 0:
             p = sin(xx)
         else:
             p = cos(xx)
 
     return p
 
 
-#--------------------------------------------------
+# --------------------------------------------------
 class ccgFilter(object):
     """
 
     Input Parameters
     ----------
     xp : list
         time values for input data
@@ -239,15 +242,28 @@
 
     getTrendCrossingDates()
       Get the dates when the smoothed curve crosses the trend curve.
       That is, when the detrended smooth seasonal cycle crosses 0.
 
     """
 
-    def __init__(self, xp, yp, shortterm=80, longterm=667, sampleinterval=0, numpolyterms=3, numharmonics=4, timezero=-1, gap=0, use_gain_factor=False, debug=False):
+    def __init__(
+        self,
+        xp,
+        yp,
+        shortterm=80,
+        longterm=667,
+        sampleinterval=0,
+        numpolyterms=3,
+        numharmonics=4,
+        timezero=-1,
+        gap=0,
+        use_gain_factor=False,
+        debug=False,
+    ):
 
         t0 = datetime.datetime.now()
 
         # save input data as numpy arrays
         # make sure data is sorted by x values
         if isinstance(xp, list):
             a = numpy.array(xp)
@@ -257,244 +273,249 @@
         c = numpy.argsort(a)
         self.xp = a[c]
         if isinstance(yp, list):
             b = numpy.array(yp)
         else:
             b = numpy.array(yp.tolist())
         self.yp = b[c]
-    #    self.xp = numpy.array(xp)
-    #    self.yp = numpy.array(yp)
+        #    self.xp = numpy.array(xp)
+        #    self.yp = numpy.array(yp)
         self.np = len(xp)
 
         # Calculate the average time interval between data points.
         # Set the sampleinterval variable if not set on the command line.
         if sampleinterval == 0:
 
             # calculate the average interval between samples that are at least 1 day apart
             sd = 0
             sdiff = 0
             tx = self.xp[0]
             for i in range(1, self.np):
-                if self.xp[i]-tx > 0.002739:
-                    diff = self.xp[i]-tx
+                if self.xp[i] - tx > 0.002739:
+                    diff = self.xp[i] - tx
                     sdiff += diff
                     sd += 1
                 tx = self.xp[i]
 
-            avginterval = sdiff/sd * 365
+            avginterval = sdiff / sd * 365
 
             if avginterval > 1:
                 self.sampleinterval = round(avginterval, 0)
             else:
                 self.sampleinterval = avginterval
             if debug:
                 print("changed sampleinterval to ", self.sampleinterval)
         else:
             self.sampleinterval = sampleinterval
 
-        self.dinterval = self.sampleinterval/365.0 # sample interval in decimal years
+        self.dinterval = self.sampleinterval / 365.0  # sample interval in decimal years
 
         # If the data is actually an average over a relatively large time period,
         # such as annual averages, change the number of harmonics to an appropriate value.
-        nh = int(365.0/(self.sampleinterval*2))
+        nh = int(365.0 / (self.sampleinterval * 2))
         if nh < numharmonics:
             self.numharm = nh
             if debug:
                 print("changed numharmonics to ", numharmonics)
         else:
             self.numharm = numharmonics
 
-
         self.use_gain_factor = use_gain_factor
         self.shortterm = shortterm
         self.longterm = longterm
         self.numpoly = numpolyterms
         if timezero < 0:
             self.timezero = int(xp[0])
             if debug:
                 print("changed timezero to ", self.timezero)
         else:
             self.timezero = timezero
         self.debug = debug
-        self.numpm = self.numpoly + 2*self.numharm
+        self.numpm = self.numpoly + 2 * self.numharm
 
         # apply filter to data
         self._filter_data(gap)
 
         # compute derivatives of polynomial and long term trend
         self._compute_deriv()
 
         # standard deviation of residuals about smooth curve
         r = self.yp - self.getSmoothValue(self.xp)
         self.rsd2 = numpy.std(r, ddof=1)
         self.rmean = numpy.mean(r)
         if self.debug:
             print("mean, rsd about smooth curve is", self.rmean, self.rsd2)
 
-
         t1 = datetime.datetime.now()
         if self.debug:
-            print("Total time elapsed: ", t1-t0)
+            print("Total time elapsed: ", t1 - t0)
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _filter_data(self, gap):
-        """ Perform the curve fitting/filtering """
+        """Perform the curve fitting/filtering"""
 
         if self.debug:
             print("=== Inside filter_data. ===")
-            print("  Number of points = %d, Sample Interval = %f days, %f years" % (self.np, self.sampleinterval, self.dinterval))
+            print(
+                "  Number of points = %d, Sample Interval = %f days, %f years"
+                % (self.np, self.sampleinterval, self.dinterval)
+            )
             print("  Cutoff 1 = %d, Cutoff 2 = %d" % (self.shortterm, self.longterm))
             print("  Numpoly = %d, Numharm = %d" % (self.numpoly, self.numharm))
             print("  Time zero = %f" % self.timezero)
             print("  First point = %e,%e" % (self.xp[0], self.yp[0]))
             print("  Last point = %e,%e" % (self.xp[-1], self.yp[-1]))
 
         # Remove the timezero value from the x data so that coefficients will be relative to the timezero date
         work = self.xp - self.timezero
 
-
         # Fit the function to the data
-        pm = [1.0] * self.numpm        # initial parameter values set to 1
-        if self.use_gain_factor:    # add amplitude gain factor parameter with initial value of 0
+        pm = [1.0] * self.numpm  # initial parameter values set to 1
+        if (
+            self.use_gain_factor
+        ):  # add amplitude gain factor parameter with initial value of 0
             pm.append(0)
             self.numpm += 1
-        self.params, self.covar, info, mesg, ier = optimize.leastsq(errfunc, pm, full_output=1, args=(work, self.yp, self.numpoly, self.numharm))
+        self.params, self.covar, info, mesg, ier = optimize.leastsq(
+            errfunc, pm, full_output=1, args=(work, self.yp, self.numpoly, self.numharm)
+        )
         if self.debug:
             print("  Finished leastsq")
             for i in range(self.numpm):
                 print("    param[%d] = %e" % (i, self.params[i]))
 
-
         #  calculate residuals from fit
         self.resid = self.yp - fitFunc(self.params, work, self.numpoly, self.numharm)
         rmean = numpy.mean(self.resid)
         self.rsd1 = numpy.std(self.resid, ddof=1)
-        self.chisq = numpy.sum(self.resid*self.resid)/(self.np - self.numpm)    # reduced chi square
+        self.chisq = numpy.sum(self.resid * self.resid) / (
+            self.np - self.numpm
+        )  # reduced chi square
         if self.debug:
             print("  Finished residuals")
-            print("    rmean = %e, rsd = %e, chisq = %e" % (rmean, self.rsd1, self.chisq))
+            print(
+                "    rmean = %e, rsd = %e, chisq = %e" % (rmean, self.rsd1, self.chisq)
+            )
 
         # from http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html#scipy.optimize.leastsq
         # "This matrix must be multiplied by the residual variance to get the covariance of the parameter estimates - see curve_fit."
         self.covar = self.covar * self.chisq * self.chisq
 
         # calculate variance of function fit
         self.funcvar = self._varnce()
         self.polyvar = self._varnce(poly=True)
         if self.debug:
             print("  variance is", self._varnce())
             print("  Function variance is", self.funcvar)
 
-
         # fit linear line to ends of residual data
         # subtract this from residuals so ends are ~ near 0
         ca, cb = self._adjustend(work, self.resid, self.longterm)
-        resid = self.resid - (ca + cb*work)
+        resid = self.resid - (ca + cb * work)
         if self.debug:
             print("  Finished adjustend")
             print("    ca = %e, cb = %e" % (ca, cb))
             print("    x[0] = %e, x[%d] = %e" % (work[0], self.np, work[-1]))
             print("    resid[0] = %e, resid[%d] = %e" % (resid[0], self.np, resid[-1]))
 
-
         # Interpolate data at evenly spaced intervals (self.sampleinterval)
         self.xinterp, yinterp = self._lin_interp(work, resid, gap)
         self.ninterp = len(self.xinterp)
 
         if self.debug:
             print("  Interpolated points.")
             print("    Number of interpolated points: %d" % (self.ninterp))
-            print("    xinterp[np-1] = %e, x[0] = %e" % (self.xinterp[-1], self.xinterp[0]))
+            print(
+                "    xinterp[np-1] = %e, x[0] = %e"
+                % (self.xinterp[-1], self.xinterp[0])
+            )
             print("    yinterp[np-1] = %e, y[0] = %e" % (yinterp[-1], yinterp[0]))
 
-
         # do fft on interpolated data
         # we'll zero pad the data to an even power of 2
         # This makes it the same method used in c version.
         n2 = int(pow(2, ceil(log(yinterp.size, 2))))
         zzz = numpy.zeros(n2)
-        nstart = int((n2 - yinterp.size)/2)
+        nstart = int((n2 - yinterp.size) / 2)
         nend = nstart + yinterp.size
         zzz[nstart:nend] = yinterp
 
         fft = fftpack.rfft(zzz)
 
         # do short term filter
         if self.debug:
             print("  Do short term filter, cutoff = ", self.shortterm)
         a = self._freq_filter(fft, self.dinterval, self.shortterm)
         yfilt = fftpack.irfft(a)
         shortTimeSeries = copy.deepcopy(yfilt)
-        self.smooth = yfilt[nstart:nend] + ca + cb*self.xinterp
-
+        self.smooth = yfilt[nstart:nend] + ca + cb * self.xinterp
 
         # do long term filter
         if self.debug:
             print("  Do long term filter, cutoff = ", self.longterm)
         a = self._freq_filter(fft, self.dinterval, self.longterm)
         yfilt = fftpack.irfft(a)
         longTimeSeries = copy.deepcopy(yfilt)
-        self.trend = yfilt[nstart:nend] + ca + cb*self.xinterp
-        #print("fft:")
-        #print(len(fft))
-        #print("shortTimeSeries:")
-        #print(len(shortTimeSeries))
-        #print("longTimeSeries:")
-        #print(len(longTimeSeries))
+        self.trend = yfilt[nstart:nend] + ca + cb * self.xinterp
+        # print("fft:")
+        # print(len(fft))
+        # print("shortTimeSeries:")
+        # print(len(shortTimeSeries))
+        # print("longTimeSeries:")
+        # print(len(longTimeSeries))
 
         # add linear fit and timezero back in to interpolated values
-        self.yinterp = yinterp + ca + cb*self.xinterp
+        self.yinterp = yinterp + ca + cb * self.xinterp
         self.xinterp = self.xinterp + self.timezero
 
-
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _adjustend(self, x, y, cutoff):
-        """ Determine the slope of the data based on just the ends, i.e. 1/4 of the cutoff """
+        """Determine the slope of the data based on just the ends, i.e. 1/4 of the cutoff"""
 
         # check if length of data is too short to adjust
-        if x[-1] - x[0] < cutoff/365.0:
+        if x[-1] - x[0] < cutoff / 365.0:
             return 0, 0
 
         # length of data to use is 1/4 of cutoff length
-        c = cutoff/365.0/4.0
+        c = cutoff / 365.0 / 4.0
 
-        z = numpy.where( (x<=x[0]+c) | (x>=x[-1]-c) )
+        z = numpy.where((x <= x[0] + c) | (x >= x[-1] - c))
 
         slope, intercept, r_value, p_value, std_err = stats.linregress(x[z], y[z])
         return intercept, slope
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _lin_interp(self, x, y, gap):
-        """ Linear interpolate between input data to get equally spaced values
+        """Linear interpolate between input data to get equally spaced values
         at every sample interval.
         """
 
         # calculate the x values for evenly spaced data at the specified sampling interval
-        xi = numpy.arange(x[0], x[-1]+self.dinterval/2, self.dinterval)
-        xi[-1] = x[-1]        # make sure last point is equal to last data point
+        xi = numpy.arange(x[0], x[-1] + self.dinterval / 2, self.dinterval)
+        xi[-1] = x[-1]  # make sure last point is equal to last data point
 
         # if there are multiple y data points at a single x value, then average them
         # to get only 1 y data point for each x
         xx = []
         yy = []
         xt = x[0]
         ys = y[0]
         ns = 1
         for xp, yp in zip(x[1:], y[1:]):
             if xp == xt:
                 ys += yp
                 ns += 1
             else:
-                ya = ys/ns
+                ya = ys / ns
                 xx.append(xt)
                 yy.append(ya)
                 ys = yp
                 ns = 1
             xt = xp
-        ya = ys/ns
+        ya = ys / ns
         xx.append(xt)
         yy.append(ya)
 
         # calculate interpolation values at each x point
         f = interpolate.interp1d(xx, yy)
 
         # if a gap setting was not made, use normal linear interpolation
@@ -502,50 +523,48 @@
         # Otherwise, fill in gaps using the function value (0) instead.
         if gap == 0:
             yi = f(xi)
 
         else:
             n = len(xx)
             ni = len(xi)
-            yi = numpy.zeros( (ni) )
+            yi = numpy.zeros((ni))
             j = 0
             for i in range(ni):
                 while xi[i] >= xx[j]:
                     j += 1
-                    if j >= n-1:
+                    if j >= n - 1:
                         break
 
                 j -= 1
-                if (xx[j+1] - xx[j]) > gap/365.0: #  8*self.dinterval:
+                if (xx[j + 1] - xx[j]) > gap / 365.0:  #  8*self.dinterval:
                     yi[i] = 0
                 else:
                     yi[i] = f(xi[i])
 
-
         return xi, yi
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _freq_filter(self, fft, dinterv, cutoff):
-        """ Apply low-pass filter to fft data.
+        """Apply low-pass filter to fft data.
         Multiply each discrete frequency in fft by a
         low-pass filter function value set by the value 'cutoff'
         input:
             fft - results of fft
             dinterv - sampling interval in years
             cutoff - cutoff value in days
         """
 
         n2 = len(fft)
-        cf = cutoff/365.0    # convert cutoff to years
-        cutoff2 = 1.0/cf    # change to cycles/year
-
-        freq = fftpack.rfftfreq(n2, dinterv)    # get array of frequencies
-        rw = self._vfilt(freq, cutoff2, 6)    # get filter value at frequencies
-        filt = fft*rw                # apply filter values to fft
+        cf = cutoff / 365.0  # convert cutoff to years
+        cutoff2 = 1.0 / cf  # change to cycles/year
 
+        freq = fftpack.rfftfreq(n2, dinterv)  # get array of frequencies
+        rw = self._vfilt(freq, cutoff2, 6)  # get filter value at frequencies
+        filt = fft * rw  # apply filter values to fft
 
         return filt
 
     # this is how to do filtering at each frequency
     #    filt = numpy.zeros( (len(fft)) )
     #    b = 1.0/(n2*dinterv)
     #    for i in range(1, n2-1, 2):
@@ -567,115 +586,111 @@
     #    else:
     #        f = 1.0 / pow(2.0, z)
     #
     #    return f
     #
     #    return filt
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _vfilt(self, freq, sigma, power):
-        """ vectorized version of getting filter value at a frequency
+        """vectorized version of getting filter value at a frequency
         input:
             freq - array of frequencies
             sigma - cutoff value in cycles/year
             power - integer value for f/fc^power
         returns:
             array of filter values at each frequency
 
         the clip statement insures against underflow, although
         numpy seems to handle it internally anyway
         """
 
-        z = numpy.power((freq/sigma), power)
+        z = numpy.power((freq / sigma), power)
         z = numpy.clip(z, 0, 20.0)
         f = 1.0 / numpy.power(2.0, z)
         return f
 
-
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _compute_deriv(self):
-        """ Compute derivative of trend.
+        """Compute derivative of trend.
         This is the derivative of self.trend + derivative of polynomial part of the function
         """
 
         # Connect trend data points with spline to get derivative at each point
         tck = interpolate.splrep(self.xinterp, self.trend, s=0.0)
         self.deriv = interpolate.splev(self.xinterp, tck, der=1)
 
         # compute derivative of polynomial at each interpolated data point
         # we need to reverse order of polynomial coefficients for input into poly1d
-        poly = numpy.poly1d(self.params[self.numpoly-1::-1])
+        poly = numpy.poly1d(self.params[self.numpoly - 1 :: -1])
         pd = numpy.polyder(poly)
         self.deriv += pd(self.xinterp - self.timezero)
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _varnce(self, poly=False):
-        """ calculate variance of mean response, using equations from
+        """calculate variance of mean response, using equations from
         https://en.wikipedia.org/wiki/Mean_and_predicted_response
         """
 
         # use first data point
-    #    x = self.xp[0] - self.timezero
+        #    x = self.xp[0] - self.timezero
         x = numpy.mean(self.xp - self.timezero)
         C = self.covar
 
-
         # calculate partial derivatives of function with respect to the parameters
-        if poly:   # polynomial only
+        if poly:  # polynomial only
             numparam = self.numpoly
-        else:       # entire function, poly + harmonics
+        else:  # entire function, poly + harmonics
             numparam = self.numpm
 
-
         dfdp = []
         for i in range(numparam):
             dfdp.append(partial(i, x, self.numpoly))
 
         df2 = 0.0
         for j in range(numparam):
             for k in range(numparam):
                 df2 += dfdp[j] * dfdp[k] * C[j, k]
 
-
         return df2
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def _filtvar(self, which):
-        """ calculate the filter variance at cutoff f """
+        """calculate the filter variance at cutoff f"""
 
         # First step: Compute weights of filter by filtering a single
         # point in the middle of zero values (impulse response)
 
         if which == "short":
             cutoff = self.shortterm
         else:
             cutoff = self.longterm
 
-        n0 = 4 * int(cutoff/365.0/self.dinterval)
+        n0 = 4 * int(cutoff / 365.0 / self.dinterval)
 
-    #    z = 1
-    #    while pow(2, z) < n0:
-    #        z += 1
-    #    n0 = pow(2, z)
+        #    z = 1
+        #    while pow(2, z) < n0:
+        #        z += 1
+        #    n0 = pow(2, z)
 
-        ytemp = numpy.zeros( (n0) )
-        ytemp[int(n0/2)] = 1.0
+        ytemp = numpy.zeros((n0))
+        ytemp[int(n0 / 2)] = 1.0
 
         # do fft
         fft = fftpack.rfft(ytemp)
 
         # do filter
         if self.debug:
             print("  In filtvar, do filter, cutoff = ", cutoff, "n0 is ", n0)
 
         a = self._freq_filter(fft, self.dinterval, cutoff)
         weights = fftpack.irfft(a)
 
         # Compute sum of squares of weights
-        ssw = numpy.sum(weights*weights)
+        ssw = numpy.sum(weights * weights)
         if self.debug:
             print("ssw =", ssw)
 
         # calculate residuals from smooth/trend curve
         if which == "short":
             f = interpolate.interp1d(self.xinterp, self.smooth, bounds_error=False)
         else:
@@ -684,82 +699,93 @@
         yy = self.resid - yp
         rmean = numpy.mean(yy)
         rsd = numpy.std(yy, ddof=1)
         n = yy.size
 
         # Compute lag 1 auto covariance
         # http://itl.nist.gov/div898/handbook/eda/section3/eda35c.htm
-        sm = numpy.sum( (yy[0:-1]-rmean) * (yy[1:]-rmean) )
-        cor = sm / (n-1) / (rsd*rsd)    # equivalent to sm/numpy.sum(numpy.square(yy-rmean))
+        sm = numpy.sum((yy[0:-1] - rmean) * (yy[1:] - rmean))
+        cor = (
+            sm / (n - 1) / (rsd * rsd)
+        )  # equivalent to sm/numpy.sum(numpy.square(yy-rmean))
 
         if self.debug:
             print("cor is", cor)
 
-
         # Compute auto covariances
         # r(k) = r(1)^k
         sm = 0.0
-        for i in range(n0-1):
-            for j in range(i+1, n0):
-                r = pow(cor, j-i)
-                if r < 1e-5: break # speed things up by ignoring really small values
-                sm += r*weights[i]*weights[j]
-
+        for i in range(n0 - 1):
+            for j in range(i + 1, n0):
+                r = pow(cor, j - i)
+                if r < 1e-5:
+                    break  # speed things up by ignoring really small values
+                sm += r * weights[i] * weights[j]
 
-        var = rsd*rsd*(ssw+2*sm)
+        var = rsd * rsd * (ssw + 2 * sm)
 
         if self.debug:
             print("sm is", sm, "var is ", var)
 
         return var
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def stats(self):
-        """ Generate statistics about the curve fitting. """
+        """Generate statistics about the curve fitting."""
 
         outs = ""
         if self.np == 0:
             outs += "No data points.  No Statistics available."
             return
 
         # calculate variance of each filter
         varf1 = self._filtvar("short")
         varf2 = self._filtvar("long")
-    #        GetCalendarDate(self.timezero, &year, &month, &day, &hour, &minute, &second);
+        #        GetCalendarDate(self.timezero, &year, &month, &day, &hour, &minute, &second);
 
         outs += "*****  Filter Statistics.  *****\n"
 
         outs += "Beginning date:                 %.6f\n" % self.xp[0]
-        outs += "Ending date:                    %.6f\n" % self.xp[self.np-1]
+        outs += "Ending date:                    %.6f\n" % self.xp[self.np - 1]
 
         outs += "Number of data points:          %d\n\n" % self.np
 
         outs += "FUNCTION PARAMETERS\n"
         outs += "Time = 0 on %f\n" % self.timezero  # year, month, day
         outs += "Number of polynomial terms:     %d\n" % self.numpoly
         outs += "Number of harmonic terms:       %d\n" % self.numharm
         outs += "Total Number of parameters:     %d\n" % self.numpm
         outs += "------------------------------------------------------\n"
         outs += "Parameter          Value          Standard Deviation\n"
         outs += " Polynomial\n"
         for i in range(self.numpm):
-            if i == self.numpoly: outs += " Harmonics\n"
-            if i == self.numpoly + 2*self.numharm: outs += " Amplitude Gain Factor\n"
+            if i == self.numpoly:
+                outs += " Harmonics\n"
+            if i == self.numpoly + 2 * self.numharm:
+                outs += " Amplitude Gain Factor\n"
             outs += "%5d %20.6f %20.6f\n" % (i, self.params[i], sqrt(self.covar[i][i]))
 
         outs += "------------------------------------------------------\n"
         outs += "Harmonic   Amplitude  Std. Dev.    Phase (degrees)  Std. Dev.\n"
-        for i in range(1, self.numharm+1):
-            ix = 2*(i-1)+self.numpoly
+        for i in range(1, self.numharm + 1):
+            ix = 2 * (i - 1) + self.numpoly
             a = self.params[ix]
-            b = self.params[ix+1]
-            c = a*a+b*b
-            siga = (a*a*self.covar[ix][ix]+b*b*self.covar[ix+1][ix+1])/c
-            sigtheta = (b*b*self.covar[ix][ix]+a*a*self.covar[ix+1][ix+1])/(c*c)
-            outs += "%5d %11.2f %10.2f %16.2f %12.2f\n" % (i, sqrt(c), sqrt(siga), atan2(b, a)*180/pi, sqrt(sigtheta)*180.0/pi)
+            b = self.params[ix + 1]
+            c = a * a + b * b
+            siga = (a * a * self.covar[ix][ix] + b * b * self.covar[ix + 1][ix + 1]) / c
+            sigtheta = (
+                b * b * self.covar[ix][ix] + a * a * self.covar[ix + 1][ix + 1]
+            ) / (c * c)
+            outs += "%5d %11.2f %10.2f %16.2f %12.2f\n" % (
+                i,
+                sqrt(c),
+                sqrt(siga),
+                atan2(b, a) * 180 / pi,
+                sqrt(sigtheta) * 180.0 / pi,
+            )
 
         outs += "------------------------------------------------------\n"
         outs += "Full covariance matrix:\n"
         for i in range(self.numpm):
             for j in range(self.numpm):
                 outs += "%13.4e" % self.covar[i][j]
             outs += "\n"
@@ -774,39 +800,49 @@
         outs += "Long term self cutoff:        %3d days\n" % self.longterm
         outs += "Sampling interval:            %3g days\n" % self.sampleinterval
         outs += "\n"
         outs += "Function Standard Deviation:          %8.4f\n" % sqrt(self.funcvar)
         outs += "Polynomial Standard Deviation:        %8.4f\n" % sqrt(self.polyvar)
         outs += "Short Term Filter Standard Deviation: %8.4f\n" % sqrt(varf1)
         outs += "Long  Term Filter Standard Deviation: %8.4f\n" % sqrt(varf2)
-        outs += "Smoothed curve Standard Deviation:    %8.4f\n" % sqrt(varf1 + self.funcvar)
-        outs += "Trend curve Standard Deviation:       %8.4f\n" % sqrt(varf2 + self.polyvar)
-        outs += "Detrended Cycle Standard Deviation:   %8.4f\n" % sqrt(varf2 + varf1 + 2*self.funcvar)
-        outs += "Growth Rate Standard Deviation:       %8.4f\n" % sqrt(2*(varf2 + self.polyvar))
+        outs += "Smoothed curve Standard Deviation:    %8.4f\n" % sqrt(
+            varf1 + self.funcvar
+        )
+        outs += "Trend curve Standard Deviation:       %8.4f\n" % sqrt(
+            varf2 + self.polyvar
+        )
+        outs += "Detrended Cycle Standard Deviation:   %8.4f\n" % sqrt(
+            varf2 + varf1 + 2 * self.funcvar
+        )
+        outs += "Growth Rate Standard Deviation:       %8.4f\n" % sqrt(
+            2 * (varf2 + self.polyvar)
+        )
         outs += "\n"
 
         outs += "Residual standard deviation about smooth curve: %f\n" % self.rsd2
 
         return outs
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getAmplitudes(self):
-        """ Get amplitudes of seasonal cycle for each year.
+        """Get amplitudes of seasonal cycle for each year.
         The amplitude is from the detrended data, which is the
         harmonic part of function + smooth curve - trend curve.
         Find max and min values of this for each year, save the values
         and the dates at which they occur.
 
         Returns
         --------
         A list of tuples, each tuple has 6 values (year, total_amplitude, max_date, max_value, min_date, min_value)
         """
 
         # get harmonic part of function at interpolated data points
-        ycycle = harmonics(self.params, self.xinterp-self.timezero, self.numpoly, self.numharm)
+        ycycle = harmonics(
+            self.params, self.xinterp - self.timezero, self.numpoly, self.numharm
+        )
 
         # added short term smoothed data
         ycycle = ycycle + self.smooth - self.trend
 
         # Find max and min values of the seasonal cycle
         tyear = int(self.xinterp[0])
         amps = []
@@ -829,127 +865,126 @@
                 dmax = x
             if y < amin:
                 amin = y
                 dmin = x
 
         return amps
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getFunctionValue(self, x):
-        """ Determine the value of the function at time x.
+        """Determine the value of the function at time x.
         x can be either a single point or a list
         """
 
         if isinstance(x, list):
             xp = numpy.array(x)
         else:
             xp = x
-        return fitFunc(self.params, xp-self.timezero, self.numpoly, self.numharm)
+        return fitFunc(self.params, xp - self.timezero, self.numpoly, self.numharm)
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getSmoothValue(self, x):
-        """ Return the 'smoothed' data at time x
+        """Return the 'smoothed' data at time x
         This is the function plus the smoothed residuals.
         """
 
         ysmooth = self.getFunctionValue(self.xinterp)
         ysmooth = ysmooth + self.smooth
 
         f = interpolate.interp1d(self.xinterp, ysmooth, bounds_error=False)
         yi = f(x)
 
         return yi
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getTrendValue(self, x):
-        """ Return the 'trend' of the data at time x
+        """Return the 'trend' of the data at time x
         This is the polynomial part of the function plus the trend of the residuals.
         i.e., poly plus the long term filter of the residuals
         """
 
         ytrend = self.getPolyValue(self.xinterp)
         ytrend = ytrend + self.trend
 
         f = interpolate.interp1d(self.xinterp, ytrend, bounds_error=False)
         yi = f(x)
 
         return yi
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getPolyValue(self, x):
-        """ Get the values of the polynomial part of the function time x
+        """Get the values of the polynomial part of the function time x
 
         Returns
         -------
         A numpy 1d array with the polynomial values at the given x
         """
 
         xa = numpy.array(x)
 
-        p = numpy.polyval(self.params[self.numpoly-1::-1], xa-self.timezero)
+        p = numpy.polyval(self.params[self.numpoly - 1 :: -1], xa - self.timezero)
 
         return p
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getHarmonicValue(self, x):
-        """ Get the values of the harmonic part of the function time x
+        """Get the values of the harmonic part of the function time x
 
         Returns
         -------
         A numpy 1d array with the harmonic values at the given x
         """
 
         xa = numpy.array(x)
 
         # get harmonic part of function at x
-        y = harmonics(self.params, xa-self.timezero, self.numpoly, self.numharm)
+        y = harmonics(self.params, xa - self.timezero, self.numpoly, self.numharm)
 
         return y
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getGrowthRateValue(self, x):
-        """ Get the values of the derivative of the trend
+        """Get the values of the derivative of the trend
 
         Returns
         -------
         A numpy 1d array with the growth rate values at the given x
         """
 
-        f = interpolate.interp1d(self.xinterp, self.deriv) # , bounds_error=False)
+        f = interpolate.interp1d(self.xinterp, self.deriv)  # , bounds_error=False)
         yi = f(x)
 
         return yi
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getFilterResponse(self, cutoff):
-        """ Get the filter response for a range of frequencies.
+        """Get the filter response for a range of frequencies.
         Input
         -----
             cutoff - cutoff value in days for the filter
 
         Returns
         -------
         Two 1d numpy arrays, length 1000, with the frequency and the corresponding filter response
         for the given cutoff.
 
         Range of frequencies is 0 to 2*cutoff frequency, in 1000 steps
         """
 
-        fmax = (365.0/float(cutoff)) * 2
-        cf = cutoff/365.0
-        cutoff2 = 1.0/cf
+        fmax = (365.0 / float(cutoff)) * 2
+        cf = cutoff / 365.0
+        cutoff2 = 1.0 / cf
         freq = numpy.linspace(0, fmax, 1000)
-        rw = self._vfilt(freq, cutoff2, 6)    # get filter value at frequencies
+        rw = self._vfilt(freq, cutoff2, 6)  # get filter value at frequencies
 
         return freq, rw
 
-
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getMonthlyMeans(self, data=None):
-        """ Get monthly mean values from the smoothed curve
+        """Get monthly mean values from the smoothed curve
         Note: first and last months could be incomplete
 
         Returns
         --------
         A list of tuples, each tuple has 5 values (year, month, value, std. deviation, n)
         """
 
@@ -978,50 +1013,53 @@
 
         mean = numpy.mean(a)
         std = numpy.std(a, ddof=1)
         data.append((dt.year, dt.month, mean, std, len(a)))
 
         return data
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getAnnualMeans(self, data=None):
-        """ Get annual mean values from the smoothed curve
+        """Get annual mean values from the smoothed curve
         Note: first and last years could be incomplete
 
         Returns
         --------
         A list of tuples, each tuple has 4 values (year, value, std. deviation, n)
         """
 
-
         if data is None:
             ysmooth = self.getSmoothValue(self.xinterp)
         else:
             ysmooth = data
 
         firstyear = int(self.xinterp[0])
         lastyear = int(self.xinterp[-1])
 
         data = []
-        for year in range(firstyear, lastyear+1):
-            w = numpy.where( (self.xinterp >= float(year)) & (self.xinterp < float(year+1)))
+        for year in range(firstyear, lastyear + 1):
+            w = numpy.where(
+                (self.xinterp >= float(year)) & (self.xinterp < float(year + 1))
+            )
             mean = numpy.mean(ysmooth[w])
             std = numpy.std(ysmooth[w], ddof=1)
             data.append((year, mean, std, len(w[0])))
 
         return data
 
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def getTrendCrossingDates(self):
-        """ Get the dates when the smoothed curve crosses the trend curve.
+        """Get the dates when the smoothed curve crosses the trend curve.
         That is, when the detrended smooth seasonal cycle crosses 0.
         """
 
         # get harmonic part of function at interpolated data points
-        ycycle = harmonics(self.params, self.xinterp-self.timezero, self.numpoly, self.numharm)
+        ycycle = harmonics(
+            self.params, self.xinterp - self.timezero, self.numpoly, self.numharm
+        )
 
         # added short term smoothed data
         ycycle = ycycle + self.smooth - self.trend
 
         tcup = []
         tcdown = []
         ty = ycycle[0]
@@ -1030,28 +1068,26 @@
             if ty < 0.0 and y >= 0.0:
                 tcup.append(x)
             if ty > 0.0 and y <= 0.0:
                 tcdown.append(x)
 
             ty = y
 
-
         return (tcup, tcdown)
 
-
-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     def calendarDate(self, decyear):
-        """ Convert decimal date to calendar components """
+        """Convert decimal date to calendar components"""
 
         dyr = int(decyear)
         fyr = decyear - dyr
 
         if dyr % 4 == 0:
-            nsec = fyr * (366*86400)
+            nsec = fyr * (366 * 86400)
         else:
-            nsec = fyr * (365*86400)
+            nsec = fyr * (365 * 86400)
 
         nsec = round(nsec, 0)
 
         dt = datetime.datetime(dyr, 1, 1) + datetime.timedelta(seconds=nsec)
 
         return dt
```

### Comparing `ILAMB-2.6/src/ILAMB/data/cmip.cfg` & `ILAMB-2.7/src/ILAMB/data/cmip.cfg`

 * *Files 9% similar despite different names*

```diff
@@ -187,147 +187,167 @@
 [h2: Soil Carbon]
 variable       = "cSoilAbove1m"
 alternate_vars = "cSoil"
 weight         = 5
 mass_weighting = True
 
 [HWSD]
-source     = "DATA/soilc/HWSD/soilc_0.5x0.5.nc"
+source     = "DATA/cSoil/HWSD/soilc_0.5x0.5.nc"
 weight     = 15
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 skip_rmse  = True
 
 [NCSCDV22]
-source     = "DATA/soilc/NCSCDV22/soilc_0.5x0.5.nc"
+source     = "DATA/cSoil/NCSCDV22/soilc_0.5x0.5.nc"
 weight     = 12
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 skip_rmse  = True
 
 [Koven]
 ctype        = "ConfSoilCarbon"
-source       = "DATA/soilc/NCSCDV22/soilc_0.5x0.5.nc"
+source       = "DATA/cSoil/NCSCDV22/soilc_0.5x0.5.nc"
 weight       = 15
-soilc_source = "DATA/soilc/NCSCDV22/soilc_0.5x0.5.nc, DATA/soilc/HWSD/soilc_0.5x0.5.nc"
+soilc_source = "DATA/cSoil/NCSCDV22/soilc_0.5x0.5.nc, DATA/cSoil/HWSD/soilc_0.5x0.5.nc"
 tas_source   = "DATA/tas/CRU4.02/tas.nc"
 pr_source    = "DATA/pr/GPCCv2018/pr.nc"
-npp_source   = "DATA/soilc/Koven/npp_0.5x0.5.nc"
-pet_source   = "DATA/soilc/Koven/pet_0.5x0.5.nc"
-fracpeat_source = "DATA/soilc/Koven/fracpeat_0.5x0.5.nc"
+npp_source   = "DATA/cSoil/Koven/npp_0.5x0.5.nc"
+pet_source   = "DATA/cSoil/Koven/pet_0.5x0.5.nc"
+fracpeat_source = "DATA/cSoil/Koven/fracpeat_0.5x0.5.nc"
 
 ###########################################################################
 
 [h1: Hydrology Cycle]
 bgcolor = "#E6F9FF"
 
 [h2: Evapotranspiration]
 variable       = "et"
 alternate_vars = "evspsbl"
 cmap           = "Blues"
 weight         = 5
 mass_weighting = True
 
 [GLEAMv3.3a]
-source        = "DATA/et/GLEAMv3.3a/et.nc"
+source        = "DATA/evspsbl/GLEAMv3.3a/et.nc"
 weight        = 15
 table_unit    = "mm d-1"
 plot_unit     = "mm d-1"
 relationships = "Precipitation/GPCPv2.3","SurfaceAirTemperature/CRU4.02"
 
 [MODIS]
-source        = "DATA/et/MODIS/et_0.5x0.5.nc"
+source        = "DATA/evspsbl/MODIS/et_0.5x0.5.nc"
 weight        = 15
 table_unit    = "mm d-1"
 plot_unit     = "mm d-1"
 relationships = "Precipitation/GPCPv2.3","SurfaceAirTemperature/CRU4.02"
 
 [MOD16A2]
-source        = "DATA/et/MOD16A2/et.nc"
+source        = "DATA/evspsbl/MOD16A2/et.nc"
 weight        = 15
 table_unit    = "mm d-1"
 plot_unit     = "mm d-1"
 relationships = "Precipitation/GPCPv2.3","SurfaceAirTemperature/CRU4.02"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Evaporative Fraction]
 variable       = "EvapFrac"
 weight         = 5
 mass_weighting = True
 ctype          = "ConfEvapFraction"
 
 [FLUXCOM]
-hfss_source = "DATA/sh/FLUXCOM/sh.nc"
-hfls_source = "DATA/le/FLUXCOM/le.nc"
+hfss_source = "DATA/hfss/FLUXCOM/sh.nc"
+hfls_source = "DATA/hfls/FLUXCOM/le.nc"
+weight      = 9
+skip_rmse   = True
+
+[CLASS]
+hfss_source = "DATA/hfss/CLASS/hfss.nc"
+hfls_source = "DATA/hfls/CLASS/hfls.nc"
 weight      = 9
 skip_rmse   = True
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Latent Heat]
 variable       = "hfls"
 alternate_vars = "le"
 cmap           = "Oranges"
 weight         = 5
 mass_weighting = True
 
 [FLUXNET2015]
-source   = "DATA/le/FLUXNET2015/hfls.nc"
+source   = "DATA/hfls/FLUXNET2015/hfls.nc"
 weight   = 3
 
 [FLUXCOM]
-source   = "DATA/le/FLUXCOM/le.nc"
+source   = "DATA/hfls/FLUXCOM/le.nc"
 land     = True
 weight   = 9
 skip_iav = True
 
 [DOLCE]
-source   = "DATA/et/DOLCE/DOLCE.nc"
+source   = "DATA/evspsbl/DOLCE/DOLCE.nc"
 weight   = 15
 land     = True
 
+[CLASS]
+source         = "DATA/hfls/CLASS/hfls.nc"
+weight         = 15
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Runoff]
 variable       = "runoff"
 alternate_vars = "mrro"
 weight         = 5
 mass_weighting = True
 
 [Dai]
 ctype          = "ConfRunoff"
-source         = "DATA/runoff/Dai/runoff.nc"
+source         = "DATA/mrro/Dai/runoff.nc"
 weight         = 15
 
 [LORA]
-source         = "DATA/runoff/LORA/LORA.nc"
+source         = "DATA/mrro/LORA/LORA.nc"
+table_unit     = "mm d-1"
+plot_unit      = "mm d-1"
+weight         = 15
+
+[CLASS]
+source         = "DATA/mrro/CLASS/mrro.nc"
 table_unit     = "mm d-1"
 plot_unit      = "mm d-1"
 weight         = 15
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Sensible Heat]
 variable       = "hfss"
 alternate_vars = "sh"
 weight         = 2
 mass_weighting = True
 
 [FLUXNET2015]
-source   = "DATA/sh/FLUXNET2015/hfss.nc"
+source   = "DATA/hfss/FLUXNET2015/hfss.nc"
 weight   = 9
 
 [FLUXCOM]
-source   = "DATA/sh/FLUXCOM/sh.nc"
+source   = "DATA/hfss/FLUXCOM/sh.nc"
 weight   = 15
 skip_iav = True
 
+[CLASS]
+source         = "DATA/hfss/CLASS/hfss.nc"
+weight         = 15
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Terrestrial Water Storage Anomaly]
 variable       = "twsa"
 alternate_vars = "tws"
 derived        = "pr-evspsbl-mrro"
 cmap           = "Blues"
@@ -480,14 +500,18 @@
 source   = "DATA/rns/GEWEX.SRB/rns_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
 source   = "DATA/rns/WRMC.BSRN/rns.nc"
 weight   = 12
 
+[CLASS]
+source   = "DATA/rns/CLASS/rns.nc"
+weight   = 15
+
 ###########################################################################
 
 [h1: Forcings]
 bgcolor = "#EDEDED"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
@@ -569,14 +593,22 @@
 source     = "DATA/pr/GPCPv2.3/pr.nc"
 land       = True
 weight     = 20
 table_unit = "mm d-1"
 plot_unit  = "mm d-1"
 space_mean = True
 
+[CLASS]
+source         = "DATA/pr/CLASS/pr.nc"
+land           = True
+weight         = 15
+table_unit     = "mm d-1"
+plot_unit      = "mm d-1"
+space_mean     = True
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Relative Humidity]
 variable       = "rhums"
 alternate_vars = "hurs"
 cmap           = "Blues"
 weight         = 3
```

### Comparing `ILAMB-2.6/src/ILAMB/data/diurnal.cfg` & `ILAMB-2.7/src/ILAMB/data/diurnal.cfg`

 * *Files identical despite different names*

### Comparing `ILAMB-2.6/src/ILAMB/data/ilamb.cfg` & `ILAMB-2.7/src/ILAMB/data/ilamb.cfg`

 * *Files 18% similar despite different names*

```diff
@@ -1,301 +1,433 @@
 # This configure file uses observational data which can be obtained by
 # running the following command after exporting ILAMB_ROOT to the
 # appropriate location.
 #
 #   ilamb-fetch --remote_root https://www.ilamb.org/ILAMB-Data
 #
+# Extra ilamb-run options are defined here with the '#!' symbols:
+#
+#! define_regions = DATA/regions/GlobalLand.nc
+#
 [h1: Ecosystem and Carbon Cycle]
 bgcolor = "#ECFFE6"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Biomass]
 variable       = "biomass"
 alternate_vars = "cVeg"
 weight         = 5
 skip_rmse      = True
 mass_weighting = True
 
-[GEOCARBON]
-source     = "DATA/biomass/GEOCARBON/biomass_0.5x0.5.nc"
+[ESACCI]
+source = "DATA/biomass/ESACCI/biomass.nc"
+scale_factor = 0.714 # x 0.5 (total mass to carbon) / 0.7 (above only to total)
 weight     = 20
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 
-[Tropical]
-source     = "DATA/biomass/Tropical/biomass_0.5x0.5.nc"
-weight     = 20
+[GEOCARBON]
+source       = "DATA/biomass/GEOCARBON/biomass.nc"
+scale_factor = 0.714 # x 0.5 (total mass to carbon) / 0.7 (above only to total)
+weight       = 16
+table_unit   = "Pg"
+plot_unit    = "kg m-2"
+space_mean   = False
+
+[NBCD2000]
+source     = "DATA/biomass/NBCD2000/biomass_0.5x0.5.nc"
+weight     = 8
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 
-[GlobalCarbon]
-source     = "DATA/biomass/GLOBAL.CARBON/biomass_0.5x0.5.nc"
+[Saatchi2011]
+source     = "DATA/biomass/Saatchi2011/biomass_0.5x0.5.nc"
 weight     = 16
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 
-[NBCD2000]
-source     = "DATA/biomass/NBCD2000/biomass_0.5x0.5.nc"
-weight     = 8
+[Thurner]
+source = "DATA/biomass/Thurner/biomass_0.5x0.5.nc"
+weight     = 16
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 
 [USForest]
-source     = "DATA/biomass/US.FOREST/biomass_0.5x0.5.nc"
+source     = "DATA/biomass/USForest/biomass_0.5x0.5.nc"
+scale_factor = 0.714 # x 0.5 (total mass to carbon) / 0.7 (above only to total)
 weight     = 8
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 
+[XuSaatchi2021]
+source     = "DATA/biomass/XuSaatchi2021/XuSaatchi.nc"
+weight     = 20
+table_unit = "Pg"
+plot_unit  = "kg m-2"
+space_mean = False
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Burned Area]
 variable       = "burntArea"
+alternate_vars = "burntFractionAll"
 weight         = 4
 cmap           = "OrRd"
 mass_weighting = True
 
-[GFED4S]          
-source        = "DATA/burntArea/GFED4S/burntArea_0.5x0.5.nc"
+[GFED4.1S]          
+source        = "DATA/burntArea/GFED4.1S/burntArea.nc"
 weight        = 20
-relationships = "Precipitation/GPCP2","SurfaceAirTemperature/CRU"
+relationships = "Precipitation/GPCPv2.3","SurfaceAirTemperature/CRU4.02"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Carbon Dioxide]
 variable      = "co2"
-ctype         = "ConfCO2"
 weight        = 5
 
-[NOAA]
+[NOAA.Emulated]
+ctype         = "ConfCO2"
 source        = "DATA/co2/NOAA.GMD/co2.nc"
 emulated_flux = "nbp"
 sites         = "alt,asc,azr,bhd,bmw,brw,cba,cgo,chr,crz,gmi,hba,ice,key,kum,mhd,mid,pocs35,pocs30,pocs25,pocs20,pocs15,pocs10,pocs05,poc000,pocn05,pocn10,pocn15,pocn20,pocn25,psa,rpb,sey,shm,smo,spo,syo,zep"
 lat_bands     = "-90,-60,-23,0,+23,+60,+90"
+force_emulation = True
+
+[HIPPOAToM]
+ctype      = "ConfGSNF"
+variable   = "gsnf"
+source     = "DATA/co2/HIPPO_AToM/HIPPO_AToM.nc"
+model_flux = "ra+rh-gpp"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Gross Primary Productivity]
 variable       = "gpp"
 cmap           = "Greens"
 weight         = 5
 mass_weighting = True
 
-[Fluxnet]
-source     = "DATA/gpp/FLUXNET/gpp.nc"
+[FLUXNET2015]
+source     = "DATA/gpp/FLUXNET2015/gpp.nc"
 weight     = 9
 table_unit = "g m-2 d-1"
 plot_unit  = "g m-2 d-1"
 
-[GBAF]
-source        = "DATA/gpp/GBAF/gpp_0.5x0.5.nc"
+[FLUXCOM]
+source        = "DATA/gpp/FLUXCOM/gpp.nc"
 weight        = 15
 table_unit    = "Pg yr-1"
 plot_unit     = "g m-2 d-1"
 space_mean    = False
 skip_iav      = True
-relationships = "Evapotranspiration/GLEAM","Precipitation/GPCP2","SurfaceDownwardSWRadiation/CERES","SurfaceNetSWRadiation/CERES","SurfaceAirTemperature/CRU"
+relationships = "Evapotranspiration/GLEAMv3.3a","Precipitation/GPCPv2.3","SurfaceDownwardSWRadiation/CERESed4.1","SurfaceNetSWRadiation/CERESed4.1","SurfaceAirTemperature/CRU4.02"
+
+[WECANN]
+source        = "DATA/gpp/WECANN/gpp.nc"
+weight        = 15
+table_unit    = "Pg yr-1"
+plot_unit     = "g m-2 d-1"
+space_mean    = False
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Leaf Area Index]
 variable       = "lai"
 cmap           = "Greens"
 weight         = 3
 mass_weighting = True
 
 [AVHRR]
 source        = "DATA/lai/AVHRR/lai_0.5x0.5.nc"
 weight        = 15
-relationships = "Precipitation/GPCP2"
+relationships = "Precipitation/GPCPv2.3"
+
+[AVH15C1]
+source        = "DATA/lai/AVH15C1/lai.nc"
+weight        = 15
+relationships = "Precipitation/GPCPv2.3"
 
 [MODIS]
 source        = "DATA/lai/MODIS/lai_0.5x0.5.nc"
 weight        = 15
-relationships = "Precipitation/GPCP2"
+relationships = "Precipitation/GPCPv2.3"
 skip_iav      = True
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Global Net Ecosystem Carbon Balance]
 variable = "nbp"
 weight   = 5
 ctype    = "ConfNBP"
 
 [GCP]      
-source   = "DATA/nbp/GCP/nbp_1959-2012.nc"
+source   = "DATA/nbp/GCP/nbp_1959-2016.nc"
 weight   = 20
 
 [Hoffman]
 source      = "DATA/nbp/HOFFMAN/nbp_1850-2010.nc"
 weight      = 20
 skip_taylor = True
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Net Ecosystem Exchange]
 variable       = "nee"
-derived        = "gpp-ra-rh"
+derived        = "ra+rh-gpp"
 weight         = 5
 mass_weighting = True
 
-[Fluxnet]
-source     = "DATA/nee/FLUXNET/nee.nc"
+[FLUXNET2015]
+source     = "DATA/nee/FLUXNET2015/nee.nc"
 weight     = 9
 table_unit = "g m-2 d-1"
 plot_unit  = "g m-2 d-1"
 
-[GBAF]
-source     = "DATA/nee/GBAF/nee_0.5x0.5.nc"
-weight     = 4
-table_unit = "Pg yr-1"
-plot_unit  = "g m-2 d-1"
-space_mean = False
-skip_iav   = True
-
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Ecosystem Respiration]
 variable       = "reco"
 derived        = "ra+rh"
 weight         = 4
 mass_weighting = True
 
-[Fluxnet]
-source     = "DATA/reco/FLUXNET/reco.nc"
+[FLUXNET2015]
+source     = "DATA/reco/FLUXNET2015/reco.nc"
 weight     = 6
 table_unit = "g m-2 d-1"
 plot_unit  = "g m-2 d-1"
 
-[GBAF]
-source     = "DATA/reco/GBAF/reco_0.5x0.5.nc"
+[FLUXCOM]
+source     = "DATA/reco/FLUXCOM/reco.nc"
 weight     = 4
 table_unit = "Pg yr-1"
 plot_unit  = "g m-2 d-1"
 space_mean = False
 skip_iav   = True
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Soil Carbon]
-variable       = "cSoil"
-alternate_vars = "soilc"
+variable       = "cSoilAbove1m"
+alternate_vars = "cSoil"
 weight         = 5
 mass_weighting = True
 
 [HWSD]
-source     = "DATA/soilc/HWSD/soilc_0.5x0.5.nc"
+source     = "DATA/cSoil/HWSD/soilc_0.5x0.5.nc"
 weight     = 15
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 skip_rmse  = True
 
 [NCSCDV22]
-source     = "DATA/soilc/NCSCDV22/soilc_0.5x0.5.nc"
+source     = "DATA/cSoil/NCSCDV22/soilc_0.5x0.5.nc"
 weight     = 12
 table_unit = "Pg"
 plot_unit  = "kg m-2"
 space_mean = False
 skip_rmse  = True
 
+[Koven]
+ctype        = "ConfSoilCarbon"
+source       = "DATA/cSoil/NCSCDV22/soilc_0.5x0.5.nc"
+weight       = 15
+soilc_source = "DATA/cSoil/NCSCDV22/soilc_0.5x0.5.nc, DATA/cSoil/HWSD/soilc_0.5x0.5.nc"
+tas_source   = "DATA/tas/CRU4.02/tas.nc"
+pr_source    = "DATA/pr/GPCCv2018/pr.nc"
+npp_source   = "DATA/cSoil/Koven/npp_0.5x0.5.nc"
+pet_source   = "DATA/cSoil/Koven/pet_0.5x0.5.nc"
+fracpeat_source = "DATA/cSoil/Koven/fracpeat_0.5x0.5.nc"
+
+#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+[h2: Nitrogen Fixation]
+variable = "fBNF"
+cmap = "Greens"
+weight = 3
+
+[Davies-Barnard]
+source     = "DATA/fBNF/DaviesBarnard/fBNF_0.5x0.5.nc"
+table_unit = "Tg yr-1"
+plot_unit  = "kg ha-1 yr-1"
+space_mean = False
+weight     = 16
+
+#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+[h2: Methane]
+variable       = "FCH4"
+alternate_vars = "ch4"
+cmap           = "Greens"
+mass_weighting = True
+
+[FluxnetANN]
+source     = "DATA/ch4/FluxnetANN/FCH4_F_ANN_monthly_wetland_tier1.nc"
+table_unit = "g m-2 d-1"
+plot_unit  = "g m-2 d-1"
+
 ###########################################################################
 
 [h1: Hydrology Cycle]
 bgcolor = "#E6F9FF"
 
 [h2: Evapotranspiration]
 variable       = "et"
 alternate_vars = "evspsbl"
 cmap           = "Blues"
 weight         = 5
 mass_weighting = True
 
-[GLEAM]
-source        = "DATA/et/GLEAM/et_0.5x0.5.nc"
+[GLEAMv3.3a]
+source        = "DATA/evspsbl/GLEAMv3.3a/et.nc"
 weight        = 15
 table_unit    = "mm d-1"
 plot_unit     = "mm d-1"
-relationships = "Precipitation/GPCP2","SurfaceAirTemperature/CRU"
+relationships = "Precipitation/GPCPv2.3","SurfaceAirTemperature/CRU4.02"
 
 [MODIS]
-source        = "DATA/et/MODIS/et_0.5x0.5.nc"
+source        = "DATA/evspsbl/MODIS/et_0.5x0.5.nc"
+weight        = 15
+table_unit    = "mm d-1"
+plot_unit     = "mm d-1"
+relationships = "Precipitation/GPCPv2.3","SurfaceAirTemperature/CRU4.02"
+
+[MOD16A2]
+source        = "DATA/evspsbl/MOD16A2/et.nc"
 weight        = 15
 table_unit    = "mm d-1"
 plot_unit     = "mm d-1"
-relationships = "Precipitation/GPCP2","SurfaceAirTemperature/CRU"
+relationships = "Precipitation/GPCPv2.3","SurfaceAirTemperature/CRU4.02"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Evaporative Fraction]
 variable       = "EvapFrac"
 weight         = 5
 mass_weighting = True
 ctype          = "ConfEvapFraction"
+cmap           = "BuGn"
 
-[GBAF]
-source     = "DATA/EvapFrac/GBAF/EvapFrac_0.5x0.5.nc"
-weight     = 9
-skip_rmse  = True
-skip_iav   = True
-limit_type = "99per"
+[FLUXCOM]
+hfss_source = "DATA/hfss/FLUXCOM/sh.nc"
+hfls_source = "DATA/hfls/FLUXCOM/le.nc"
+weight      = 9
+skip_rmse   = True
+
+[CLASS]
+hfss_source = "DATA/hfss/CLASS/hfss.nc"
+hfls_source = "DATA/hfls/CLASS/hfls.nc"
+skip_rmse   = True
+weight      = 25
+
+[WECANN]
+hfss_source = "DATA/hfss/WECANN/hfss.nc"
+hfls_source = "DATA/hfls/WECANN/hfls.nc"
+weight      = 9
+skip_rmse   = True
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Latent Heat]
 variable       = "hfls"
 alternate_vars = "le"
 cmap           = "Oranges"
 weight         = 5
 mass_weighting = True
 
-[Fluxnet]
-source   = "DATA/le/FLUXNET/le.nc"
+[FLUXNET2015]
+source   = "DATA/hfls/FLUXNET2015/hfls.nc"
 weight   = 3
 
-[GBAF]
-source   = "DATA/le/GBAF/le_0.5x0.5.nc"
+[FLUXCOM]
+source   = "DATA/hfls/FLUXCOM/le.nc"
 land     = True
 weight   = 9
 skip_iav = True
 
+[DOLCE]
+source   = "DATA/evspsbl/DOLCE/DOLCE.nc"
+weight   = 15
+land     = True
+
+[CLASS]
+source     = "DATA/hfls/CLASS/hfls.nc"
+plot_unit  = "W m-2"
+table_unit = "W m-2"
+weight     = 25
+
+[WECANN]
+source     = "DATA/hfls/WECANN/hfls.nc"
+plot_unit  = "W m-2"
+table_unit = "W m-2"
+weight     = 9
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Runoff]
 variable       = "runoff"
 alternate_vars = "mrro"
 weight         = 5
+mass_weighting = True
 
 [Dai]
 ctype          = "ConfRunoff"
-source         = "DATA/runoff/Dai/runoff.nc"
+source         = "DATA/mrro/Dai/runoff.nc"
 weight         = 15
 
+[LORA]
+source         = "DATA/mrro/LORA/LORA.nc"
+table_unit     = "mm d-1"
+plot_unit      = "mm d-1"
+weight         = 15
+
+[CLASS]
+source         = "DATA/mrro/CLASS/mrro.nc"
+plot_unit      = "mm d-1"
+table_unit     = "mm d-1"
+weight         = 25
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Sensible Heat]
 variable       = "hfss"
 alternate_vars = "sh"
 weight         = 2
 mass_weighting = True
 
-[Fluxnet]
-source   = "DATA/sh/FLUXNET/sh.nc"
+[FLUXNET2015]
+source   = "DATA/hfss/FLUXNET2015/hfss.nc"
 weight   = 9
 
-[GBAF]
-source   = "DATA/sh/GBAF/sh_0.5x0.5.nc"
+[FLUXCOM]
+source   = "DATA/hfss/FLUXCOM/sh.nc"
 weight   = 15
 skip_iav = True
 
+[CLASS]
+source     = "DATA/hfss/CLASS/hfss.nc"
+plot_unit  = "W m-2"
+table_unit = "W m-2"
+weight     = 25
+
+[WECANN]
+source     = "DATA/hfss/WECANN/hfss.nc"
+plot_unit  = "W m-2"
+table_unit = "W m-2"
+weight     = 15
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Terrestrial Water Storage Anomaly]
 variable       = "twsa"
 alternate_vars = "tws"
 derived        = "pr-evspsbl-mrro"
 cmap           = "Blues"
@@ -330,48 +462,59 @@
 ctype    = "ConfPermafrost"
 source   = "DATA/permafrost/NSIDC/NSIDC_0.5x0.5.nc"
 y0       = 1970.
 yf       = 2000.
 Teps     = 273.15
 dmax     = 3.5
 
+#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+[h2: Surface Soil Moisture]
+variable = "mrsos"
+weight   = 3
+cmap     = "Blues"
+
+[WangMao]
+source   = "DATA/mrsos/WangMao/mrsos_olc.nc"
+weight   = 15
+
 ###########################################################################
 
 [h1: Radiation and Energy Cycle]
 bgcolor = "#FFECE6"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Albedo]
 variable = "albedo"
 weight   = 1
 ctype    = "ConfAlbedo"
 
-[CERES]
-source   = "DATA/albedo/CERES/albedo_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/albedo/CERESed4.1/albedo.nc"
 weight   = 20
 
 [GEWEX.SRB]
 source   = "DATA/albedo/GEWEX.SRB/albedo_0.5x0.5.nc"
 weight   = 20
 
-#[MODIS]
-#source   = "DATA/albedo/MODIS/albedo_0.5x0.5.nc"
-#weight   = 20
-
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Upward SW Radiation]
 variable = "rsus"
 weight   = 1
 
-[CERES]
-source   = "DATA/rsus/CERES/rsus_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/rsus/CERESed4.1/rsus.nc"
 weight   = 15
 
+[FLUXNET2015]
+source   = "DATA/rsus/FLUXNET2015/rsus.nc"
+weight   = 12
+
 [GEWEX.SRB]
 source   = "DATA/rsus/GEWEX.SRB/rsus_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
 source   = "DATA/rsus/WRMC.BSRN/rsus.nc"
 weight   = 12
@@ -379,36 +522,44 @@
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Net SW Radiation]
 variable = "rsns"
 derived  = "rsds-rsus"
 weight   = 1
 
-[CERES]
-source   = "DATA/rsns/CERES/rsns_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/rsns/CERESed4.1/rsns.nc"
 weight   = 15
 
+[FLUXNET2015]
+source   = "DATA/rsns/FLUXNET2015/rsns.nc"
+weight   = 12
+
 [GEWEX.SRB]
 source   = "DATA/rsns/GEWEX.SRB/rsns_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
 source   = "DATA/rsns/WRMC.BSRN/rsns.nc"
 weight   = 12
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Upward LW Radiation]
 variable = "rlus"
 weight   = 1
 
-[CERES]
-source   = "DATA/rlus/CERES/rlus_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/rlus/CERESed4.1/rlus.nc"
 weight   = 15 
 
+[FLUXNET2015]
+source   = "DATA/rlus/FLUXNET2015/rlus.nc"
+weight   = 12
+
 [GEWEX.SRB]
 source   = "DATA/rlus/GEWEX.SRB/rlus_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
 source   = "DATA/rlus/WRMC.BSRN/rlus.nc"
 weight   = 12
@@ -416,132 +567,198 @@
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Net LW Radiation]
 variable = "rlns"
 derived  = "rlds-rlus"
 weight   = 1
 
-[CERES]
-source   = "DATA/rlns/CERES/rlns_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/rlns/CERESed4.1/rlns.nc"
 weight   = 15 
 
+[FLUXNET2015]
+source   = "DATA/rlns/FLUXNET2015/rlns.nc"
+weight   = 12
+
 [GEWEX.SRB]
 source   = "DATA/rlns/GEWEX.SRB/rlns_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
 source   = "DATA/rlns/WRMC.BSRN/rlns.nc"
 weight   = 12
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Net Radiation]
 variable = "rns"
 derived  = "rlds-rlus+rsds-rsus"
-weight = 2
+weight   = 2
+cmap     = "RdPu"
 
-[CERES]
-source   = "DATA/rns/CERES/rns_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/rns/CERESed4.1/rns.nc"
 weight   = 15
 
-[Fluxnet]
-source   = "DATA/rns/FLUXNET/rns.nc"
+[FLUXNET2015]
+source   = "DATA/rns/FLUXNET2015/rns.nc"
 weight   = 12
 
 [GEWEX.SRB]
 source   = "DATA/rns/GEWEX.SRB/rns_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
 source   = "DATA/rns/WRMC.BSRN/rns.nc"
 weight   = 12
 
+[CLASS]
+source     = "DATA/rns/CLASS/rns.nc"
+plot_unit  = "W m-2"
+table_unit = "W m-2"
+weight     = 25
+
+#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+[h2: Ground Heat Flux]
+variable       = "hfdsl"
+alternate_vars = "FGR12"
+cmap           = "Oranges"
+
+[CLASS]
+source     = "DATA/hfdsl/CLASS/hfdsl.nc"
+plot_unit  = "W m-2"
+table_unit = "W m-2"
+
 ###########################################################################
 
 [h1: Forcings]
 bgcolor = "#EDEDED"
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Air Temperature]
 variable = "tas"
 weight   = 2
 
-[CRU]
-source   = "DATA/tas/CRU/tas_0.5x0.5.nc"
+[CRU4.02]
+source   = "DATA/tas/CRU4.02/tas.nc"
 weight   = 25
 
-[Fluxnet]
-source   = "DATA/tas/FLUXNET/tas.nc"
+[FLUXNET2015]
+source   = "DATA/tas/FLUXNET2015/tas.nc"
 weight   = 9
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
+[h2: Diurnal Max Temperature]
+variable = "tasmax"
+weight   = 2
+
+[CRU4.02]
+source   = "DATA/tasmax/CRU4.02/tasmax.nc"
+weight   = 25
+
+#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+[h2: Diurnal Min Temperature]
+variable = "tasmin"
+weight   = 2
+
+[CRU4.02]
+source   = "DATA/tasmin/CRU4.02/tasmin.nc"
+weight   = 25
+
+#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+[h2: Diurnal Temperature Range]
+variable = "dtr"
+weight   = 2
+derived  = "tasmax-tasmin"
+
+[CRU4.02]
+source   = "DATA/dtr/CRU4.02/dtr.nc"
+weight   = 25
+
+#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
 [h2: Precipitation]
 variable       = "pr"
 cmap           = "Blues"
 weight         = 2
 mass_weighting = True
 
-[CMAP]
-source     = "DATA/pr/CMAP/pr_0.5x0.5.nc"
+[CMAPv1904]
+source     = "DATA/pr/CMAPv1904/pr.nc"
 land       = True
 weight     = 20
 table_unit = "mm d-1"
 plot_unit  = "mm d-1"
 space_mean = True
 
-[Fluxnet]
-source     = "DATA/pr/FLUXNET/pr.nc"
+[FLUXNET2015]
+source     = "DATA/pr/FLUXNET2015/pr.nc"
 land       = True
 weight     = 9
 table_unit = "mm d-1"
 plot_unit  = "mm d-1"
 
-[GPCC]
-source     = "DATA/pr/GPCC/pr_0.5x0.5.nc"
+[GPCCv2018]
+source     = "DATA/pr/GPCCv2018/pr.nc"
 land       = True
 weight     = 20
 table_unit = "mm d-1"
 plot_unit  = "mm d-1"
 space_mean = True
 
-[GPCP2]
-source     = "DATA/pr/GPCP2/pr_0.5x0.5.nc"
+[GPCPv2.3]
+source     = "DATA/pr/GPCPv2.3/pr.nc"
 land       = True
 weight     = 20
 table_unit = "mm d-1"
 plot_unit  = "mm d-1"
 space_mean = True
 
+[CLASS]
+source         = "DATA/pr/CLASS/pr.nc"
+land           = True
+weight         = 15
+table_unit     = "mm d-1"
+plot_unit      = "mm d-1"
+space_mean     = True
+
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Relative Humidity]
 variable       = "rhums"
 alternate_vars = "hurs"
 cmap           = "Blues"
 weight         = 3
 mass_weighting = True
 
-[ERA]
-source     = "DATA/rhums/ERA/rhums_0.5x0.5.nc"
+[ERA5]
+source     = "DATA/rhums/ERA5/rhums.nc"
+weight     = 10
+
+[CRU4.02]
+source     = "DATA/rhums/CRU4.02/rhums.nc"
 weight     = 10
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Downward SW Radiation]
 variable = "rsds"
 weight   = 2
 
-[CERES]
-source   = "DATA/rsds/CERES/rsds_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/rsds/CERESed4.1/rsds.nc"
 weight   = 15
 
-[Fluxnet]
-source   = "DATA/rsds/FLUXNET/rsds.nc"
+[FLUXNET2015]
+source   = "DATA/rsds/FLUXNET2015/rsds.nc"
 weight   = 12
 
 [GEWEX.SRB]
 source   = "DATA/rsds/GEWEX.SRB/rsds_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
@@ -550,18 +767,23 @@
 
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 [h2: Surface Downward LW Radiation]
 variable = "rlds"
 weight   = 1
 
-[CERES]
-source   = "DATA/rlds/CERES/rlds_0.5x0.5.nc"
+[CERESed4.1]
+source   = "DATA/rlds/CERESed4.1/rlds.nc"
 weight   = 15
 
+[FLUXNET2015]
+source   = "DATA/rlds/FLUXNET2015/rlds.nc"
+weight   = 12
+
 [GEWEX.SRB]
 source   = "DATA/rlds/GEWEX.SRB/rlds_0.5x0.5.nc"
 weight   = 15
 
 [WRMC.BSRN]
 source   = "DATA/rlds/WRMC.BSRN/rlds.nc"
 weight   = 12
+
```

### Comparing `ILAMB-2.6/src/ILAMB/ilamblib.py` & `ILAMB-2.7/src/ILAMB/ilamblib.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,66 +1,104 @@
-from scipy.interpolate import NearestNDInterpolator
-from .constants import mid_months,bnd_months
-from .Regions import Regions
-from netCDF4 import Dataset
-from datetime import datetime
-from cf_units import Unit
+import logging
+import os
+import re
 from copy import deepcopy
-from mpi4py import MPI
-import numpy as np
-import logging,re,os
+
 import cftime as cf
-from pkg_resources import parse_version, get_distribution
+import numpy as np
+from cf_units import Unit
+from mpi4py import MPI
+from netCDF4 import Dataset
+from pkg_resources import get_distribution, parse_version
+from scipy.interpolate import NearestNDInterpolator
+
+from ILAMB.Regions import Regions
 
 logger = logging.getLogger("%i" % MPI.COMM_WORLD.rank)
 
+
 class VarNotInFile(Exception):
-    def __str__(self): return "VarNotInFile"
+    def __str__(self):
+        return "VarNotInFile"
+
 
 class VarNotMonthly(Exception):
-    def __str__(self): return "VarNotMonthly"
+    def __str__(self):
+        return "VarNotMonthly"
+
 
 class VarNotInModel(Exception):
-    def __str__(self): return "VarNotInModel"
+    def __str__(self):
+        return "VarNotInModel"
+
+
+class SiteNotInModel(Exception):
+    def __str__(self):
+        return "SiteNotInModel"
+
 
 class VarsNotComparable(Exception):
-    def __str__(self): return "VarNotComparable"
+    def __str__(self):
+        return "VarNotComparable"
+
 
 class VarNotOnTimeScale(Exception):
-    def __str__(self): return "VarNotOnTimeScale"
+    def __str__(self):
+        return "VarNotOnTimeScale"
+
 
 class UnknownUnit(Exception):
-    def __str__(self): return "UnknownUnit"
+    def __str__(self):
+        return "UnknownUnit"
+
 
 class AreasNotInModel(Exception):
-    def __str__(self): return "AreasNotInModel"
+    def __str__(self):
+        return "AreasNotInModel"
+
 
 class MisplacedData(Exception):
-    def __str__(self): return "MisplacedData"
+    def __str__(self):
+        return "MisplacedData"
+
 
 class NotTemporalVariable(Exception):
-    def __str__(self): return "NotTemporalVariable"
+    def __str__(self):
+        return "NotTemporalVariable"
+
 
 class NotSpatialVariable(Exception):
-    def __str__(self): return "NotSpatialVariable"
+    def __str__(self):
+        return "NotSpatialVariable"
+
 
 class UnitConversionError(Exception):
-    def __str__(self): return "UnitConversionError"
+    def __str__(self):
+        return "UnitConversionError"
+
 
 class AnalysisError(Exception):
-    def __str__(self): return "AnalysisError"
+    def __str__(self):
+        return "AnalysisError"
+
 
 class NotLayeredVariable(Exception):
-    def __str__(self): return "NotLayeredVariable"
+    def __str__(self):
+        return "NotLayeredVariable"
+
 
 class NotDatasiteVariable(Exception):
-    def __str__(self): return "NotDatasiteVariable"
+    def __str__(self):
+        return "NotDatasiteVariable"
+
 
 class MonotonicityError(Exception):
-    def __str__(self): return "MonotonicityError"
+    def __str__(self):
+        return "MonotonicityError"
+
 
 def FixDumbUnits(unit):
     r"""Try to fix the dumb units people insist on using.
 
     Parameters
     ----------
     unit : str
@@ -68,41 +106,41 @@
 
     Returns
     -------
     unit : str
         the fixed unit
     """
     # Various synonyms for 1
-    if unit.lower().strip() in ["unitless",
-                                "n/a",
-                                "none"]: unit = "1"
+    if unit.lower().strip() in ["unitless", "n/a", "none"]:
+        unit = "1"
     # Remove the C which so often is used to mean carbon but actually means coulomb
     tokens = re.findall(r"[\w']+", unit)
-    for i,token in enumerate(tokens):
+    for i, token in enumerate(tokens):
         if token.endswith("C"):
             try:
                 if Unit(token[:-1]).is_convertible(Unit("g")):
-                    unit = unit.replace(token,token[:-1])
-                elif (i > 0) and Unit(tokens[i-1]).is_convertible(Unit("g")):
-                    unit = unit.replace(" C","")
+                    unit = unit.replace(token, token[:-1])
+                elif (i > 0) and Unit(tokens[i - 1]).is_convertible(Unit("g")):
+                    unit = unit.replace(" C", "")
             except:
                 pass
     # ... and Nitrogen
-    for i,token in enumerate(tokens):
+    for i, token in enumerate(tokens):
         if token.endswith("N"):
             try:
                 if Unit(token[:-1]).is_convertible(Unit("g")):
-                    unit = unit.replace(token,token[:-1])
-                elif (i > 0) and Unit(tokens[i-1]).is_convertible(Unit("g")):
-                    unit = unit.replace(" N","")
+                    unit = unit.replace(token, token[:-1])
+                elif (i > 0) and Unit(tokens[i - 1]).is_convertible(Unit("g")):
+                    unit = unit.replace(" N", "")
             except:
                 pass
     return unit
 
-def GenerateDistinctColors(N,saturation=0.67,value=0.67):
+
+def GenerateDistinctColors(N, saturation=0.67, value=0.67):
     r"""Generates a series of distinct colors.
 
     Computes N distinct colors using HSV color space, holding the
     saturation and value levels constant and linearly vary the
     hue. Colors are returned as a RGB tuple.
 
     Parameters
@@ -116,18 +154,20 @@
 
     Returns
     -------
     RGB_tuples : list
        list of N distinct RGB tuples
     """
     from colorsys import hsv_to_rgb
-    HSV_tuples = [(x/float(N), saturation, value) for x in range(N)]
+
+    HSV_tuples = [(x / float(N), saturation, value) for x in range(N)]
     RGB_tuples = list(map(lambda x: hsv_to_rgb(*x), HSV_tuples))
     return RGB_tuples
 
+
 def GuessAlpha(t):
     """Guess at what point in the time interval is the given time.
 
     Although part of the CF standard, many datasets do not have the
     bounds specifed on the time variable. This is complicated by the
     many conventions that groups have regarding at what point in the
     time interval the time is reported. In the absence of given
@@ -141,36 +181,42 @@
     Returns
     -------
     alpha: float
         estimated location in the time interval where the given time
         is defined
 
     """
-    if t.size == 1: return 0.5
-    t0 = cf.num2date(t[0],t.units,calendar=t.calendar)
-    t1 = cf.num2date(t[1],t.units,calendar=t.calendar)
-    dt = (t1-t0).total_seconds()/3600/24
-    if np.allclose(dt,365.,atol=10):
+    if t.size == 1:
+        return 0.5
+    t0 = cf.num2date(t[0], t.units, calendar=t.calendar)
+    t1 = cf.num2date(t[1], t.units, calendar=t.calendar)
+    dt = (t1 - t0).total_seconds() / 3600 / 24
+    if np.allclose(dt, 365.0, atol=10):
         # annual: assumes that the first time entry represents the beginning of a decade
-        y0 = cf.date2num(cf.datetime(10*(t0.year/10),1,1),t.units,calendar=t.calendar)
-        alpha = np.round((t[0]-y0)/dt,1).clip(0,1)
-    elif np.allclose(dt,30,atol=4):
+        y0 = cf.date2num(
+            cf.datetime(10 * (t0.year / 10), 1, 1), t.units, calendar=t.calendar
+        )
+        alpha = np.round((t[0] - y0) / dt, 1).clip(0, 1)
+    elif np.allclose(dt, 30, atol=4):
         # monthly: assumes that the first time entry represents the beginning of a year
-        m0 = cf.date2num(cf.datetime(t0.year,1,1),t.units,calendar=t.calendar)
-        alpha = np.round((t[0]-m0)/dt,1).clip(0,1)
+        m0 = cf.date2num(cf.datetime(t0.year, 1, 1), t.units, calendar=t.calendar)
+        alpha = np.round((t[0] - m0) / dt, 1).clip(0, 1)
     elif dt < 0.9:
         # sub-daily: assumes that the first time entry represents the beginning of a day
-        h0 = cf.date2num(cf.datetime(t0.year,t0.month,t0.day),t.units,calendar=t.calendar)
-        alpha = np.round((t[0]-h0)/dt,1)
+        h0 = cf.date2num(
+            cf.datetime(t0.year, t0.month, t0.day), t.units, calendar=t.calendar
+        )
+        alpha = np.round((t[0] - h0) / dt, 1)
     else:
         msg = "GuessAlpha for dt = %f [d] not implemented yet" % (dt)
         raise ValueError(msg)
     return alpha
 
-def CreateTimeBounds(t,alpha=0.5):
+
+def CreateTimeBounds(t, alpha=0.5):
     """Create time bounds from a time array.
 
     Parameters
     ----------
     t: netCDF4.Variable or numpy.ndarray
         the dataset or array representing the times of the intervals
     alpha: float
@@ -184,183 +230,231 @@
     Notes
     -----
     Using alpha = 0.5 will create bounds in the middle of each time
     point. An alpha = 0 will use the given time as the beginning of
     the interval and alpha = 1 will use the given time as the end of
     the interval.
     """
-    if ((alpha<0)+(alpha>1)):
+    if (alpha < 0) + (alpha > 1):
         msg = "Alpha out of bounds, should be (0 <= %g <= 1)" % alpha
         raise ValueError(msg)
-    if t.size == 1: return np.asarray([[t[0],t[0]]])
+    if t.size == 1:
+        return np.asarray([[t[0], t[0]]])
     dt = np.diff(t)
-    dt = np.hstack([dt[0],dt,dt[-1]])
-    tb = np.asarray([t -     alpha *dt[:-1],
-                     t + (1.-alpha)*dt[+1:]]).T
+    dt = np.hstack([dt[0], dt, dt[-1]])
+    tb = np.asarray([t - alpha * dt[:-1], t + (1.0 - alpha) * dt[+1:]]).T
     return tb
 
-def ConvertCalendar(t,units,calendar):
-    """
-    """
-    return cf.date2num(cf.datetime(t.year,t.month,t.day,t.hour,t.minute,t.second),units,calendar=calendar)
 
-def GetTime(var,t0=None,tf=None,convert_calendar=True,ignore_time_array=True):
-    """
-    """
+def ConvertCalendar(t, units, calendar):
+    """ """
+    return cf.date2num(
+        cf.datetime(t.year, t.month, t.day, t.hour, t.minute, t.second),
+        units,
+        calendar=calendar,
+    )
+
+
+def GetTime(var, t0=None, tf=None, convert_calendar=True, ignore_time_array=True):
+    """ """
     # New method of handling time does not like my biggest/smallest time convention
     if t0 is not None:
-        if np.allclose(t0,-1e20): t0 = None
+        if np.allclose(t0, -1e20):
+            t0 = None
     if tf is not None:
-        if np.allclose(tf,+1e20): tf = None
+        if np.allclose(tf, +1e20):
+            tf = None
 
     # Get parent dataset/group
-    dset  = var.group()
-    vname = "%s:%s" % (dset.filepath(),var.name)
-    CB    = None
+    dset = var.group()
+    vname = "%s:%s" % (dset.filepath(), var.name)
+    CB = None
 
     # What is the time dimension called in the dataset/variable?
     time_name = [name for name in var.dimensions if "time" in name.lower()]
     if len(time_name) == 0:
-        return None,None,None,None,None,None
+        return None, None, None, None, None, None
     elif len(time_name) > 1:
-        msg = "Ambiguous 'time' dimension in the variable %s, one of these [%s]" % (vname,",".join(time_name))
+        msg = "Ambiguous 'time' dimension in the variable %s, one of these [%s]" % (
+            vname,
+            ",".join(time_name),
+        )
         raise IOError(msg)
     time_name = time_name[0]
     t = dset.variables[time_name]
 
     # Check for units on time
     if "units" not in t.ncattrs():
-        msg = "No units given in the time variable in %s:%s" % (dset.filepath(),time_name)
+        msg = "No units given in the time variable in %s:%s" % (
+            dset.filepath(),
+            time_name,
+        )
         raise ValueError(msg)
     if "calendar" not in t.ncattrs():
-        msg = "No calendar given in the time variable in %s:%s" % (dset.filepath(),time_name)
+        msg = "No calendar given in the time variable in %s:%s" % (
+            dset.filepath(),
+            time_name,
+        )
         raise ValueError(msg)
 
     # If no time bounds we create them
     time_bnds_name = t.bounds if "bounds" in t.ncattrs() else None
     if time_bnds_name is not None:
         if time_bnds_name not in dset.variables.keys():
-            msg = "Time bounds specified in %s as %s, but not a variable in the dataset, found these [%s]" % (time_name,time_bnds_name,dset.variables.keys())
+            msg = (
+                "Time bounds specified in %s as %s, but not a variable in the dataset, found these [%s]"
+                % (time_name, time_bnds_name, dset.variables.keys())
+            )
             raise IOError(msg)
         tb = dset.variables[time_bnds_name][...]
     else:
-        tb = CreateTimeBounds(t,alpha=GuessAlpha(t))
+        tb = CreateTimeBounds(t, alpha=GuessAlpha(t))
 
     if "climatology" in t.ncattrs():
         clim_name = t.climatology
         if clim_name not in dset.variables.keys():
-            msg = "Climatology bounds specified in %s as %s, but not a variable in the dataset, found these [%s]" % (time_name,clim_name,dset.variables.keys())
+            msg = (
+                "Climatology bounds specified in %s as %s, but not a variable in the dataset, found these [%s]"
+                % (time_name, clim_name, dset.variables.keys())
+            )
             raise IOError(msg)
         CB = dset.variables[clim_name][...]
-        if not np.allclose(CB.shape,[12,2]):
+        if not np.allclose(CB.shape, [12, 2]):
             msg = "ILAMB only supports annual cycle style climatologies"
             raise IOError(msg)
-        CB = np.round(CB[0,:]/365.+1850.)
+        CB = np.round(CB[0, :] / 365.0 + 1850.0)
 
     # Convert the input beginning/ending time to the current calendar/datum
     if t0 is not None:
-        t0 = cf.num2date(t0,units="days since 1850-1-1 00:00:00",calendar="noleap")
-        t0 = ConvertCalendar(t0,t.units,t.calendar)
-        if (t0 > tb[-1,1]): return None,None,None,None,None,None
+        t0 = cf.num2date(t0, units="days since 1850-1-1 00:00:00", calendar="noleap")
+        t0 = ConvertCalendar(t0, t.units, t.calendar)
+        if t0 > tb[-1, 1]:
+            return None, None, None, None, None, None
     if tf is not None:
-        tf = cf.num2date(tf,units="days since 1850-1-1 00:00:00",calendar="noleap")
-        tf = ConvertCalendar(tf,t.units,t.calendar)
-        if (tf < tb[0,0]): return None,None,None,None,None,None
+        tf = cf.num2date(tf, units="days since 1850-1-1 00:00:00", calendar="noleap")
+        tf = ConvertCalendar(tf, t.units, t.calendar)
+        if tf < tb[0, 0]:
+            return None, None, None, None, None, None
 
     # Subset by the desired initial and final times
-    dt    = np.diff(tb,axis=1)[:,0]
+    dt = np.diff(tb, axis=1)[:, 0]
     begin = 0
-    end   = t.size-1
+    end = t.size - 1
     if t0 is not None:
-        begin = np.where(t0>(tb[:,0]-0.01*dt))[0]
+        begin = np.where(t0 > (tb[:, 0] - 0.01 * dt))[0]
         begin = begin[-1] if begin.size > 0 else 0
     if tf is not None:
-        end   = np.where(tf<(tb[:,1]+0.01*dt))[0]
-        end   = end[0] if end.size > 0 else t.size-1
-    T     = np.asarray(t [begin:(end+1)])
-    TB    = np.asarray(tb[begin:(end+1)])
-    if ignore_time_array: T = TB.mean(axis=1)
+        end = np.where(tf < (tb[:, 1] + 0.01 * dt))[0]
+        end = end[0] if end.size > 0 else t.size - 1
+    T = np.asarray(t[begin : (end + 1)])
+    TB = np.asarray(tb[begin : (end + 1)])
+    if ignore_time_array:
+        T = TB.mean(axis=1)
 
     # Are the time intervals consecutively
-    if not np.allclose(TB[1:,0],TB[:-1,1]):
-        msg = "Time intervals defined in %s:%s are not continuous" % (dset.filepath(),time_bnds_name)
+    if not np.allclose(TB[1:, 0], TB[:-1, 1]):
+        msg = "Time intervals defined in %s:%s are not continuous" % (
+            dset.filepath(),
+            time_bnds_name,
+        )
         raise ValueError(msg)
 
     # Do the times lie in the bounds
-    TF = (T >= TB[:,0])*(T <= TB[:,1])
+    TF = (T >= TB[:, 0]) * (T <= TB[:, 1])
     if not TF.all():
-        index = ["%d" % i for i in np.where(TF==False)[0]]
-        msg = "Times at indices [%s] do not fall inside of the corresponding bounds in %s:%s and %s" % (",".join(index),dset.filepath(),time_name,time_bnds_name)
+        index = ["%d" % i for i in np.where(TF == False)[0]]
+        msg = (
+            "Times at indices [%s] do not fall inside of the corresponding bounds in %s:%s and %s"
+            % (",".join(index), dset.filepath(), time_name, time_bnds_name)
+        )
         raise ValueError(msg)
 
     # Convert time array to ILAMB default calendar / datum
     try:
-        datum_shift = (cf.num2date(0,"days since 1850-1-1 00:00:00",calendar=t.calendar)-
-                       cf.num2date(0,t.units                       ,calendar=t.calendar)).total_seconds()
+        datum_shift = (
+            cf.num2date(0, "days since 1850-1-1 00:00:00", calendar=t.calendar)
+            - cf.num2date(0, t.units, calendar=t.calendar)
+        ).total_seconds()
     except:
-        msg = "Error in computing the datum: t.units = %s, t.calendar = %s" % (t.units,t.calendar)
+        msg = "Error in computing the datum: t.units = %s, t.calendar = %s" % (
+            t.units,
+            t.calendar,
+        )
         raise ValueError(msg)
-        
-    if ((abs(datum_shift) > 60) or (convert_calendar and t.calendar != "noleap")):
-        T  = cf.num2date(T ,units=t.units,calendar=t.calendar)
-        TB = cf.num2date(TB,units=t.units,calendar=t.calendar)
-        for index,x in np.ndenumerate(T):
-            T [index] = ConvertCalendar(x,"days since 1850-1-1 00:00:00","noleap" if convert_calendar else t.calendar)
-        for index,x in np.ndenumerate(TB):
-            TB[index] = ConvertCalendar(x,"days since 1850-1-1 00:00:00","noleap" if convert_calendar else t.calendar)
-    cal = "noleap" if convert_calendar else t.calendar
 
-    return T.astype(float),TB.astype(float),CB,begin,end,cal
+    if (abs(datum_shift) > 60) or (convert_calendar and t.calendar != "noleap"):
+        T = cf.num2date(T, units=t.units, calendar=t.calendar)
+        TB = cf.num2date(TB, units=t.units, calendar=t.calendar)
+        for index, x in np.ndenumerate(T):
+            T[index] = ConvertCalendar(
+                x,
+                "days since 1850-1-1 00:00:00",
+                "noleap" if convert_calendar else t.calendar,
+            )
+        for index, x in np.ndenumerate(TB):
+            TB[index] = ConvertCalendar(
+                x,
+                "days since 1850-1-1 00:00:00",
+                "noleap" if convert_calendar else t.calendar,
+            )
+    cal = "noleap" if convert_calendar else t.calendar
 
+    return T.astype(float), TB.astype(float), CB, begin, end, cal
 
 
-def CellAreas(lat,lon,lat_bnds=None,lon_bnds=None):
+def CellAreas(lat, lon, lat_bnds=None, lon_bnds=None):
     """Given arrays of latitude and longitude, return cell areas in square meters.
 
     Parameters
     ----------
     lat : numpy.ndarray
         a 1D array of latitudes which represent cell centroids
     lon : numpy.ndarray
         a 1D array of longitudes which represent cell centroids
 
     Returns
     -------
     areas : numpy.ndarray
         a 2D array of cell areas in [m2]
     """
-    from .constants import earth_rad
+    from ILAMB.constants import earth_rad
 
-    if (lat_bnds is not None and lon_bnds is not None):
-        return earth_rad**2*np.outer((np.sin(lat_bnds[:,1]*np.pi/180.)-
-                                      np.sin(lat_bnds[:,0]*np.pi/180.)),
-                                     (lon_bnds[:,1]-lon_bnds[:,0])*np.pi/180.)
-
-    x = np.zeros(lon.size+1)
-    x[1:-1] = 0.5*(lon[1:]+lon[:-1])
-    x[ 0]   = lon[ 0]-0.5*(lon[ 1]-lon[ 0])
-    x[-1]   = lon[-1]+0.5*(lon[-1]-lon[-2])
-    if(x.max() > 181): x -= 180
-    x  = x.clip(-180,180)
-    x *= np.pi/180.
-
-    y = np.zeros(lat.size+1)
-    y[1:-1] = 0.5*(lat[1:]+lat[:-1])
-    y[ 0]   = lat[ 0]-0.5*(lat[ 1]-lat[ 0])
-    y[-1]   = lat[-1]+0.5*(lat[-1]-lat[-2])
-    y       = y.clip(-90,90)
-    y *= np.pi/180.
-
-    dx    = earth_rad*(x[1:]-x[:-1])
-    dy    = earth_rad*(np.sin(y[1:])-np.sin(y[:-1]))
-    areas = np.outer(dx,dy).T
+    if lat_bnds is not None and lon_bnds is not None:
+        return earth_rad**2 * np.outer(
+            (
+                np.sin(lat_bnds[:, 1] * np.pi / 180.0)
+                - np.sin(lat_bnds[:, 0] * np.pi / 180.0)
+            ),
+            (lon_bnds[:, 1] - lon_bnds[:, 0]) * np.pi / 180.0,
+        )
+
+    x = np.zeros(lon.size + 1)
+    x[1:-1] = 0.5 * (lon[1:] + lon[:-1])
+    x[0] = lon[0] - 0.5 * (lon[1] - lon[0])
+    x[-1] = lon[-1] + 0.5 * (lon[-1] - lon[-2])
+    if x.max() > 181:
+        x -= 180
+    x = x.clip(-180, 180)
+    x *= np.pi / 180.0
+
+    y = np.zeros(lat.size + 1)
+    y[1:-1] = 0.5 * (lat[1:] + lat[:-1])
+    y[0] = lat[0] - 0.5 * (lat[1] - lat[0])
+    y[-1] = lat[-1] + 0.5 * (lat[-1] - lat[-2])
+    y = y.clip(-90, 90)
+    y *= np.pi / 180.0
+
+    dx = earth_rad * (x[1:] - x[:-1])
+    dy = earth_rad * (np.sin(y[1:]) - np.sin(y[:-1]))
+    areas = np.outer(dx, dy).T
 
     return areas
 
-def GlobalLatLonGrid(res,**keywords):
+
+def GlobalLatLonGrid(res, **keywords):
     r"""Generates a latitude/longitude grid at a desired resolution
 
     Computes 1D arrays of latitude and longitude values which
     correspond to cell interfaces and centroids at a given resolution.
 
     Parameters
     ----------
@@ -376,27 +470,29 @@
     lon_bnd : numpy.ndarray
         a 1D array of longitudes which represent cell interfaces
     lat : numpy.ndarray
         a 1D array of latitudes which represent cell centroids
     lon : numpy.ndarray
         a 1D array of longitudes which represent cell centroids
     """
-    from_zero = keywords.get("from_zero",False)
-    res_lat   = keywords.get("res_lat",res)
-    res_lon   = keywords.get("res_lon",res)
-    nlon    = int(360./res_lon)+1
-    nlat    = int(180./res_lat)+1
-    lon_bnd = np.linspace(-180,180,nlon)
-    if from_zero: lon_bnd += 180
-    lat_bnd = np.linspace(-90,90,nlat)
-    lat     = 0.5*(lat_bnd[1:]+lat_bnd[:-1])
-    lon     = 0.5*(lon_bnd[1:]+lon_bnd[:-1])
-    return lat_bnd,lon_bnd,lat,lon
+    from_zero = keywords.get("from_zero", False)
+    res_lat = keywords.get("res_lat", res)
+    res_lon = keywords.get("res_lon", res)
+    nlon = int(360.0 / res_lon) + 1
+    nlat = int(180.0 / res_lat) + 1
+    lon_bnd = np.linspace(-180, 180, nlon)
+    if from_zero:
+        lon_bnd += 180
+    lat_bnd = np.linspace(-90, 90, nlat)
+    lat = 0.5 * (lat_bnd[1:] + lat_bnd[:-1])
+    lon = 0.5 * (lon_bnd[1:] + lon_bnd[:-1])
+    return lat_bnd, lon_bnd, lat, lon
+
 
-def NearestNeighborInterpolation(lat1,lon1,data1,lat2,lon2):
+def NearestNeighborInterpolation(lat1, lon1, data1, lat2, lon2):
     r"""Interpolates globally grided data at another resolution
 
     Parameters
     ----------
     lat1 : numpy.ndarray
         a 1D array of latitudes of cell centroids corresponding to the
         source data
@@ -413,20 +509,23 @@
         target resolution
 
     Returns
     -------
     data2 : numpy.ndarray
         an array of interpolated data of shape = (lat2.size,lon2.size,...)
     """
-    rows  = np.apply_along_axis(np.argmin,1,np.abs(lat2[:,np.newaxis]-lat1))
-    cols  = np.apply_along_axis(np.argmin,1,np.abs(lon2[:,np.newaxis]-lon1))
-    data2 = data1[np.ix_(rows,cols)]
+    rows = np.apply_along_axis(np.argmin, 1, np.abs(lat2[:, np.newaxis] - lat1))
+    cols = np.apply_along_axis(np.argmin, 1, np.abs(lon2[:, np.newaxis] - lon1))
+    data2 = data1[np.ix_(rows, cols)]
     return data2
 
-def TrueError(lat1_bnd,lon1_bnd,lat1,lon1,data1,lat2_bnd,lon2_bnd,lat2,lon2,data2):
+
+def TrueError(
+    lat1_bnd, lon1_bnd, lat1, lon1, data1, lat2_bnd, lon2_bnd, lat2, lon2, data2
+):
     r"""Computes the pointwise difference between two sets of gridded data
 
     To obtain the pointwise error we populate a list of common cell
     interfaces and then interpolate both input arrays to the composite
     grid resolution using nearest-neighbor interpolation.
 
     Parameters
@@ -447,172 +546,140 @@
     lat_bnd, lon_bnd, lat, lon : numpy.ndarray
         1D arrays corresponding to the latitude/longitudes of the cell
         interfaces/centroids of the resulting error
     error : numpy array
         an array of the pointwise error of shape = (lat.size,lon.size,...)
     """
     # combine limits, sort and remove duplicates
-    lat_bnd = np.hstack((lat1_bnd,lat2_bnd)); lat_bnd.sort(); lat_bnd = np.unique(lat_bnd)
-    lon_bnd = np.hstack((lon1_bnd,lon2_bnd)); lon_bnd.sort(); lon_bnd = np.unique(lon_bnd)
+    lat_bnd = np.hstack((lat1_bnd, lat2_bnd))
+    lat_bnd.sort()
+    lat_bnd = np.unique(lat_bnd)
+    lon_bnd = np.hstack((lon1_bnd, lon2_bnd))
+    lon_bnd.sort()
+    lon_bnd = np.unique(lon_bnd)
 
     # need centroids of new grid for nearest-neighbor interpolation
-    lat = 0.5*(lat_bnd[1:]+lat_bnd[:-1])
-    lon = 0.5*(lon_bnd[1:]+lon_bnd[:-1])
+    lat = 0.5 * (lat_bnd[1:] + lat_bnd[:-1])
+    lon = 0.5 * (lon_bnd[1:] + lon_bnd[:-1])
 
     # interpolate datasets at new grid
-    d1 = NearestNeighborInterpolation(lat1,lon1,data1,lat,lon)
-    d2 = NearestNeighborInterpolation(lat2,lon2,data2,lat,lon)
+    d1 = NearestNeighborInterpolation(lat1, lon1, data1, lat, lon)
+    d2 = NearestNeighborInterpolation(lat2, lon2, data2, lat, lon)
 
     # relative to the first grid/data
-    error = d2-d1
-    return lat_bnd,lon_bnd,lat,lon,error
+    error = d2 - d1
+    return lat_bnd, lon_bnd, lat, lon, error
+
 
-def SympifyWithArgsUnits(expression,args,units):
+def SympifyWithArgsUnits(expression, args, units):
     """Uses symbolic algebra to determine the final unit of an expression.
 
     Parameters
     ----------
     expression : str
         the expression whose units you wish to simplify
     args : dict
         a dictionary of numpy arrays whose keys are the
         variables written in the input expression
     units : dict
         a dictionary of strings representing units whose keys are the
         variables written in the input expression
 
     """
-    from sympy import sympify,postorder_traversal
+    from sympy import postorder_traversal, sympify
 
     expression = sympify(expression)
 
     # try to convert all arguments to same units if possible, it
     # catches most use cases
     keys = list(args.keys())
-    for i,key0 in enumerate(keys):
-        for key in keys[(i+1):]:
+    for i, key0 in enumerate(keys):
+        for key in keys[(i + 1) :]:
             try:
-                Unit(units[key]).convert(args[key],Unit(units[key0]),inplace=True)
+                Unit(units[key]).convert(args[key], Unit(units[key0]), inplace=True)
                 units[key] = units[key0]
             except:
                 pass
 
     for expr in postorder_traversal(expression):
         ekey = str(expr)
         if expr.is_Add:
-
             # if there are scalars in the expression, these will not
             # be in the units dictionary. Add them and give them an
             # implicit unit of 1
             keys = [str(arg) for arg in expr.args]
             for key in keys:
-                if key not in units: units[key] = "1"
+                if key not in units:
+                    units[key] = "1"
 
             # if we are adding, all arguments must have the same unit.
             key0 = keys[0]
             for key in keys:
-                Unit(units[key]).convert(np.ones(1),Unit(units[key0]))
+                Unit(units[key]).convert(np.ones(1), Unit(units[key0]))
                 units[key] = units[key0]
             units[ekey] = "%s" % (units[key0])
 
         elif expr.is_Pow:
-
             # if raising to a power, just create the new unit
             keys = [str(arg) for arg in expr.args]
-            units[ekey] = "(%s)%s" % (units[keys[0]],keys[1])
+            units[ekey] = "(%s)%s" % (units[keys[0]], keys[1])
 
         elif expr.is_Mul:
-
             # just create the new unit
             keys = [str(arg) for arg in expr.args]
-            units[ekey] = " ".join(["(%s)" % units[key] for key in keys if key in units])
-    return sympify(str(expression),locals=args),units[ekey]
+            units[ekey] = " ".join(
+                ["(%s)" % units[key] for key in keys if key in units]
+            )
+    return sympify(str(expression), locals=args), units[ekey]
+
 
-def ComputeIndexingArrays(lat2d,lon2d,lat,lon):
+def ComputeIndexingArrays(lat2d, lon2d, lat, lon):
     """Blah.
 
     Parameters
     ----------
     lat : numpy.ndarray
         A 1D array of latitudes of cell centroids
     lon : numpy.ndarray
         A 1D array of longitudes of cell centroids
 
     """
     # Prepare the interpolator
-    points   = np.asarray([lat2d.flatten(),lon2d.flatten()]).T
-    values   = np.asarray([(np.arange(lat2d.shape[0])[:,np.newaxis]*np.ones  (lat2d.shape[1])).flatten(),
-                           (np.ones  (lat2d.shape[0])[:,np.newaxis]*np.arange(lat2d.shape[1])).flatten()]).T
-    fcn      = NearestNDInterpolator(points,values)
-    LAT,LON  = np.meshgrid(lat,lon,indexing='ij')
-    gmap     = fcn(LAT.flatten(),LON.flatten()).astype(int)
-    return gmap[:,0].reshape(LAT.shape),gmap[:,1].reshape(LAT.shape)
-
-def ExtendAnnualCycle(time,cycle_data,cycle_time):
-    ind = np.abs((time[:,np.newaxis] % 365)-(cycle_time % 365)).argmin(axis=1)
-    assert (ind.max() < 12)*(ind.min() >= 0)
+    points = np.asarray([lat2d.flatten(), lon2d.flatten()]).T
+    values = np.asarray(
+        [
+            (
+                np.arange(lat2d.shape[0])[:, np.newaxis] * np.ones(lat2d.shape[1])
+            ).flatten(),
+            (
+                np.ones(lat2d.shape[0])[:, np.newaxis] * np.arange(lat2d.shape[1])
+            ).flatten(),
+        ]
+    ).T
+    fcn = NearestNDInterpolator(points, values)
+    LAT, LON = np.meshgrid(lat, lon, indexing="ij")
+    gmap = fcn(LAT.flatten(), LON.flatten()).astype(int)
+    return gmap[:, 0].reshape(LAT.shape), gmap[:, 1].reshape(LAT.shape)
+
+
+def ExtendAnnualCycle(time, cycle_data, cycle_time):
+    ind = np.abs((time[:, np.newaxis] % 365) - (cycle_time % 365)).argmin(axis=1)
+    assert (ind.max() < 12) * (ind.min() >= 0)
     return cycle_data[ind]
 
-def _shiftDatum(t,datum,calendar):
-    return date2num(num2date(t[...],datum,calendar=calendar),
-                    "days since 1850-1-1",
-                    calendar=calendar)
-
-def _removeLeapDay(t,v,datum=None,calendar=None,t0=None,tf=None):
-    """
-    Shifts the datum and removes leap day if present.
-
-    Parameters
-    ----------
-    t : netCDF4._netCDF4.Variable
-        the variable representing time in any calendar with any datum
-    v : netCDF4._netCDF4.Variable
-        the variable representing the data
-    datum : str, optional
-        the datum to which we will shift the variable
-    t0 : float, optional
-        the initial time in 'days since 1850-1-1'
-    tf : float, optional
-        the final time in 'days since 1850-1-1'
-    """
-    datum0 = "days since 1850-1-1"
-    if calendar is None:
-        if "calendar" in t.ncattrs(): calendar = t.calendar
-    if datum is None:
-        if "units" in t.ncattrs(): datum = t.units
-
-    # shift their datum to our datum
-    tdata = _shiftDatum(t,datum,calendar)
-    if tdata.ndim > 1: tdata = tdata[:,0]
-
-    # convert the time array to datetime objects in the native calendar
-    T  = num2date(tdata,"days since 1850-1-1",calendar=calendar)
-
-    # find a boolean array which is true where time values are on leap day
-    leap_day = np.asarray([1 if (x.month == 2 and x.day == 29) else 0 for x in T],dtype=bool)
-
-    # then we need to shift the times by the times we will remove
-    dt       = np.hstack([0,np.diff(tdata)]) # assumes that the time is at the beginning of the interval
-    tdata   -= (leap_day*dt).cumsum()        # shift by removed time
-    t_index  = np.where(leap_day==False)[0]  # indices that we keep
-    tdata    = tdata[t_index]                # remove leap day
-
-    # finally we need to shift by the number of leap days since our new datum
-    tdata   -= date2num(T[0],"days since 1850-1-1",calendar) - date2num(T[0],"days since 1850-1-1","noleap")
-
-    # where do we slice the array
-    begin = 0; end = t.size
-    if t0 is not None: begin = max(tdata.searchsorted(t0)-1,begin)
-    if tf is not None: end   = min(tdata.searchsorted(tf)+1,end)
-    tdata = tdata[             begin:end    ]
-    vdata =     v[t_index,...][begin:end,...] # not as memory efficient as it could be
-
-    return tdata,vdata
 
-def FromNetCDF4(filename,variable_name,alternate_vars=[],t0=None,tf=None,group=None,convert_calendar=True):
+def FromNetCDF4(
+    filename,
+    variable_name,
+    alternate_vars=[],
+    t0=None,
+    tf=None,
+    group=None,
+    convert_calendar=True,
+):
     """Extracts data from a netCDF4 datafile for use in a Variable object.
 
     Intended to be used inside of the Variable constructor. Some of
     the return arguments will be None depending on the contents of the
     netCDF4 file.
 
     Parameters
@@ -650,279 +717,429 @@
         A 2D array of the cell areas
     ndata : int
         Number of data sites this data represents
     depth_bnds : numpy.ndarray
         A 1D array of the depth boundaries of each cell
     """
     try:
-        dset = Dataset(filename,mode="r")
+        dset = Dataset(filename, mode="r")
         if parse_version(get_distribution("netCDF4").version) >= parse_version("1.4.1"):
             dset.set_always_mask(False)
         if group is None:
             grp = dset
         else:
             grp = dset.groups[group]
     except RuntimeError:
         raise RuntimeError("Unable to open the file: %s" % filename)
 
-    found     = False
+    found = False
     if variable_name in grp.variables.keys():
         found = True
-        var   = grp.variables[variable_name]
+        var = grp.variables[variable_name]
     else:
-        while alternate_vars.count(None) > 0: alternate_vars.pop(alternate_vars.index(None))
+        while alternate_vars.count(None) > 0:
+            alternate_vars.pop(alternate_vars.index(None))
         for var_name in alternate_vars:
             if var_name in grp.variables.keys():
                 found = True
-                var   = grp.variables[var_name]
+                var = grp.variables[var_name]
     if found == False:
-        alternate_vars.insert(0,variable_name)
-        raise RuntimeError("Unable to find [%s] in the file: %s" % (",".join(alternate_vars),filename))
+        alternate_vars.insert(0, variable_name)
+        raise RuntimeError(
+            "Unable to find [%s] in the file: %s" % (",".join(alternate_vars), filename)
+        )
 
     # Copy attributes into a dictionary
-    attr = { attr: var.getncattr(attr) for attr in var.ncattrs() }
-    
+    attr = {attr: var.getncattr(attr) for attr in var.ncattrs()}
+
     # Check on dimensions
-    time_name  = [name for name in var.dimensions if "time" in name.lower()]
-    lat_name   = [name for name in var.dimensions if "lat"  in name.lower()]
-    lon_name   = [name for name in var.dimensions if "lon"  in name.lower()]
-    data_name  = [name for name in var.dimensions if name.lower() in ["data","lndgrid","gridcell"]]
-    missed     = [name for name in var.dimensions if name not in (time_name +
-                                                                  lat_name  +
-                                                                  lon_name  +
-                                                                  data_name)]
+    time_name = [name for name in var.dimensions if "time" in name.lower()]
+    lat_name = [name for name in var.dimensions if "lat" in name.lower()]
+    lon_name = [name for name in var.dimensions if "lon" in name.lower()]
+    data_name = [
+        name
+        for name in var.dimensions
+        if name.lower() in ["data", "lndgrid", "gridcell"]
+    ]
+    missed = [
+        name
+        for name in var.dimensions
+        if name not in (time_name + lat_name + lon_name + data_name)
+    ]
 
     # Lat/lon might be indexing arrays, find their shape
     shp = None
-    if (len(lat_name) == 0 and len(lon_name) == 0 and len(missed) >= 2 and len(data_name) == 0):
+    if (
+        len(lat_name) == 0
+        and len(lon_name) == 0
+        and len(missed) >= 2
+        and len(data_name) == 0
+    ):
         # remove these dimensions from the missed variables
-        i,j = var.dimensions[-2],var.dimensions[-1]
-        if i in missed: missed.pop(missed.index(i))
-        if j in missed: missed.pop(missed.index(j))
-        i = grp.variables[i]
-        j = grp.variables[j]
-        if (np.issubdtype(i.dtype,np.integer) and
-            np.issubdtype(j.dtype,np.integer)): shp = [len(i),len(j)]
+        i, j = var.dimensions[-2], var.dimensions[-1]
+        if i in missed:
+            missed.pop(missed.index(i))
+        if j in missed:
+            missed.pop(missed.index(j))
+        if i in grp.variables and j in grp.variables:
+            i = grp.variables[i]
+            j = grp.variables[j]
+            if np.issubdtype(i.dtype, np.integer) and np.issubdtype(
+                j.dtype, np.integer
+            ):
+                shp = [len(i), len(j)]
 
     # Lat/lon might just be sizes
-    if (len(lat_name) == 1 and len(lon_name) == 1):
+    if len(lat_name) == 1 and len(lon_name) == 1:
         if not (lat_name[0] in grp.variables and lon_name[0] in grp.variables):
-            shp = [len(grp.dimensions[lat_name[0]]),len(grp.dimensions[lon_name[0]])]
+            shp = [len(grp.dimensions[lat_name[0]]), len(grp.dimensions[lon_name[0]])]
 
     # If these were sizes, then we need to find the correct 2D lat/lon arrays
     if shp is not None:
-
         # We want to remove any false positives we might find. I don't
         # want to consider variables which are 'bounds' or dimensions
         # of others, nor those that don't have the correct shape.
-        bnds = [grp.variables[v].bounds for v in grp.variables if "bounds" in grp.variables[v].ncattrs()]
+        bnds = [
+            grp.variables[v].bounds
+            for v in grp.variables
+            if "bounds" in grp.variables[v].ncattrs()
+        ]
         dims = [v for v in grp.variables if (v in grp.dimensions)]
-        poss = [v for v in grp.variables if (v not in dims and
-                                             v not in bnds and
-                                             np.allclose(shp,grp.variables[v].shape) if len(shp) == len(grp.variables[v].shape) else False)]
+        poss = [
+            v
+            for v in grp.variables
+            if (
+                v not in dims
+                and v not in bnds
+                and np.allclose(shp, grp.variables[v].shape)
+                if len(shp) == len(grp.variables[v].shape)
+                else False
+            )
+        ]
         lat_name = [name for name in poss if "lat" in name.lower()]
         lon_name = [name for name in poss if "lon" in name.lower()]
 
         # If still ambiguous, look inside the variable attributes for
         # the presence of the variable name to give further
         # preference.
         attrs = [str(var.getncattr(attr)) for attr in var.ncattrs()]
-        if len(lat_name) == 0: raise ValueError("Unable to find values for the latitude dimension in %s" % (filename))
+        if len(lat_name) == 0:
+            raise ValueError(
+                "Unable to find values for the latitude dimension in %s" % (filename)
+            )
         if len(lat_name) > 1:
-            tmp_name = [name for name in lat_name if np.any([name in attr for attr in attrs])]
-            if len(tmp_name) > 0: lat_name = tmp_name
-        if len(lon_name) == 0: raise ValueError("Unable to find values for the longitude dimension in %s" % (filename))
+            tmp_name = [
+                name for name in lat_name if np.any([name in attr for attr in attrs])
+            ]
+            if len(tmp_name) > 0:
+                lat_name = tmp_name
+        if len(lon_name) == 0:
+            raise ValueError(
+                "Unable to find values for the longitude dimension in %s" % (filename)
+            )
         if len(lon_name) > 1:
-            tmp_name = [name for name in lon_name if np.any([name in attr for attr in attrs])]
-            if len(tmp_name) > 0: lon_name = tmp_name
+            tmp_name = [
+                name for name in lon_name if np.any([name in attr for attr in attrs])
+            ]
+            if len(tmp_name) > 0:
+                lon_name = tmp_name
+
+    # A final attempt to figure out what dimensions the data is
+    if shp is None and not lat_name and not lon_name and "coordinates" in var.ncattrs():
+        dims = var.getncattr("coordinates").split()
+        lat_name = [dims[0]]
+        lon_name = [dims[1]]
 
     # Lat dimension
     if len(lat_name) == 1:
-        lat_name     = lat_name[0]
-        lat_bnd_name = grp.variables[lat_name].bounds if (lat_name in grp.variables and
-                                                          "bounds" in grp.variables[lat_name].ncattrs()) else None
-        if lat_bnd_name not in grp.variables: lat_bnd_name = None
+        lat_name = lat_name[0]
+        lat_bnd_name = (
+            grp.variables[lat_name].bounds
+            if (
+                lat_name in grp.variables
+                and "bounds" in grp.variables[lat_name].ncattrs()
+            )
+            else None
+        )
+        if lat_bnd_name not in grp.variables:
+            lat_bnd_name = None
     elif len(lat_name) >= 1:
-        raise ValueError("Ambiguous choice of values for the latitude dimension [%s] in %s" % (",".join(lat_name),filename))
+        raise ValueError(
+            "Ambiguous choice of values for the latitude dimension [%s] in %s"
+            % (",".join(lat_name), filename)
+        )
     else:
-        lat_name     = None
+        lat_name = None
         lat_bnd_name = None
 
     # Lon dimension
     if len(lon_name) == 1:
-        lon_name     = lon_name[0]
-        lon_bnd_name = grp.variables[lon_name].bounds if (lon_name in grp.variables and
-                                                          "bounds" in grp.variables[lon_name].ncattrs()) else None
-        if lon_bnd_name not in grp.variables: lon_bnd_name = None
+        lon_name = lon_name[0]
+        lon_bnd_name = (
+            grp.variables[lon_name].bounds
+            if (
+                lon_name in grp.variables
+                and "bounds" in grp.variables[lon_name].ncattrs()
+            )
+            else None
+        )
+        if lon_bnd_name not in grp.variables:
+            lon_bnd_name = None
     elif len(lon_name) >= 1:
-        raise ValueError("Ambiguous choice of values for the longitude dimension [%s] in %s" % (",".join(lon_name),filename))
+        raise ValueError(
+            "Ambiguous choice of values for the longitude dimension [%s] in %s"
+            % (",".join(lon_name), filename)
+        )
     else:
-        lon_name     = None
+        lon_name = None
         lon_bnd_name = None
 
     # Data dimension
     if len(data_name) == 1:
-        data_name     = data_name[0]
+        data_name = data_name[0]
     elif len(data_name) >= 1:
-        raise ValueError("Ambiguous choice of values for the data dimension [%s] in %s" % (",".join(data_name),filename))
+        raise ValueError(
+            "Ambiguous choice of values for the data dimension [%s] in %s"
+            % (",".join(data_name), filename)
+        )
     else:
-        data_name     = None
+        data_name = None
 
     # The layered dimension is whatever is leftover since its name
     # could be many things
     if len(missed) == 1:
         depth_name = missed[0]
-        depth_bnd_name = grp.variables[depth_name].bounds if (depth_name in grp.variables and
-                                                              "bounds" in grp.variables[depth_name].ncattrs()) else None
-        if depth_bnd_name not in grp.variables: depth_bnd_name = None
+        depth_bnd_name = (
+            grp.variables[depth_name].bounds
+            if (
+                depth_name in grp.variables
+                and "bounds" in grp.variables[depth_name].ncattrs()
+            )
+            else None
+        )
+        if depth_bnd_name not in grp.variables:
+            depth_bnd_name = None
     elif len(missed) >= 1:
-        raise ValueError("Ambiguous choice of values for the layered dimension [%s] in %s" % (",".join(missed),filename))
+        raise ValueError(
+            "Ambiguous choice of values for the layered dimension [%s] in %s"
+            % (",".join(missed), filename)
+        )
     else:
-        depth_name     = None
+        depth_name = None
         depth_bnd_name = None
 
     # Based on present values, get dimensions and bounds
-    t       = None; t_bnd     = None
-    lat     = None; lat_bnd   = None
-    lon     = None; lon_bnd   = None
-    depth   = None; depth_bnd = None
-    data    = None;
+    t = None
+    t_bnd = None
+    lat = None
+    lat_bnd = None
+    lon = None
+    lon_bnd = None
+    depth = None
+    depth_bnd = None
+    data = None
     cbounds = None
-    t,t_bnd,cbounds,begin,end,calendar = GetTime(var,t0=t0,tf=tf,convert_calendar=convert_calendar)
+    t, t_bnd, cbounds, begin, end, calendar = GetTime(
+        var, t0=t0, tf=tf, convert_calendar=convert_calendar
+    )
 
     # Are there uncertainties?
     v_bnd = None
-    if "bounds" in var.ncattrs(): v_bnd = var.bounds
+    if "bounds" in var.ncattrs():
+        v_bnd = var.bounds
     v_bnd = grp.variables[v_bnd] if v_bnd in grp.variables.keys() else None
     if begin is None:
         v = var[...]
-        if v_bnd: v_bnd = v_bnd[...]
+        if v_bnd:
+            v_bnd = v_bnd[...]
     else:
-        v = var[begin:(end+1),...]
-        if v_bnd: v_bnd = v_bnd[begin:(end+1),...]
-    if lat_name       is not None: lat       = grp.variables[lat_name]      [...]
-    if lat_bnd_name   is not None: lat_bnd   = grp.variables[lat_bnd_name]  [...]
-    if lon_name       is not None: lon       = grp.variables[lon_name]      [...]
-    if lon_bnd_name   is not None: lon_bnd   = grp.variables[lon_bnd_name]  [...]
-    if depth_name     is not None:
+        v = var[begin : (end + 1), ...]
+        if v_bnd:
+            v_bnd = v_bnd[begin : (end + 1), ...]
+    if lat_name is not None:
+        lat = grp.variables[lat_name][...]
+    if lat_bnd_name is not None:
+        lat_bnd = grp.variables[lat_bnd_name][...]
+    if lon_name is not None:
+        lon = grp.variables[lon_name][...]
+    if lon_bnd_name is not None:
+        lon_bnd = grp.variables[lon_bnd_name][...]
+    if depth_name is not None:
         dunit = None
-        if "units" in grp.variables[depth_name].ncattrs(): dunit = grp.variables[depth_name].units
+        if "units" in grp.variables[depth_name].ncattrs():
+            dunit = grp.variables[depth_name].units
         depth = grp.variables[depth_name][...]
         if depth_bnd_name is not None:
             depth_bnd = grp.variables[depth_bnd_name][...]
         if dunit is not None:
-            if not (Unit(dunit).is_convertible(Unit("m")) or Unit(dunit).is_convertible(Unit("Pa"))):
-                raise ValueError("Units [%s] not compatible with [m,Pa] of the layered dimension [%s] in %s" % (dunit,depth_name,filename))
+            if not (
+                Unit(dunit).is_convertible(Unit("m"))
+                or Unit(dunit).is_convertible(Unit("Pa"))
+            ):
+                raise ValueError(
+                    "Units [%s] not compatible with [m,Pa] of the layered dimension [%s] in %s"
+                    % (dunit, depth_name, filename)
+                )
             if Unit(dunit).is_convertible(Unit("m")):
-                depth = Unit(dunit).convert(depth,Unit("m"),inplace=True)
+                depth = Unit(dunit).convert(depth, Unit("m"), inplace=True)
                 if depth_bnd is not None:
-                    depth_bnd = Unit(dunit).convert(depth_bnd,Unit("m"),inplace=True)
+                    depth_bnd = Unit(dunit).convert(depth_bnd, Unit("m"), inplace=True)
             if Unit(dunit).is_convertible(Unit("Pa")):
                 # FIX: converts the pressure to meters, but assumes it
                 # is coming from the atmosphere. This will be
                 # problematic for ocean quantities.
-                depth = Unit(dunit).convert(depth,Unit("Pa"),inplace=True)
-                Pb = 101325.; Tb = 273.15; R  = 8.3144598; g  = 9.80665; M  = 0.0289644
-                depth = -np.log(depth/Pb)*R*Tb/M/g
+                depth = Unit(dunit).convert(depth, Unit("Pa"), inplace=True)
+                Pb = 101325.0
+                Tb = 273.15
+                R = 8.3144598
+                g = 9.80665
+                M = 0.0289644
+                depth = -np.log(depth / Pb) * R * Tb / M / g
                 if depth_bnd is not None:
-                    depth_bnd = Unit(dunit).convert(depth_bnd,Unit("Pa"),inplace=True)
-                    depth_bnd = -np.log(depth_bnd/Pb)*R*Tb/M/g
+                    depth_bnd = Unit(dunit).convert(depth_bnd, Unit("Pa"), inplace=True)
+                    depth_bnd = -np.log(depth_bnd / Pb) * R * Tb / M / g
     if data_name is not None:
         data = len(grp.dimensions[data_name])
         # if we have data sites, there may be lat/lon/depth data to
         # come along with them although not a dimension of the
         # variable
-        lat_name = []; lon_name = []
+        lat_name = []
+        lon_name = []
         for key in grp.variables.keys():
-            if "lat" in key: lat_name.append(key)
-            if "lon" in key: lon_name.append(key)
-            if "altitude" in key: depth_name = key
-        if len(lat_name) > 1: lat_name = [n for n in lat_name if grp.variables[n].dimensions[0] in var.dimensions]
-        if len(lon_name) > 1: lon_name = [n for n in lon_name if grp.variables[n].dimensions[0] in var.dimensions]        
-        if len(lat_name) > 0: lat   = grp.variables[lat_name[0]][...]
-        if len(lon_name) > 0: lon   = grp.variables[lon_name[0]][...]
-        if depth_name is not None: depth = grp.variables[depth_name][...]
+            if "lat" in key:
+                lat_name.append(key)
+            if "lon" in key:
+                lon_name.append(key)
+            if "altitude" in key:
+                depth_name = key
+        if len(lat_name) > 1:
+            lat_name = [
+                n for n in lat_name if grp.variables[n].dimensions[0] in var.dimensions
+            ]
+        if len(lon_name) > 1:
+            lon_name = [
+                n for n in lon_name if grp.variables[n].dimensions[0] in var.dimensions
+            ]
+        if len(lat_name) > 0:
+            lat = grp.variables[lat_name[0]][...]
+        if len(lon_name) > 0:
+            lon = grp.variables[lon_name[0]][...]
+        if depth_name is not None:
+            depth = grp.variables[depth_name][...]
         if lat is not None:
-            if lat.size != data: lat = None
+            if lat.size != data:
+                lat = None
         if lon is not None:
-            if lon.size != data: lon = None
+            if lon.size != data:
+                lon = None
         if depth is not None:
-            if depth.size != data: depth = None
+            if depth.size != data:
+                depth = None
 
     # If lat and lon are 2D, then we will need to interpolate things
     if lat is not None and lon is not None:
         if lat.ndim == 2 and lon.ndim == 2:
             assert lat.shape == lon.shape
 
             # Create the grid
-            res          = 1.0
-            lat_bnds     = np.arange(round(lat.min(),0),
-                                     round(lat.max(),0)+res/2.,res)
-            lon_bnds     = np.arange(round(lon.min(),0),
-                                     round(lon.max(),0)+res/2.,res)
-            lats         = 0.5*(lat_bnds[:-1]+lat_bnds[1:])
-            lons         = 0.5*(lon_bnds[:-1]+lon_bnds[1:])
-            ilat,ilon    = ComputeIndexingArrays(lat,lon,lats,lons)
-            r            = np.sqrt( (lat[ilat,ilon]-lats[:,np.newaxis])**2 +
-                                    (lon[ilat,ilon]-lons[np.newaxis,:])**2 )
-            v            = v[...,ilat,ilon]
-            v            = np.ma.masked_array(v,mask=v.mask+(r>2*res))
-            lat          = lats
-            lon          = lons
-            lat_bnd      = np.zeros((lat.size,2))
-            lat_bnd[:,0] = lat_bnds[:-1]
-            lat_bnd[:,1] = lat_bnds[+1:]
-            lon_bnd      = lon_bnds
-            lon_bnd      = np.zeros((lon.size,2))
-            lon_bnd[:,0] = lon_bnds[:-1]
-            lon_bnd[:,1] = lon_bnds[+1:]
+            res = 1.0
+            lat_bnds = np.arange(
+                round(lat.min(), 0), round(lat.max(), 0) + res / 2.0, res
+            )
+            lon_bnds = np.arange(
+                round(lon.min(), 0), round(lon.max(), 0) + res / 2.0, res
+            )
+            lats = 0.5 * (lat_bnds[:-1] + lat_bnds[1:])
+            lons = 0.5 * (lon_bnds[:-1] + lon_bnds[1:])
+            ilat, ilon = ComputeIndexingArrays(lat, lon, lats, lons)
+            r = np.sqrt(
+                (lat[ilat, ilon] - lats[:, np.newaxis]) ** 2
+                + (lon[ilat, ilon] - lons[np.newaxis, :]) ** 2
+            )
+            v = v[..., ilat, ilon]
+            v = np.ma.masked_array(v, mask=v.mask + (r > 2 * res))
+            lat = lats
+            lon = lons
+            lat_bnd = np.zeros((lat.size, 2))
+            lat_bnd[:, 0] = lat_bnds[:-1]
+            lat_bnd[:, 1] = lat_bnds[+1:]
+            lon_bnd = lon_bnds
+            lon_bnd = np.zeros((lon.size, 2))
+            lon_bnd[:, 0] = lon_bnds[:-1]
+            lon_bnd[:, 1] = lon_bnds[+1:]
 
     # handle incorrect or absent masking of arrays
     if type(v) != type(np.ma.empty(1)):
-        mask = np.zeros(v.shape,dtype=int)
-        if "_FillValue"    in var.ncattrs(): mask += (np.abs(v-var._FillValue   )<1e-12)
-        if "missing_value" in var.ncattrs(): mask += (np.abs(v-var.missing_value)<1e-12)
-        v = np.ma.masked_array(v,mask=mask,copy=False)
+        mask = np.zeros(v.shape, dtype=int)
+        if "_FillValue" in var.ncattrs():
+            mask += np.abs(v - var._FillValue) < 1e-12
+        if "missing_value" in var.ncattrs():
+            mask += np.abs(v - var.missing_value) < 1e-12
+        v = np.ma.masked_array(v, mask=mask, copy=False)
 
     if "units" in var.ncattrs():
         units = FixDumbUnits(var.units)
     else:
         units = "1"
     dset.close()
 
-    return v,v_bnd,units,variable_name,t,t_bnd,lat,lat_bnd,lon,lon_bnd,depth,depth_bnd,cbounds,data,calendar,attr
+    return (
+        v,
+        v_bnd,
+        units,
+        variable_name,
+        t,
+        t_bnd,
+        lat,
+        lat_bnd,
+        lon,
+        lon_bnd,
+        depth,
+        depth_bnd,
+        cbounds,
+        data,
+        calendar,
+        attr,
+    )
+
 
-def Score(var,normalizer):
+def Score(var, normalizer):
     """Remaps a normalized variable to the interval [0,1].
 
     Parameters
     ----------
     var : ILAMB.Variable.Variable
         The variable to normalize, usually represents an error of some sort
     normalizer : ILAMB.Variable.Variable
         The variable by which we normalize
     """
-    from .Variable import Variable
-    name = var.name.replace("bias","bias_score")
-    name =     name.replace("diff","diff_score")
-    name =     name.replace("rmse","rmse_score")
-    name =     name.replace("iav" ,"iav_score")
-    np.seterr(over='ignore',under='ignore')
-    data = np.exp(-np.abs(var.data/normalizer.data))
-    data[data<1e-16] = 0.
-    np.seterr(over='raise',under='raise')
-    return Variable(name  = name,
-                    data  = data,
-                    unit  = "1",
-                    ndata = var.ndata,
-                    lat   = var.lat, lat_bnds = var.lat_bnds,
-                    lon   = var.lon, lon_bnds = var.lon_bnds,
-                    area  = var.area)
+    from ILAMB.Variable import Variable
 
-def ComposeSpatialGrids(var1,var2):
+    name = var.name.replace("bias", "bias_score")
+    name = name.replace("diff", "diff_score")
+    name = name.replace("rmse", "rmse_score")
+    name = name.replace("iav", "iav_score")
+    np.seterr(over="ignore", under="ignore")
+    data = np.exp(-np.abs(var.data / normalizer.data))
+    data[data < 1e-16] = 0.0
+    np.seterr(over="raise", under="raise")
+    return Variable(
+        name=name,
+        data=data,
+        unit="1",
+        ndata=var.ndata,
+        lat=var.lat,
+        lat_bnds=var.lat_bnds,
+        lon=var.lon,
+        lon_bnds=var.lon_bnds,
+        area=var.area,
+    )
+
+
+def ComposeSpatialGrids(var1, var2):
     """Creates a grid which conforms the boundaries of both variables.
 
     This routine takes the union of the latitude and longitude
     cell boundaries of both grids and returns a new set of
     latitudes and longitudes which represent cell centers of the
     new grid.
 
@@ -934,59 +1151,75 @@
     Returns
     -------
     lat : numpy.ndarray
         a 1D array of latitudes of cell centroids
     lon : numpy.ndarray
         a 1D array of longitudes of cell centroids
     """
-    if not var1.spatial: il.NotSpatialVariable()
-    if not var2.spatial: il.NotSpatialVariable()
+    if not var1.spatial:
+        NotSpatialVariable()
+    if not var2.spatial:
+        NotSpatialVariable()
+
     def _make_bnds(x):
-        bnds       = np.zeros(x.size+1)
-        bnds[1:-1] = 0.5*(x[1:]+x[:-1])
-        bnds[ 0]   = max(x[ 0]-0.5*(x[ 1]-x[ 0]),-180)
-        bnds[-1]   = min(x[-1]+0.5*(x[-1]-x[-2]),+180)
+        bnds = np.zeros(x.size + 1)
+        bnds[1:-1] = 0.5 * (x[1:] + x[:-1])
+        bnds[0] = max(x[0] - 0.5 * (x[1] - x[0]), -180)
+        bnds[-1] = min(x[-1] + 0.5 * (x[-1] - x[-2]), +180)
         return bnds
+
     lat1_bnd = _make_bnds(var1.lat)
     lon1_bnd = _make_bnds(var1.lon)
     lat2_bnd = _make_bnds(var2.lat)
     lon2_bnd = _make_bnds(var2.lon)
-    lat_bnd  = np.hstack((lat1_bnd,lat2_bnd)); lat_bnd.sort(); lat_bnd = np.unique(lat_bnd)
-    lon_bnd  = np.hstack((lon1_bnd,lon2_bnd)); lon_bnd.sort(); lon_bnd = np.unique(lon_bnd)
-    lat      = 0.5*(lat_bnd[1:]+lat_bnd[:-1])
-    lon      = 0.5*(lon_bnd[1:]+lon_bnd[:-1])
-    return lat,lon
+    lat_bnd = np.hstack((lat1_bnd, lat2_bnd))
+    lat_bnd.sort()
+    lat_bnd = np.unique(lat_bnd)
+    lon_bnd = np.hstack((lon1_bnd, lon2_bnd))
+    lon_bnd.sort()
+    lon_bnd = np.unique(lon_bnd)
+    lat = 0.5 * (lat_bnd[1:] + lat_bnd[:-1])
+    lon = 0.5 * (lon_bnd[1:] + lon_bnd[:-1])
+    return lat, lon
+
 
 def ScoreSeasonalCycle(phase_shift):
     """Computes the seasonal cycle score from the phase shift.
 
     Possible remove this function as we do not compute other score
     components via a ilamblib function.
     """
-    from .Variable import Variable
-    return Variable(data  = (1+np.cos(np.abs(phase_shift.data)/365*2*np.pi))*0.5,
-                    unit  = "1",
-                    name  = phase_shift.name.replace("phase_shift","phase_shift_score"),
-                    ndata = phase_shift.ndata,
-                    lat   = phase_shift.lat, lat_bnds = phase_shift.lat_bnds,
-                    lon   = phase_shift.lon, lon_bnds = phase_shift.lon_bnds,
-                    area  = phase_shift.area)
-
-def _composeGrids(v1,v2):
-    lat_bnds = np.unique(np.hstack([v1.lat_bnds.flatten(),v2.lat_bnds.flatten()]))
-    lon_bnds = np.unique(np.hstack([v1.lon_bnds.flatten(),v2.lon_bnds.flatten()]))
-    lat_bnds = lat_bnds[(lat_bnds>=- 90)*(lat_bnds<=+ 90)]
-    lon_bnds = lon_bnds[(lon_bnds>=-180)*(lon_bnds<=+180)]
-    lat_bnds = np.vstack([lat_bnds[:-1],lat_bnds[+1:]]).T
-    lon_bnds = np.vstack([lon_bnds[:-1],lon_bnds[+1:]]).T
-    lat      = lat_bnds.mean(axis=1)
-    lon      = lon_bnds.mean(axis=1)
-    return lat,lon,lat_bnds,lon_bnds
+    from ILAMB.Variable import Variable
+
+    return Variable(
+        data=(1 + np.cos(np.abs(phase_shift.data) / 365 * 2 * np.pi)) * 0.5,
+        unit="1",
+        name=phase_shift.name.replace("phase_shift", "phase_shift_score"),
+        ndata=phase_shift.ndata,
+        lat=phase_shift.lat,
+        lat_bnds=phase_shift.lat_bnds,
+        lon=phase_shift.lon,
+        lon_bnds=phase_shift.lon_bnds,
+        area=phase_shift.area,
+    )
+
+
+def _composeGrids(v1, v2):
+    lat_bnds = np.unique(np.hstack([v1.lat_bnds.flatten(), v2.lat_bnds.flatten()]))
+    lon_bnds = np.unique(np.hstack([v1.lon_bnds.flatten(), v2.lon_bnds.flatten()]))
+    lat_bnds = lat_bnds[(lat_bnds >= -90) * (lat_bnds <= +90)]
+    lon_bnds = lon_bnds[(lon_bnds >= -180) * (lon_bnds <= +180)]
+    lat_bnds = np.vstack([lat_bnds[:-1], lat_bnds[+1:]]).T
+    lon_bnds = np.vstack([lon_bnds[:-1], lon_bnds[+1:]]).T
+    lat = lat_bnds.mean(axis=1)
+    lon = lon_bnds.mean(axis=1)
+    return lat, lon, lat_bnds, lon_bnds
+
 
-def AnalysisMeanStateSites(ref,com,**keywords):
+def AnalysisMeanStateSites(ref, com, **keywords):
     """Perform a mean state analysis.
 
     This mean state analysis examines the model mean state in space
     and time. We compute the mean variable value over the time period
     at each spatial cell or data site as appropriate, as well as the
     bias and RMSE relative to the observational variable. We will
     output maps of the period mean values and bias. For each spatial
@@ -1017,251 +1250,429 @@
     table_unit : str, optional
         the unit to use when displaying output in tables on the HTML page
     plots_unit : str, optional
         the unit to use when displaying output on plots on the HTML page
 
     """
 
-    from .Variable import Variable
-    regions           = keywords.get("regions"          ,["global"])
-    dataset           = keywords.get("dataset"          ,None)
-    benchmark_dataset = keywords.get("benchmark_dataset",None)
-    space_mean        = keywords.get("space_mean"       ,True)
-    table_unit        = keywords.get("table_unit"       ,None)
-    plot_unit         = keywords.get("plot_unit"        ,None)
-    mass_weighting    = keywords.get("mass_weighting"   ,False)
-    skip_rmse         = keywords.get("skip_rmse"        ,False)
-    skip_iav          = keywords.get("skip_iav"         ,True )
-    skip_cycle        = keywords.get("skip_cycle"       ,False)
-    ILAMBregions      = Regions()
-    normalizer        = None
+    from ILAMB.Variable import Variable
+
+    regions = keywords.get("regions", ["global"])
+    dataset = keywords.get("dataset", None)
+    benchmark_dataset = keywords.get("benchmark_dataset", None)
+    space_mean = keywords.get("space_mean", True)
+    table_unit = keywords.get("table_unit", None)
+    plot_unit = keywords.get("plot_unit", None)
+    mass_weighting = keywords.get("mass_weighting", False)
+    skip_rmse = keywords.get("skip_rmse", False)
+    skip_iav = keywords.get("skip_iav", True)
+    skip_cycle = keywords.get("skip_cycle", False)
+    df_errs = keywords.get("df_errs", None)
+    ILAMBregions = Regions()
+    normalizer = None
+    name = ref.name
 
     # Convert str types to booleans
     if type(skip_rmse) == type(""):
-        skip_rmse = (skip_rmse.lower() == "true")
-    if type(skip_iav ) == type(""):
-        skip_iav  = (skip_iav .lower() == "true")
+        skip_rmse = skip_rmse.lower() == "true"
+    if type(skip_iav) == type(""):
+        skip_iav = skip_iav.lower() == "true"
     if type(skip_cycle) == type(""):
-        skip_cycle = (skip_cycle.lower() == "true")
-        
+        skip_cycle = skip_cycle.lower() == "true"
+
     # Only study the annual cycle if it makes sense
-    if    not ref.monthly: skip_cycle = True
-    if ref.time.size < 12: skip_cycle = True
-    if skip_rmse         : skip_iav   = True
+    if not ref.monthly:
+        skip_cycle = True
+    if ref.time.size < 12:
+        skip_cycle = True
+    if skip_rmse:
+        skip_iav = True
 
     # We find the mean values over the time period on the original
     # grid/datasites of each dataset
     ref_timeint = ref.integrateInTime(mean=True)
     com_timeint = com.integrateInTime(mean=True)
-    REF         = ref
-    COM         = com
+    REF = ref
+    COM = com
     REF_timeint = ref_timeint
     COM_timeint = com_timeint
-    if mass_weighting: normalizer = REF_timeint.data
+    if mass_weighting:
+        normalizer = REF_timeint.data
 
     # Compute the bias, RMSE, and RMS maps using the interpolated
     # quantities
     bias = REF_timeint.bias(COM_timeint)
-    cREF = Variable(name = "centralized %s" % REF.name, unit = REF.unit,
-                    data = np.ma.masked_array(REF.data-REF_timeint.data[np.newaxis,...],mask=REF.data.mask),
-                    time = REF.time, time_bnds = REF.time_bnds,
-                    lat  = REF.lat , lat_bnds  = REF.lat_bnds,
-                    lon  = REF.lon , lon_bnds  = REF.lon_bnds,
-                    area = REF.area, ndata     = REF.ndata)
-    crms = cREF.rms ()
-    bias_score_map = Score(bias,crms)
+    cREF = Variable(
+        name="centralized %s" % REF.name,
+        unit=REF.unit,
+        data=np.ma.masked_array(
+            REF.data - REF_timeint.data[np.newaxis, ...], mask=REF.data.mask
+        ),
+        time=REF.time,
+        time_bnds=REF.time_bnds,
+        lat=REF.lat,
+        lat_bnds=REF.lat_bnds,
+        lon=REF.lon,
+        lon_bnds=REF.lon_bnds,
+        area=REF.area,
+        ndata=REF.ndata,
+    )
+    crms = cREF.rms()
+
+    if df_errs is not None and name in df_errs["variable"].unique():
+        values = []
+        mask = []
+        for region in df_errs.region.unique():
+            val = df_errs.loc[
+                (df_errs["variable"] == name)
+                & (df_errs["type"] == "bias")
+                & (df_errs["region"] == region)
+                & (df_errs["quantile"] == 70),
+                "value",
+            ]
+            if len(val) > 0:
+                mask.append(ILAMBregions.getMask(region, bias))
+                values.append((mask[-1] == False) * float(val))
+        bias_score_map = deepcopy(bias)
+        bias_score_map.data = np.ma.masked_array(
+            np.array(values).sum(axis=0), mask=np.array(mask).all(axis=0)
+        )
+        msg = f"[{name}] Bias scored using regional quantiles"
+        logger.info(msg)
+        with np.errstate(under="ignore"):
+            bias_score_map.data = (1 - np.abs(bias.data) / bias_score_map.data).clip(
+                0, 1
+            )
+        bias_score_map.unit = "1"
+        bias_score_map.name = "biasscore_map_of_%s" % name
+    else:
+        msg = f"[{name}] Bias scored using Collier2018"
+        logger.info(msg)
+        bias_score_map = Score(bias, crms)
+
     if not skip_rmse:
-        cCOM = Variable(name = "centralized %s" % COM.name, unit = COM.unit,
-                        data = np.ma.masked_array(COM.data-COM_timeint.data[np.newaxis,...],mask=COM.data.mask),
-                        time = COM.time, time_bnds = COM.time_bnds,
-                        lat  = COM.lat , lat_bnds  = COM.lat_bnds,
-                        lon  = COM.lon , lon_bnds  = COM.lon_bnds,
-                        area = COM.area, ndata     = COM.ndata)
-        rmse  =  REF.rmse( COM)
+        cCOM = Variable(
+            name="centralized %s" % COM.name,
+            unit=COM.unit,
+            data=np.ma.masked_array(
+                COM.data - COM_timeint.data[np.newaxis, ...], mask=COM.data.mask
+            ),
+            time=COM.time,
+            time_bnds=COM.time_bnds,
+            lat=COM.lat,
+            lat_bnds=COM.lat_bnds,
+            lon=COM.lon,
+            lon_bnds=COM.lon_bnds,
+            area=COM.area,
+            ndata=COM.ndata,
+        )
+        rmse = REF.rmse(COM)
         crmse = cREF.rmse(cCOM)
-        rmse_score_map = Score(crmse,crms)
+
+        if df_errs is not None and name in df_errs["variable"].unique():
+            values = []
+            mask = []
+            for region in df_errs.region.unique():
+                val = df_errs.loc[
+                    (df_errs["variable"] == name)
+                    & (df_errs["type"] == "rmse")
+                    & (df_errs["region"] == region)
+                    & (df_errs["quantile"] == 70),
+                    "value",
+                ]
+                if len(val) > 0:
+                    mask.append(ILAMBregions.getMask(region, crmse))
+                    values.append((mask[-1] == False) * float(val))
+            rmse_score_map = deepcopy(crmse)
+            rmse_score_map.data = np.ma.masked_array(
+                np.array(values).sum(axis=0), mask=np.array(mask).all(axis=0)
+            )
+            msg = f"[{name}] RMSE scored using regional quantiles"
+            logger.info(msg)
+            with np.errstate(under="ignore"):
+                rmse_score_map.data = (1 - crmse.data / rmse_score_map.data).clip(0, 1)
+            rmse_score_map.unit = "1"
+            rmse_score_map.name = "rmsescore_map_of_%s" % name
+        else:
+            msg = f"[{name}] RMSE scored using Collier2018"
+            logger.info(msg)
+            rmse_score_map = Score(crmse, crms)
     if not skip_iav:
-        ref_iav = Variable(name = "centralized %s" % ref.name, unit = ref.unit,
-                           data = np.ma.masked_array(ref.data-ref_timeint.data[np.newaxis,...],mask=ref.data.mask),
-                           time = ref.time, time_bnds = ref.time_bnds,
-                           lat  = ref.lat , lat_bnds  = ref.lat_bnds,
-                           lon  = ref.lon , lon_bnds  = ref.lon_bnds,
-                           area = ref.area, ndata     = ref.ndata).rms()
-        com_iav = Variable(name = "centralized %s" % com.name, unit = com.unit,
-                           data = np.ma.masked_array(com.data-com_timeint.data[np.newaxis,...],mask=com.data.mask),
-                           time = com.time, time_bnds = com.time_bnds,
-                           lat  = com.lat , lat_bnds  = com.lat_bnds,
-                           lon  = com.lon , lon_bnds  = com.lon_bnds,
-                           area = com.area, ndata     = com.ndata).rms()
-        REF_iav = Variable(name = "centralized %s" % REF.name, unit = REF.unit,
-                           data = np.ma.masked_array(REF.data-REF_timeint.data[np.newaxis,...],mask=REF.data.mask),
-                           time = REF.time, time_bnds = REF.time_bnds,
-                           lat  = REF.lat , lat_bnds  = REF.lat_bnds,
-                           lon  = REF.lon , lon_bnds  = REF.lon_bnds,
-                           area = REF.area, ndata     = REF.ndata).rms()
-        COM_iav = Variable(name = "centralized %s" % COM.name, unit = COM.unit,
-                           data = np.ma.masked_array(COM.data-COM_timeint.data[np.newaxis,...],mask=COM.data.mask),
-                           time = COM.time, time_bnds = COM.time_bnds,
-                           lat  = COM.lat , lat_bnds  = COM.lat_bnds,
-                           lon  = COM.lon , lon_bnds  = COM.lon_bnds,
-                           area = COM.area, ndata     = COM.ndata).rms()
-        iav_score_map = Score(Variable(name = "diff %s" % REF.name, unit = REF.unit,
-                                       data = (COM_iav.data-REF_iav.data),
-                                       lat  = REF.lat , lat_bnds  = REF.lat_bnds,
-                                       lon  = REF.lon , lon_bnds  = REF.lon_bnds,
-                                       area = REF.area, ndata     = REF.ndata),
-                              REF_iav)
+        ref_iav = Variable(
+            name="centralized %s" % ref.name,
+            unit=ref.unit,
+            data=np.ma.masked_array(
+                ref.data - ref_timeint.data[np.newaxis, ...], mask=ref.data.mask
+            ),
+            time=ref.time,
+            time_bnds=ref.time_bnds,
+            lat=ref.lat,
+            lat_bnds=ref.lat_bnds,
+            lon=ref.lon,
+            lon_bnds=ref.lon_bnds,
+            area=ref.area,
+            ndata=ref.ndata,
+        ).rms()
+        com_iav = Variable(
+            name="centralized %s" % com.name,
+            unit=com.unit,
+            data=np.ma.masked_array(
+                com.data - com_timeint.data[np.newaxis, ...], mask=com.data.mask
+            ),
+            time=com.time,
+            time_bnds=com.time_bnds,
+            lat=com.lat,
+            lat_bnds=com.lat_bnds,
+            lon=com.lon,
+            lon_bnds=com.lon_bnds,
+            area=com.area,
+            ndata=com.ndata,
+        ).rms()
+        REF_iav = Variable(
+            name="centralized %s" % REF.name,
+            unit=REF.unit,
+            data=np.ma.masked_array(
+                REF.data - REF_timeint.data[np.newaxis, ...], mask=REF.data.mask
+            ),
+            time=REF.time,
+            time_bnds=REF.time_bnds,
+            lat=REF.lat,
+            lat_bnds=REF.lat_bnds,
+            lon=REF.lon,
+            lon_bnds=REF.lon_bnds,
+            area=REF.area,
+            ndata=REF.ndata,
+        ).rms()
+        COM_iav = Variable(
+            name="centralized %s" % COM.name,
+            unit=COM.unit,
+            data=np.ma.masked_array(
+                COM.data - COM_timeint.data[np.newaxis, ...], mask=COM.data.mask
+            ),
+            time=COM.time,
+            time_bnds=COM.time_bnds,
+            lat=COM.lat,
+            lat_bnds=COM.lat_bnds,
+            lon=COM.lon,
+            lon_bnds=COM.lon_bnds,
+            area=COM.area,
+            ndata=COM.ndata,
+        ).rms()
+        iav_score_map = Score(
+            Variable(
+                name="diff %s" % REF.name,
+                unit=REF.unit,
+                data=(COM_iav.data - REF_iav.data),
+                lat=REF.lat,
+                lat_bnds=REF.lat_bnds,
+                lon=REF.lon,
+                lon_bnds=REF.lon_bnds,
+                area=REF.area,
+                ndata=REF.ndata,
+            ),
+            REF_iav,
+        )
 
     # The phase shift comes from the interpolated quantities
     if not skip_cycle:
-        ref_cycle       = REF.annualCycle()
-        com_cycle       = COM.annualCycle()
-        ref_maxt_map    = ref_cycle.timeOfExtrema(etype="max")
-        com_maxt_map    = com_cycle.timeOfExtrema(etype="max")
-        shift_map       = ref_maxt_map.phaseShift(com_maxt_map)
+        ref_cycle = REF.annualCycle()
+        com_cycle = COM.annualCycle()
+        ref_maxt_map = ref_cycle.timeOfExtrema(etype="max")
+        com_maxt_map = com_cycle.timeOfExtrema(etype="max")
+        shift_map = ref_maxt_map.phaseShift(com_maxt_map)
         shift_score_map = ScoreSeasonalCycle(shift_map)
-        shift_map.data /= 30.; shift_map.unit = "months"
+        shift_map.data /= 30.0
+        shift_map.unit = "months"
 
     # Scalars
-    ref_period_mean = {}; ref_spaceint = {}; ref_mean_cycle = {}; ref_dtcycle = {}
-    com_period_mean = {}; com_spaceint = {}; com_mean_cycle = {}; com_dtcycle = {}
-    bias_val  = {}; bias_score = {}; rmse_val = {}; rmse_score = {}
-    space_std = {}; space_cor  = {}; sd_score = {}; shift = {}; shift_score = {}; iav_score = {}
-    ref_union_mean = {}; ref_comp_mean = {}
-    com_union_mean = {}; com_comp_mean = {}
+    ref_period_mean = {}
+    ref_spaceint = {}
+    ref_mean_cycle = {}
+    ref_dtcycle = {}
+    com_period_mean = {}
+    com_spaceint = {}
+    com_mean_cycle = {}
+    com_dtcycle = {}
+    bias_val = {}
+    bias_score = {}
+    rmse_val = {}
+    rmse_score = {}
+    space_std = {}
+    space_cor = {}
+    sd_score = {}
+    shift = {}
+    shift_score = {}
+    iav_score = {}
+    ref_union_mean = {}
+    ref_comp_mean = {}
+    com_union_mean = {}
+    com_comp_mean = {}
     for region in regions:
-        ref_period_mean[region] = ref_timeint    .siteStats(region=region)
-        ref_spaceint   [region] = ref            .siteStats(region=region)
-        com_period_mean[region] = com_timeint    .siteStats(region=region)
-        com_spaceint   [region] = com            .siteStats(region=region)
-        bias_val       [region] = bias           .siteStats(region=region)
-        bias_score     [region] = bias_score_map .siteStats(region=region,weight=normalizer)
+        ref_period_mean[region] = ref_timeint.siteStats(region=region)
+        ref_spaceint[region] = ref.siteStats(region=region)
+        com_period_mean[region] = com_timeint.siteStats(region=region)
+        com_spaceint[region] = com.siteStats(region=region)
+        bias_val[region] = bias.siteStats(region=region)
+        bias_score[region] = bias_score_map.siteStats(region=region, weight=normalizer)
         if not skip_cycle:
-            ref_mean_cycle [region] = ref_cycle  .siteStats(region=region)
-            ref_dtcycle    [region] = deepcopy(ref_mean_cycle[region])
-            ref_dtcycle    [region].data -= ref_mean_cycle[region].data.mean()
-            com_mean_cycle [region] = com_cycle  .siteStats(region=region)
-            com_dtcycle    [region] = deepcopy(com_mean_cycle[region])
-            com_dtcycle    [region].data -= com_mean_cycle[region].data.mean()
-            shift          [region] = shift_map  .siteStats(region=region,intabs=True)
-            shift_score    [region] = shift_score_map.siteStats(region=region,weight=normalizer)
+            ref_mean_cycle[region] = ref_cycle.siteStats(region=region)
+            ref_dtcycle[region] = deepcopy(ref_mean_cycle[region])
+            ref_dtcycle[region].data -= ref_mean_cycle[region].data.mean()
+            com_mean_cycle[region] = com_cycle.siteStats(region=region)
+            com_dtcycle[region] = deepcopy(com_mean_cycle[region])
+            com_dtcycle[region].data -= com_mean_cycle[region].data.mean()
+            shift[region] = shift_map.siteStats(region=region, intabs=True)
+            shift_score[region] = shift_score_map.siteStats(
+                region=region, weight=normalizer
+            )
         if not skip_rmse:
-            rmse_val   [region] = rmse           .siteStats(region=region)
-            rmse_score [region] = rmse_score_map .siteStats(region=region,weight=normalizer)
+            rmse_val[region] = rmse.siteStats(region=region)
+            rmse_score[region] = rmse_score_map.siteStats(
+                region=region, weight=normalizer
+            )
         if not skip_iav:
-            iav_score  [region] = iav_score_map .siteStats(region=region,weight=normalizer)
+            iav_score[region] = iav_score_map.siteStats(
+                region=region, weight=normalizer
+            )
 
         ref_period_mean[region].name = "Period Mean (original grids) %s" % (region)
-        ref_spaceint   [region].name = "spaceint_of_%s_over_%s"        % (ref.name,region)
+        ref_spaceint[region].name = "spaceint_of_%s_over_%s" % (ref.name, region)
         com_period_mean[region].name = "Period Mean (original grids) %s" % (region)
-        com_spaceint   [region].name = "spaceint_of_%s_over_%s"        % (ref.name,region)
-        bias_val       [region].name = "Bias %s"                       % (region)
-        bias_score     [region].name = "Bias Score %s"                 % (region)
+        com_spaceint[region].name = "spaceint_of_%s_over_%s" % (ref.name, region)
+        bias_val[region].name = "Bias %s" % (region)
+        bias_score[region].name = "Bias Score %s" % (region)
         if not skip_rmse:
-            rmse_val   [region].name = "RMSE %s"                       % (region)
-            rmse_score [region].name = "RMSE Score %s"                 % (region)
+            rmse_val[region].name = "RMSE %s" % (region)
+            rmse_score[region].name = "RMSE Score %s" % (region)
         if not skip_iav:
-            iav_score  [region].name = "Interannual Variability Score %s" % (region)
+            iav_score[region].name = "Interannual Variability Score %s" % (region)
         if not skip_cycle:
-            ref_mean_cycle[region].name = "cycle_of_%s_over_%s"           % (ref.name,region)
-            ref_dtcycle   [region].name = "dtcycle_of_%s_over_%s"         % (ref.name,region)
-            com_mean_cycle[region].name = "cycle_of_%s_over_%s"           % (ref.name,region)
-            com_dtcycle   [region].name = "dtcycle_of_%s_over_%s"         % (ref.name,region)
-            shift         [region].name = "Phase Shift %s"                % (region)
-            shift_score   [region].name = "Seasonal Cycle Score %s"       % (region)
+            ref_mean_cycle[region].name = "cycle_of_%s_over_%s" % (ref.name, region)
+            ref_dtcycle[region].name = "dtcycle_of_%s_over_%s" % (ref.name, region)
+            com_mean_cycle[region].name = "cycle_of_%s_over_%s" % (ref.name, region)
+            com_dtcycle[region].name = "dtcycle_of_%s_over_%s" % (ref.name, region)
+            shift[region].name = "Phase Shift %s" % (region)
+            shift_score[region].name = "Seasonal Cycle Score %s" % (region)
 
     # Unit conversions
-    def _convert(var,unit):
+    def _convert(var, unit):
         if type(var) == type({}):
-            for key in var.keys(): var[key].convert(unit)                
+            for key in var.keys():
+                var[key].convert(unit)
         else:
             var.convert(unit)
-            
+
     if table_unit is not None:
-        for var in [ref_period_mean,com_period_mean,ref_union_mean,com_union_mean,ref_comp_mean,com_comp_mean]:
-            _convert(var,table_unit)
+        for var in [
+            ref_period_mean,
+            com_period_mean,
+            ref_union_mean,
+            com_union_mean,
+            ref_comp_mean,
+            com_comp_mean,
+        ]:
+            _convert(var, table_unit)
     if plot_unit is not None:
-        plot_vars = [com_timeint,ref_timeint,bias,com_spaceint,ref_spaceint,bias_val]
-        if not skip_rmse:  plot_vars += [rmse,rmse_val]
-        if not skip_cycle: plot_vars += [com_mean_cycle,ref_mean_cycle,com_dtcycle,ref_dtcycle]
-        if not skip_iav:   plot_vars += [com_iav]
-        for var in plot_vars: _convert(var,plot_unit)
+        plot_vars = [
+            com_timeint,
+            ref_timeint,
+            bias,
+            com_spaceint,
+            ref_spaceint,
+            bias_val,
+        ]
+        if not skip_rmse:
+            plot_vars += [rmse, rmse_val]
+        if not skip_cycle:
+            plot_vars += [com_mean_cycle, ref_mean_cycle, com_dtcycle, ref_dtcycle]
+        if not skip_iav:
+            plot_vars += [com_iav]
+        for var in plot_vars:
+            _convert(var, plot_unit)
 
     # Rename and optionally dump out information to netCDF4 files
-    com_timeint    .name = "timeint_of_%s"        % ref.name
-    bias           .name = "bias_map_of_%s"       % ref.name
-    bias_score_map .name = "biasscore_map_of_%s"  % ref.name
-
-    out_vars = [com_period_mean,
-                ref_union_mean,
-                com_union_mean,
-                ref_comp_mean,
-                com_comp_mean,
-                com_timeint,
-                com_mean_cycle,
-                com_dtcycle,
-                bias,
-                bias_score_map,
-                bias_val,
-                bias_score,
-                shift,
-                shift_score]
-    if com_spaceint[list(com_spaceint.keys())[0]].data.size > 1: out_vars.append(com_spaceint)
+    com_timeint.name = "timeint_of_%s" % ref.name
+    bias.name = "bias_map_of_%s" % ref.name
+    bias_score_map.name = "biasscore_map_of_%s" % ref.name
+
+    out_vars = [
+        com_period_mean,
+        ref_union_mean,
+        com_union_mean,
+        ref_comp_mean,
+        com_comp_mean,
+        com_timeint,
+        com_mean_cycle,
+        com_dtcycle,
+        bias,
+        bias_score_map,
+        bias_val,
+        bias_score,
+        shift,
+        shift_score,
+    ]
+    if com_spaceint[list(com_spaceint.keys())[0]].data.size > 1:
+        out_vars.append(com_spaceint)
     if not skip_cycle:
-        com_maxt_map   .name = "phase_map_of_%s"      % ref.name
-        shift_map      .name = "shift_map_of_%s"      % ref.name
+        com_maxt_map.name = "phase_map_of_%s" % ref.name
+        shift_map.name = "shift_map_of_%s" % ref.name
         shift_score_map.name = "shiftscore_map_of_%s" % ref.name
         out_vars.append(com_maxt_map)
         out_vars.append(shift_map)
         out_vars.append(shift_score_map)
     if not skip_rmse:
-        rmse          .name = "rmse_map_of_%s"       % ref.name
-        rmse_score_map.name = "rmsescore_map_of_%s"  % ref.name
-        out_vars.append(rmse)
+        crmse.name = "rmse_map_of_%s" % ref.name
+        rmse_score_map.name = "rmsescore_map_of_%s" % ref.name
+        out_vars.append(crmse)
         out_vars.append(rmse_score_map)
         out_vars.append(rmse_val)
         out_vars.append(rmse_score)
     if not skip_iav:
-        com_iav.name       = "iav_map_of_%s" % ref.name
-        iav_score_map.name = "iavscore_map_of_%s"  % ref.name
+        com_iav.name = "iav_map_of_%s" % ref.name
+        iav_score_map.name = "iavscore_map_of_%s" % ref.name
         out_vars.append(com_iav)
         out_vars.append(iav_score_map)
         out_vars.append(iav_score)
     if dataset is not None:
         for var in out_vars:
             if type(var) == type({}):
-                for key in var.keys(): var[key].toNetCDF4(dataset,group="MeanState")
+                for key in var.keys():
+                    var[key].toNetCDF4(dataset, group="MeanState")
             else:
-                var.toNetCDF4(dataset,group="MeanState")
+                var.toNetCDF4(dataset, group="MeanState")
     for key in sd_score.keys():
-        sd_score[key].toNetCDF4(dataset,group="MeanState",
-                                attributes={"std":space_std[key].data,
-                                            "R"  :space_cor[key].data})
+        sd_score[key].toNetCDF4(
+            dataset,
+            group="MeanState",
+            attributes={"std": space_std[key].data, "R": space_cor[key].data},
+        )
 
     # Rename and optionally dump out information to netCDF4 files
-    out_vars = [ref_period_mean,ref_timeint]
-    if ref_spaceint[list(ref_spaceint.keys())[0]].data.size > 1: out_vars.append(ref_spaceint)
-    ref_timeint .name = "timeint_of_%s"        % ref.name
+    out_vars = [ref_period_mean, ref_timeint]
+    if ref_spaceint[list(ref_spaceint.keys())[0]].data.size > 1:
+        out_vars.append(ref_spaceint)
+    ref_timeint.name = "timeint_of_%s" % ref.name
     if not skip_cycle:
-        ref_maxt_map.name = "phase_map_of_%s"      % ref.name
-        out_vars += [ref_maxt_map,ref_mean_cycle,ref_dtcycle]
+        ref_maxt_map.name = "phase_map_of_%s" % ref.name
+        out_vars += [ref_maxt_map, ref_mean_cycle, ref_dtcycle]
     if not skip_iav:
-        ref_iav.name      = "iav_map_of_%s" % ref.name
+        ref_iav.name = "iav_map_of_%s" % ref.name
         out_vars.append(ref_iav)
     if benchmark_dataset is not None:
         for var in out_vars:
             if type(var) == type({}):
-                for key in var.keys(): var[key].toNetCDF4(benchmark_dataset,group="MeanState")
+                for key in var.keys():
+                    var[key].toNetCDF4(benchmark_dataset, group="MeanState")
             else:
-                var.toNetCDF4(benchmark_dataset,group="MeanState")
+                var.toNetCDF4(benchmark_dataset, group="MeanState")
 
     return
 
 
-def AnalysisMeanStateSpace(ref,com,**keywords):
+def AnalysisMeanStateSpace(ref, com, **keywords):
     """Perform a mean state analysis.
 
     This mean state analysis examines the model mean state in space
     and time. We compute the mean variable value over the time period
     at each spatial cell or data site as appropriate, as well as the
     bias and RMSE relative to the observational variable. We will
     output maps of the period mean values and bias. For each spatial
@@ -1291,366 +1702,600 @@
         mean values
     table_unit : str, optional
         the unit to use when displaying output in tables on the HTML page
     plots_unit : str, optional
         the unit to use when displaying output on plots on the HTML page
 
     """
-    from .Variable import Variable
-    regions           = keywords.get("regions"          ,["global"])
-    dataset           = keywords.get("dataset"          ,None)
-    benchmark_dataset = keywords.get("benchmark_dataset",None)
-    space_mean        = keywords.get("space_mean"       ,True)
-    table_unit        = keywords.get("table_unit"       ,None)
-    plot_unit         = keywords.get("plot_unit"        ,None)
-    mass_weighting    = keywords.get("mass_weighting"   ,False)
-    skip_rmse         = keywords.get("skip_rmse"        ,False)
-    skip_iav          = keywords.get("skip_iav"         ,True )
-    skip_cycle        = keywords.get("skip_cycle"       ,False)
-    ref_timeint       = keywords.get("ref_timeint"      ,None)
-    com_timeint       = keywords.get("com_timeint"      ,None)
-    rmse_score_basis  = keywords.get("rmse_score_basis" ,"cycle")
-    ILAMBregions      = Regions()
-    spatial           = ref.spatial
-    
+    from ILAMB.Variable import Variable
+
+    regions = keywords.get("regions", ["global"])
+    dataset = keywords.get("dataset", None)
+    benchmark_dataset = keywords.get("benchmark_dataset", None)
+    space_mean = keywords.get("space_mean", True)
+    table_unit = keywords.get("table_unit", None)
+    plot_unit = keywords.get("plot_unit", None)
+    mass_weighting = keywords.get("mass_weighting", False)
+    skip_rmse = keywords.get("skip_rmse", False)
+    skip_iav = keywords.get("skip_iav", True)
+    skip_cycle = keywords.get("skip_cycle", False)
+    ref_timeint = keywords.get("ref_timeint", None)
+    com_timeint = keywords.get("com_timeint", None)
+    rmse_score_basis = keywords.get("rmse_score_basis", "cycle")
+    df_errs = keywords.get("df_errs", None)
+    ILAMBregions = Regions()
+    spatial = ref.spatial
+
     # Convert str types to booleans
     if type(skip_rmse) == type(""):
-        skip_rmse = (skip_rmse.lower() == "true")
-    if type(skip_iav ) == type(""):
-        skip_iav  = (skip_iav .lower() == "true")
+        skip_rmse = skip_rmse.lower() == "true"
+    if type(skip_iav) == type(""):
+        skip_iav = skip_iav.lower() == "true"
     if type(skip_cycle) == type(""):
-        skip_cycle = (skip_cycle.lower() == "true")
+        skip_cycle = skip_cycle.lower() == "true"
+    if df_errs is not None:
+        mass_weighting = False
 
     # Check if we need to skip parts of the analysis
-    if not ref.monthly   : skip_cycle = True
-    if ref.time.size < 12: skip_cycle = True
-    if ref.time.size == 1: skip_rmse  = True
-    if skip_rmse         : skip_iav   = True
+    if not ref.monthly:
+        skip_cycle = True
+    if ref.time.size < 12:
+        skip_cycle = True
+    if ref.time.size == 1:
+        skip_rmse = True
+    if skip_rmse:
+        skip_iav = True
     name = ref.name
 
     # Interpolate both reference and comparison to a grid composed of
     # their cell breaks
     ref.convert(plot_unit)
     com.convert(plot_unit)
-    lat,lon,lat_bnds,lon_bnds = _composeGrids(ref,com)
-    REF   = ref.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
-    COM   = com.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
-    unit  = REF.unit
-    area  = REF.area
+    lat, lon, lat_bnds, lon_bnds = _composeGrids(ref, com)
+    REF = ref.interpolate(lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds)
+    COM = com.interpolate(lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds)
+    unit = REF.unit
+    area = REF.area
     ndata = REF.ndata
-            
+
     # Find the mean values over the time period
     if ref_timeint is None:
         ref_timeint = ref.integrateInTime(mean=True).convert(plot_unit)
         REF_timeint = REF.integrateInTime(mean=True).convert(plot_unit)
     else:
         ref_timeint.convert(plot_unit)
-        REF_timeint = ref_timeint.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
+        REF_timeint = ref_timeint.interpolate(
+            lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds
+        )
     if com_timeint is None:
         com_timeint = com.integrateInTime(mean=True).convert(plot_unit)
         COM_timeint = COM.integrateInTime(mean=True).convert(plot_unit)
     else:
         com_timeint.convert(plot_unit)
-        COM_timeint = com_timeint.interpolate(lat=lat,lon=lon,lat_bnds=lat_bnds,lon_bnds=lon_bnds)
-    normalizer  = REF_timeint.data if mass_weighting else None
+        COM_timeint = com_timeint.interpolate(
+            lat=lat, lon=lon, lat_bnds=lat_bnds, lon_bnds=lon_bnds
+        )
+    normalizer = REF_timeint.data if mass_weighting else None
 
     # Report period mean values over all possible representations of
     # land
     ref_and_com = (REF_timeint.data.mask == False) * (COM_timeint.data.mask == False)
-    ref_not_com = (REF_timeint.data.mask == False) * (COM_timeint.data.mask == True )
-    com_not_ref = (REF_timeint.data.mask == True ) * (COM_timeint.data.mask == False)
+    ref_not_com = (REF_timeint.data.mask == False) * (COM_timeint.data.mask == True)
+    com_not_ref = (REF_timeint.data.mask == True) * (COM_timeint.data.mask == False)
     if benchmark_dataset is not None:
-
         ref_timeint.name = "timeint_of_%s" % name
-        ref_timeint.toNetCDF4(benchmark_dataset,group="MeanState")
+        ref_timeint.toNetCDF4(benchmark_dataset, group="MeanState")
         for region in regions:
-
             # reference period mean on original grid
-            ref_period_mean = ref_timeint.integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            ref_period_mean = ref_timeint.integrateInSpace(
+                region=region, mean=space_mean
+            ).convert(table_unit)
             ref_period_mean.name = "Period Mean (original grids) %s" % region
-            ref_period_mean.toNetCDF4(benchmark_dataset,group="MeanState")
+            ref_period_mean.toNetCDF4(benchmark_dataset, group="MeanState")
 
     if dataset is not None:
-
         com_timeint.name = "timeint_of_%s" % name
-        com_timeint.toNetCDF4(dataset,group="MeanState")
+        com_timeint.toNetCDF4(dataset, group="MeanState")
         for region in regions:
-
             # reference period mean on intersection of land
-            ref_union_mean = Variable(name = "REF_and_com", unit = REF_timeint.unit,
-                                      data = np.ma.masked_array(REF_timeint.data,mask=(ref_and_com==False)),
-                                      lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                      area = REF_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            ref_union_mean = (
+                Variable(
+                    name="REF_and_com",
+                    unit=REF_timeint.unit,
+                    data=np.ma.masked_array(
+                        REF_timeint.data, mask=(ref_and_com == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=REF_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
             ref_union_mean.name = "Benchmark Period Mean (intersection) %s" % region
-            ref_union_mean.toNetCDF4(dataset,group="MeanState")
+            ref_union_mean.toNetCDF4(dataset, group="MeanState")
 
             # reference period mean on complement of land
-            ref_comp_mean = Variable(name = "REF_not_com", unit = REF_timeint.unit,
-                                     data = np.ma.masked_array(REF_timeint.data,mask=(ref_not_com==False)),
-                                     lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                     area = REF_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            ref_comp_mean = (
+                Variable(
+                    name="REF_not_com",
+                    unit=REF_timeint.unit,
+                    data=np.ma.masked_array(
+                        REF_timeint.data, mask=(ref_not_com == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=REF_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
             ref_comp_mean.name = "Benchmark Period Mean (complement) %s" % region
-            ref_comp_mean.toNetCDF4(dataset,group="MeanState")
+            ref_comp_mean.toNetCDF4(dataset, group="MeanState")
 
             # comparison period mean on original grid
-            com_period_mean = com_timeint.integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            com_period_mean = com_timeint.integrateInSpace(
+                region=region, mean=space_mean
+            ).convert(table_unit)
             com_period_mean.name = "Period Mean (original grids) %s" % region
-            com_period_mean.toNetCDF4(dataset,group="MeanState")
+            com_period_mean.toNetCDF4(dataset, group="MeanState")
 
             # comparison period mean on intersection of land
-            com_union_mean = Variable(name = "ref_and_COM", unit = COM_timeint.unit,
-                                      data = np.ma.masked_array(COM_timeint.data,mask=(ref_and_com==False)),
-                                      lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                      area = COM_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            com_union_mean = (
+                Variable(
+                    name="ref_and_COM",
+                    unit=COM_timeint.unit,
+                    data=np.ma.masked_array(
+                        COM_timeint.data, mask=(ref_and_com == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=COM_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
             com_union_mean.name = "Model Period Mean (intersection) %s" % region
-            com_union_mean.toNetCDF4(dataset,group="MeanState")
+            com_union_mean.toNetCDF4(dataset, group="MeanState")
 
             # comparison period mean on complement of land
-            com_comp_mean = Variable(name = "COM_not_ref", unit = COM_timeint.unit,
-                                     data = np.ma.masked_array(COM_timeint.data,mask=(com_not_ref==False)),
-                                     lat  = lat, lat_bnds = lat_bnds, lon  = lon, lon_bnds = lon_bnds,
-                                     area = COM_timeint.area).integrateInSpace(region=region,mean=space_mean).convert(table_unit)
+            com_comp_mean = (
+                Variable(
+                    name="COM_not_ref",
+                    unit=COM_timeint.unit,
+                    data=np.ma.masked_array(
+                        COM_timeint.data, mask=(com_not_ref == False)
+                    ),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=COM_timeint.area,
+                )
+                .integrateInSpace(region=region, mean=space_mean)
+                .convert(table_unit)
+            )
             com_comp_mean.name = "Model Period Mean (complement) %s" % region
-            com_comp_mean.toNetCDF4(dataset,group="MeanState")
+            com_comp_mean.toNetCDF4(dataset, group="MeanState")
 
     # Now that we are done reporting on the intersection / complement,
     # set all masks to the intersection
-    REF.data.mask += np.ones(REF.time.size,dtype=bool)[:,np.newaxis,np.newaxis] * (ref_and_com==False)
-    COM.data.mask += np.ones(COM.time.size,dtype=bool)[:,np.newaxis,np.newaxis] * (ref_and_com==False)
-    REF_timeint.data.mask = (ref_and_com==False)
-    COM_timeint.data.mask = (ref_and_com==False)
-    if mass_weighting: normalizer.mask = (ref_and_com==False)
+    REF.data.mask += np.ones(REF.time.size, dtype=bool)[:, np.newaxis, np.newaxis] * (
+        ref_and_com == False
+    )
+    COM.data.mask += np.ones(COM.time.size, dtype=bool)[:, np.newaxis, np.newaxis] * (
+        ref_and_com == False
+    )
+    REF_timeint.data.mask = ref_and_com == False
+    COM_timeint.data.mask = ref_and_com == False
+    if mass_weighting:
+        normalizer.mask = ref_and_com == False
 
     # Spatial Distribution: scalars and scores
     if dataset is not None:
         for region in regions:
-            space_std,space_cor,sd_score = REF_timeint.spatialDistribution(COM_timeint,region=region)
+            space_std, space_cor, sd_score = REF_timeint.spatialDistribution(
+                COM_timeint, region=region
+            )
             sd_score.name = "Spatial Distribution Score %s" % region
-            sd_score.toNetCDF4(dataset,group="MeanState",
-                               attributes={"std":space_std.data,
-                                           "R"  :space_cor.data})
+            sd_score.toNetCDF4(
+                dataset,
+                group="MeanState",
+                attributes={"std": space_std.data, "R": space_cor.data},
+            )
 
     # Cycle: maps, scalars, and scores
     if not skip_cycle:
-        ref_cycle         = REF.annualCycle()
-        ref_maxt_map      = ref_cycle.timeOfExtrema(etype="max")
+        ref_cycle = REF.annualCycle()
+        ref_maxt_map = ref_cycle.timeOfExtrema(etype="max")
         ref_maxt_map.name = "phase_map_of_%s" % name
-        com_cycle         = COM.annualCycle()
-        com_maxt_map      = com_cycle.timeOfExtrema(etype="max")
+        com_cycle = COM.annualCycle()
+        com_maxt_map = com_cycle.timeOfExtrema(etype="max")
         com_maxt_map.name = "phase_map_of_%s" % name
-        shift_map         = ref_maxt_map.phaseShift(com_maxt_map)
-        shift_map.name    = "shift_map_of_%s" % name
-        shift_score_map   = ScoreSeasonalCycle(shift_map)
-        shift_score_map.name  = "shiftscore_map_of_%s" % name
-        shift_map.data   /= 30.; shift_map.unit = "months"
+        shift_map = ref_maxt_map.phaseShift(com_maxt_map)
+        shift_map.name = "shift_map_of_%s" % name
+        shift_score_map = ScoreSeasonalCycle(shift_map)
+        shift_score_map.name = "shiftscore_map_of_%s" % name
+        shift_map.data /= 30.0
+        shift_map.unit = "months"
         if benchmark_dataset is not None:
-            ref_maxt_map.toNetCDF4(benchmark_dataset,group="MeanState")
+            ref_maxt_map.toNetCDF4(benchmark_dataset, group="MeanState")
             for region in regions:
-                ref_mean_cycle      = ref_cycle.integrateInSpace(region=region,mean=True)
-                ref_mean_cycle.name = "cycle_of_%s_over_%s" % (name,region)
-                ref_mean_cycle.toNetCDF4(benchmark_dataset,group="MeanState")
-                ref_dtcycle       = deepcopy(ref_mean_cycle)
+                ref_mean_cycle = ref_cycle.integrateInSpace(region=region, mean=True)
+                ref_mean_cycle.name = "cycle_of_%s_over_%s" % (name, region)
+                ref_mean_cycle.toNetCDF4(benchmark_dataset, group="MeanState")
+                ref_dtcycle = deepcopy(ref_mean_cycle)
                 ref_dtcycle.data -= ref_mean_cycle.data.mean()
-                ref_dtcycle.name  = "dtcycle_of_%s_over_%s" % (name,region)
-                ref_dtcycle.toNetCDF4(benchmark_dataset,group="MeanState")
+                ref_dtcycle.name = "dtcycle_of_%s_over_%s" % (name, region)
+                ref_dtcycle.toNetCDF4(benchmark_dataset, group="MeanState")
         if dataset is not None:
-            com_maxt_map.toNetCDF4(dataset,group="MeanState")
-            shift_map      .toNetCDF4(dataset,group="MeanState")
-            shift_score_map.toNetCDF4(dataset,group="MeanState")
+            com_maxt_map.toNetCDF4(dataset, group="MeanState")
+            shift_map.toNetCDF4(dataset, group="MeanState")
+            shift_score_map.toNetCDF4(dataset, group="MeanState")
             for region in regions:
-                com_mean_cycle      = com_cycle.integrateInSpace(region=region,mean=True)
-                com_mean_cycle.name = "cycle_of_%s_over_%s" % (name,region)
-                com_mean_cycle.toNetCDF4(dataset,group="MeanState")
-                com_dtcycle       = deepcopy(com_mean_cycle)
+                com_mean_cycle = com_cycle.integrateInSpace(region=region, mean=True)
+                com_mean_cycle.name = "cycle_of_%s_over_%s" % (name, region)
+                com_mean_cycle.toNetCDF4(dataset, group="MeanState")
+                com_dtcycle = deepcopy(com_mean_cycle)
                 com_dtcycle.data -= com_mean_cycle.data.mean()
-                com_dtcycle.name  = "dtcycle_of_%s_over_%s" % (name,region)
-                com_dtcycle.toNetCDF4(dataset,group="MeanState")
-                shift       = shift_map.integrateInSpace(region=region,mean=True,intabs=True)
-                shift_score = shift_score_map.integrateInSpace(region=region,mean=True,weight=normalizer)
-                shift      .name = "Phase Shift %s" % region
-                shift      .toNetCDF4(dataset,group="MeanState")
+                com_dtcycle.name = "dtcycle_of_%s_over_%s" % (name, region)
+                com_dtcycle.toNetCDF4(dataset, group="MeanState")
+                shift = shift_map.integrateInSpace(
+                    region=region, mean=True, intabs=True
+                )
+                shift_score = shift_score_map.integrateInSpace(
+                    region=region, mean=True, weight=normalizer
+                )
+                shift.name = "Phase Shift %s" % region
+                shift.toNetCDF4(dataset, group="MeanState")
                 shift_score.name = "Seasonal Cycle Score %s" % region
-                shift_score.toNetCDF4(dataset,group="MeanState")
+                shift_score.toNetCDF4(dataset, group="MeanState")
 
-        del shift_map,shift_score_map
+        del shift_map, shift_score_map
 
         # IAV: maps, scalars, scores
         if not skip_iav:
-            REF_iav = Variable(data = np.ma.masked_array(REF.data-ExtendAnnualCycle(REF.time,ref_cycle.data,ref_cycle.time),mask=REF.data.mask),
-                               unit = unit,
-                               time = REF.time, time_bnds = REF.time_bnds,
-                               lat  = lat, lat_bnds = lat_bnds, lon = lon, lon_bnds = lon_bnds,
-                               area = REF.area, ndata = REF.ndata).rms()
-            COM_iav = Variable(data = np.ma.masked_array(COM.data-ExtendAnnualCycle(COM.time,com_cycle.data,com_cycle.time),mask=COM.data.mask),
-                               unit = unit,
-                               time = COM.time, time_bnds = COM.time_bnds,
-                               lat  = lat, lat_bnds = lat_bnds, lon = lon, lon_bnds = lon_bnds,
-                               area = COM.area, ndata = COM.ndata).rms()
-            iav_score_map = Score(Variable(name = "diff %s" % name, unit = unit,
-                                           data = (COM_iav.data-REF_iav.data),
-                                           lat  = lat, lat_bnds = lat_bnds, lon = lon, lon_bnds = lon_bnds,
-                                           area = area, ndata = ndata),
-                                  REF_iav)
+            REF_iav = Variable(
+                data=np.ma.masked_array(
+                    REF.data
+                    - ExtendAnnualCycle(REF.time, ref_cycle.data, ref_cycle.time),
+                    mask=REF.data.mask,
+                ),
+                unit=unit,
+                time=REF.time,
+                time_bnds=REF.time_bnds,
+                lat=lat,
+                lat_bnds=lat_bnds,
+                lon=lon,
+                lon_bnds=lon_bnds,
+                area=REF.area,
+                ndata=REF.ndata,
+            ).rms()
+            COM_iav = Variable(
+                data=np.ma.masked_array(
+                    COM.data
+                    - ExtendAnnualCycle(COM.time, com_cycle.data, com_cycle.time),
+                    mask=COM.data.mask,
+                ),
+                unit=unit,
+                time=COM.time,
+                time_bnds=COM.time_bnds,
+                lat=lat,
+                lat_bnds=lat_bnds,
+                lon=lon,
+                lon_bnds=lon_bnds,
+                area=COM.area,
+                ndata=COM.ndata,
+            ).rms()
+            iav_score_map = Score(
+                Variable(
+                    name="diff %s" % name,
+                    unit=unit,
+                    data=(COM_iav.data - REF_iav.data),
+                    lat=lat,
+                    lat_bnds=lat_bnds,
+                    lon=lon,
+                    lon_bnds=lon_bnds,
+                    area=area,
+                    ndata=ndata,
+                ),
+                REF_iav,
+            )
             if benchmark_dataset is not None:
                 REF_iav.name = "iav_map_of_%s" % name
-                REF_iav.toNetCDF4(benchmark_dataset,group="MeanState")
+                REF_iav.toNetCDF4(benchmark_dataset, group="MeanState")
             if dataset is not None:
                 COM_iav.name = "iav_map_of_%s" % name
-                COM_iav.toNetCDF4(dataset,group="MeanState")
-                iav_score_map.name = "iavscore_map_of_%s"  % name
-                iav_score_map.toNetCDF4(dataset,group="MeanState")
+                COM_iav.toNetCDF4(dataset, group="MeanState")
+                iav_score_map.name = "iavscore_map_of_%s" % name
+                iav_score_map.toNetCDF4(dataset, group="MeanState")
                 for region in regions:
-                    iav_score = iav_score_map.integrateInSpace(region=region,mean=True,weight=normalizer)
+                    iav_score = iav_score_map.integrateInSpace(
+                        region=region, mean=True, weight=normalizer
+                    )
                     iav_score.name = "Interannual Variability Score %s" % region
-                    iav_score.toNetCDF4(dataset,group="MeanState")
-            del ref_cycle,com_cycle,REF_iav,COM_iav,iav_score_map
+                    iav_score.toNetCDF4(dataset, group="MeanState")
+            del ref_cycle, com_cycle, REF_iav, COM_iav, iav_score_map
 
     # Bias: maps, scalars, and scores
     bias = REF_timeint.bias(COM_timeint).convert(plot_unit)
-    cREF = Variable(name = "centralized %s" % name, unit = REF.unit,
-                    data = np.ma.masked_array(REF.data-REF_timeint.data[np.newaxis,...],mask=REF.data.mask),
-                    time = REF.time, time_bnds = REF.time_bnds, ndata = REF.ndata,
-                    lat  = lat, lat_bnds = lat_bnds, lon = lon, lon_bnds = lon_bnds, area = REF.area).convert(plot_unit)
+    cREF = Variable(
+        name="centralized %s" % name,
+        unit=REF.unit,
+        data=np.ma.masked_array(
+            REF.data - REF_timeint.data[np.newaxis, ...], mask=REF.data.mask
+        ),
+        time=REF.time,
+        time_bnds=REF.time_bnds,
+        ndata=REF.ndata,
+        lat=lat,
+        lat_bnds=lat_bnds,
+        lon=lon,
+        lon_bnds=lon_bnds,
+        area=REF.area,
+    ).convert(plot_unit)
     REF_std = cREF.rms()
-    if skip_rmse: del cREF
-    bias_score_map = Score(bias,REF_std if REF.time.size > 1 else REF_timeint)
-    bias_score_map.data.mask = (ref_and_com==False) # for some reason I need to explicitly force the mask
+    if skip_rmse:
+        del cREF
+    if df_errs is not None and name in df_errs["variable"].unique():
+        values = []
+        mask = []
+        for region in df_errs.region.unique():
+            val = df_errs.loc[
+                (df_errs["variable"] == name)
+                & (df_errs["type"] == "bias")
+                & (df_errs["region"] == region)
+                & (df_errs["quantile"] == 70),
+                "value",
+            ]
+            if len(val) > 0:
+                mask.append(ILAMBregions.getMask(region, bias))
+                values.append((mask[-1] == False) * float(val))
+        bias_score_map = deepcopy(bias)
+        bias_score_map.data = np.ma.masked_array(
+            np.array(values).sum(axis=0), mask=np.array(mask).all(axis=0)
+        )
+        msg = f"[{name}] Bias scored using regional quantiles"
+        logger.info(msg)
+        with np.errstate(under="ignore"):
+            bias_score_map.data = (1 - np.abs(bias.data) / bias_score_map.data).clip(
+                0, 1
+            )
+        bias_score_map.unit = "1"
+        bias_score_map.name = "biasscore_map_of_%s" % name
+    else:
+        msg = f"[{name}] Bias scored using Collier2018"
+        logger.info(msg)
+        bias_score_map = Score(bias, REF_std if REF.time.size > 1 else REF_timeint)
+        bias_score_map.data.mask = (
+            ref_and_com == False
+        )  # for some reason I need to explicitly force the mask
+
     if dataset is not None:
         bias.name = "bias_map_of_%s" % name
-        bias.toNetCDF4(dataset,group="MeanState")
+        bias.toNetCDF4(dataset, group="MeanState")
         bias_score_map.name = "biasscore_map_of_%s" % name
-        bias_score_map.toNetCDF4(dataset,group="MeanState")
+        bias_score_map.toNetCDF4(dataset, group="MeanState")
         for region in regions:
-            bias_val = bias.integrateInSpace(region=region,mean=True).convert(plot_unit)
+            bias_val = bias.integrateInSpace(region=region, mean=True).convert(
+                plot_unit
+            )
             bias_val.name = "Bias %s" % region
-            bias_val.toNetCDF4(dataset,group="MeanState")
-            bias_score = bias_score_map.integrateInSpace(region=region,mean=True,weight=normalizer)
+            bias_val.toNetCDF4(dataset, group="MeanState")
+            bias_score = bias_score_map.integrateInSpace(
+                region=region, mean=True, weight=normalizer
+            )
             bias_score.name = "Bias Score %s" % region
-            bias_score.toNetCDF4(dataset,group="MeanState")
-    del bias,bias_score_map
+            bias_score.toNetCDF4(dataset, group="MeanState")
+    del bias, bias_score_map
 
     # Spatial mean: plots
     if REF.time.size > 1:
         if benchmark_dataset is not None:
             for region in regions:
-                ref_spaceint = REF.integrateInSpace(region=region,mean=True)
-                ref_spaceint.name = "spaceint_of_%s_over_%s" % (name,region)
-                ref_spaceint.toNetCDF4(benchmark_dataset,group="MeanState")
+                ref_spaceint = REF.integrateInSpace(region=region, mean=True)
+                ref_spaceint.name = "spaceint_of_%s_over_%s" % (name, region)
+                ref_spaceint.toNetCDF4(benchmark_dataset, group="MeanState")
         if dataset is not None:
             for region in regions:
-                com_spaceint = COM.integrateInSpace(region=region,mean=True)
-                com_spaceint.name = "spaceint_of_%s_over_%s" % (name,region)
-                com_spaceint.toNetCDF4(dataset,group="MeanState")
+                com_spaceint = COM.integrateInSpace(region=region, mean=True)
+                com_spaceint.name = "spaceint_of_%s_over_%s" % (name, region)
+                com_spaceint.toNetCDF4(dataset, group="MeanState")
 
     # RMSE: maps, scalars, and scores
     if (not skip_rmse) and (rmse_score_basis == "series"):
         rmse = REF.rmse(COM).convert(plot_unit)
         del REF
-        cCOM = Variable(name = "centralized %s" % name, unit = unit,
-                        data = np.ma.masked_array(COM.data-COM_timeint.data[np.newaxis,...],mask=COM.data.mask),
-                        time = COM.time, time_bnds = COM.time_bnds,
-                        lat  = lat, lat_bnds = lat_bnds, lon = lon, lon_bnds = lon_bnds,
-                        area = COM.area, ndata = COM.ndata).convert(plot_unit)
-        
+        cCOM = Variable(
+            name="centralized %s" % name,
+            unit=unit,
+            data=np.ma.masked_array(
+                COM.data - COM_timeint.data[np.newaxis, ...], mask=COM.data.mask
+            ),
+            time=COM.time,
+            time_bnds=COM.time_bnds,
+            lat=lat,
+            lat_bnds=lat_bnds,
+            lon=lon,
+            lon_bnds=lon_bnds,
+            area=COM.area,
+            ndata=COM.ndata,
+        ).convert(plot_unit)
+
         try:
             import psutil
+
             comm = MPI.COMM_WORLD
             rank = comm.Get_rank()
             pname = MPI.Get_processor_name()
             process = psutil.Process(os.getpid())
-            used = process.memory_info().rss*1e-9
-            msg = "[%d][%s] Process peak memory %.2f [Gb]" % (rank,pname,used)
+            used = process.memory_info().rss * 1e-9
+            msg = "[%d][%s] Process peak memory %.2f [Gb]" % (rank, pname, used)
             logger.info(msg)
         except:
             pass
-        
+
         del COM
         crmse = cREF.rmse(cCOM).convert(plot_unit)
-        del cREF,cCOM
-        rmse_score_map = Score(crmse,REF_std)
+        del cREF, cCOM
+
+        if df_errs is not None and name in df_errs["variable"].unique():
+            values = []
+            mask = []
+            for region in df_errs.region.unique():
+                val = df_errs.loc[
+                    (df_errs["variable"] == name)
+                    & (df_errs["type"] == "rmse")
+                    & (df_errs["region"] == region)
+                    & (df_errs["quantile"] == 70),
+                    "value",
+                ]
+                if len(val) > 0:
+                    mask.append(ILAMBregions.getMask(region, crmse))
+                    values.append((mask[-1] == False) * float(val))
+            rmse_score_map = deepcopy(crmse)
+            rmse_score_map.data = np.ma.masked_array(
+                np.array(values).sum(axis=0), mask=np.array(mask).all(axis=0)
+            )
+            msg = f"[{name}] RMSE scored using regional quantiles"
+            logger.info(msg)
+            with np.errstate(under="ignore"):
+                rmse_score_map.data = (1 - crmse.data / rmse_score_map.data).clip(0, 1)
+            rmse_score_map.unit = "1"
+            rmse_score_map.name = "rmsescore_map_of_%s" % name
+        else:
+            msg = f"[{name}] RMSE scored using Collier2018"
+            logger.info(msg)
+            rmse_score_map = Score(crmse, REF_std)
+
         if dataset is not None:
             rmse.name = "rmse_map_of_%s" % name
-            rmse.toNetCDF4(dataset,group="MeanState")
+            crmse.name = "rmse_map_of_%s" % name
+            crmse.toNetCDF4(dataset, group="MeanState")
             rmse_score_map.name = "rmsescore_map_of_%s" % name
-            rmse_score_map.toNetCDF4(dataset,group="MeanState")
+            rmse_score_map.toNetCDF4(dataset, group="MeanState")
             for region in regions:
-                rmse_val = rmse.integrateInSpace(region=region,mean=True).convert(plot_unit)
+                rmse_val = rmse.integrateInSpace(region=region, mean=True).convert(
+                    plot_unit
+                )
                 rmse_val.name = "RMSE %s" % region
-                rmse_val.toNetCDF4(dataset,group="MeanState")
-                rmse_score = rmse_score_map.integrateInSpace(region=region,mean=True,weight=normalizer)
+                rmse_val.toNetCDF4(dataset, group="MeanState")
+                rmse_score = rmse_score_map.integrateInSpace(
+                    region=region, mean=True, weight=normalizer
+                )
                 rmse_score.name = "RMSE Score %s" % region
-                rmse_score.toNetCDF4(dataset,group="MeanState")
-        del rmse,crmse,rmse_score_map
+                rmse_score.toNetCDF4(dataset, group="MeanState")
+        del rmse, crmse, rmse_score_map
 
     # RMSE based on annual cycle
     if (not skip_rmse) and (rmse_score_basis == "cycle"):
         ref_cycle = REF.annualCycle()
         ref_dtcycle = deepcopy(ref_cycle)
         com_cycle = COM.annualCycle()
         com_dtcycle = deepcopy(com_cycle)
-        with np.errstate(under='ignore'):
+        with np.errstate(under="ignore"):
             ref_dtcycle.data -= ref_cycle.data.mean(axis=0)
             com_dtcycle.data -= com_cycle.data.mean(axis=0)
-        del REF,COM,cREF
+        del REF, COM, cREF
         rmse = ref_cycle.rmse(com_cycle).convert(plot_unit)
         crmse = ref_dtcycle.rmse(com_dtcycle).convert(plot_unit)
-        rmse_score_map = Score(crmse,REF_std)
+        if df_errs is not None and name in df_errs["variable"].unique():
+            values = []
+            mask = []
+            for region in df_errs.region.unique():
+                val = df_errs.loc[
+                    (df_errs["variable"] == name)
+                    & (df_errs["type"] == "rmse")
+                    & (df_errs["region"] == region)
+                    & (df_errs["quantile"] == 70),
+                    "value",
+                ]
+                if len(val) > 0:
+                    mask.append(ILAMBregions.getMask(region, crmse))
+                    values.append((mask[-1] == False) * float(val))
+            rmse_score_map = deepcopy(crmse)
+            rmse_score_map.data = np.ma.masked_array(
+                np.array(values).sum(axis=0), mask=np.array(mask).all(axis=0)
+            )
+            msg = f"[{name}] RMSE scored using regional quantiles"
+            logger.info(msg)
+            with np.errstate(under="ignore"):
+                rmse_score_map.data = (1 - crmse.data / rmse_score_map.data).clip(0, 1)
+            rmse_score_map.unit = "1"
+            rmse_score_map.name = "rmsescore_map_of_%s" % name
+        else:
+            msg = f"[{name}] RMSE scored using Collier2018"
+            logger.info(msg)
+            rmse_score_map = Score(crmse, REF_std)
+
         if dataset is not None:
             rmse.name = "rmse_map_of_%s" % name
-            rmse.toNetCDF4(dataset,group="MeanState")
+            rmse.toNetCDF4(dataset, group="MeanState")
             rmse_score_map.name = "rmsescore_map_of_%s" % name
-            rmse_score_map.toNetCDF4(dataset,group="MeanState")
+            rmse_score_map.toNetCDF4(dataset, group="MeanState")
             for region in regions:
-                rmse_val = rmse.integrateInSpace(region=region,mean=True).convert(plot_unit)
+                rmse_val = rmse.integrateInSpace(region=region, mean=True).convert(
+                    plot_unit
+                )
                 rmse_val.name = "RMSE %s" % region
-                rmse_val.toNetCDF4(dataset,group="MeanState")
-                rmse_score = rmse_score_map.integrateInSpace(region=region,mean=True,weight=normalizer)
+                rmse_val.toNetCDF4(dataset, group="MeanState")
+                rmse_score = rmse_score_map.integrateInSpace(
+                    region=region, mean=True, weight=normalizer
+                )
                 rmse_score.name = "RMSE Score %s" % region
-                rmse_score.toNetCDF4(dataset,group="MeanState")
-        del rmse,crmse,rmse_score_map
-        
+                rmse_score.toNetCDF4(dataset, group="MeanState")
+        del rmse, crmse, rmse_score_map
+
     return
 
-def ClipTime(v,t0,tf):
+
+def ClipTime(v, t0, tf):
     """Remove time from a variable based on input bounds.
 
     Parameters
     ----------
     v : ILAMB.Variable.Variable
         the variable to trim
     t0,tf : float
         the times at which to trim
 
     Returns
     -------
     vtrim : ILAMB.Variable.Variable
         the trimmed variable
     """
-    begin = np.argmin(np.abs(v.time_bnds[:,0]-t0))
-    end   = np.argmin(np.abs(v.time_bnds[:,1]-tf))
-    if np.abs(v.time_bnds[begin,0]-t0) > 1e-6:
-        while v.time_bnds[begin,0] > t0:
-            begin    -= 1
+    begin = np.argmin(np.abs(v.time_bnds[:, 0] - t0))
+    end = np.argmin(np.abs(v.time_bnds[:, 1] - tf))
+    if np.abs(v.time_bnds[begin, 0] - t0) > 1e-6:
+        while v.time_bnds[begin, 0] > t0:
+            begin -= 1
             if begin <= 0:
                 begin = 0
                 break
-    if np.abs(v.time_bnds[end,1]-tf) > 1e-6:
-        while v.time_bnds[end,  1] < tf:
-            end      += 1
-            if end   >= v.time.size-1:
-                end   = v.time.size-1
+    if np.abs(v.time_bnds[end, 1] - tf) > 1e-6:
+        while v.time_bnds[end, 1] < tf:
+            end += 1
+            if end >= v.time.size - 1:
+                end = v.time.size - 1
                 break
-    v.time      = v.time     [begin:(end+1)    ]
-    v.time_bnds = v.time_bnds[begin:(end+1),...]
-    v.data      = v.data     [begin:(end+1),...]
+    v.time = v.time[begin : (end + 1)]
+    v.time_bnds = v.time_bnds[begin : (end + 1), ...]
+    v.data = v.data[begin : (end + 1), ...]
     if v.data_bnds is not None:
-        v.data_bnds = v.data_bnds[begin:(end+1),...]
+        v.data_bnds = v.data_bnds[begin : (end + 1), ...]
     return v
 
-def MakeComparable(ref,com,**keywords):
+
+def MakeComparable(ref, com, **keywords):
     r"""Make two variables comparable.
 
     Given a reference variable and a comparison variable, make the two
     variables comparable or raise an exception explaining why they are
     not.
 
     Parameters
@@ -1678,206 +2323,273 @@
     ref : ILAMB.Variable.Variable
         the modified reference variable object
     com : ILAMB.Variable.Variable
         the modified comparison variable object
 
     """
     # Process keywords
-    clip_ref    = keywords.get("clip_ref" ,False)
-    mask_ref    = keywords.get("mask_ref" ,False)
-    eps         = keywords.get("eps"      ,30./60./24.)
-    window      = keywords.get("window"   ,0.)
-    extents     = keywords.get("extents"  ,np.asarray([[-90.,+90.],[-180.,+180.]]))
-    logstring   = keywords.get("logstring","")
-    prune_sites = keywords.get("prune_sites",False)
-    site_atol   = keywords.get("site_atol",0.25)
-    allow_diff_times = keywords.get("allow_diff_times",False)
-    
+    clip_ref = keywords.get("clip_ref", False)
+    mask_ref = keywords.get("mask_ref", False)
+    eps = keywords.get("eps", 30.0 / 60.0 / 24.0)
+    window = keywords.get("window", 0.0)
+    extents = keywords.get("extents", np.asarray([[-90.0, +90.0], [-180.0, +180.0]]))
+    logstring = keywords.get("logstring", "")
+    prune_sites = keywords.get("prune_sites", False)
+    site_atol = keywords.get("site_atol", 0.25)
+    allow_diff_times = keywords.get("allow_diff_times", False)
+
     # If one variable is temporal, then they both must be
     if ref.temporal != com.temporal:
-        msg  = "%s Datasets are not uniformly temporal: " % logstring
-        msg += "reference = %s, comparison = %s" % (ref.temporal,com.temporal)
+        msg = "%s Datasets are not uniformly temporal: " % logstring
+        msg += "reference = %s, comparison = %s" % (ref.temporal, com.temporal)
         logger.debug(msg)
         raise VarsNotComparable()
 
     # If the reference is spatial, the comparison must be
     if ref.spatial and not com.spatial:
-        ref  = ref.extractDatasites(com.lat,com.lon)
-        msg  = "%s The reference dataset is spatial but the comparison is site-based. " % logstring
-        msg += "Extracted %s sites from the reference to match the comparison." % ref.ndata
+        ref = ref.extractDatasites(com.lat, com.lon)
+        msg = (
+            "%s The reference dataset is spatial but the comparison is site-based. "
+            % logstring
+        )
+        msg += (
+            "Extracted %s sites from the reference to match the comparison." % ref.ndata
+        )
         logger.info(msg)
 
     # If the reference is layered, the comparison must be
     if ref.layered and not com.layered:
         if ref.depth.size == 1:
-            com.layered    = True
-            com.depth      = ref.depth
+            com.layered = True
+            com.depth = ref.depth
             com.depth_bnds = ref.depth_bnds
-            shp            = list(com.data.shape)
-            insert         = 0
-            if com.temporal: insert = 1
-            shp.insert(insert,1)
-            com.data       = com.data.reshape(shp)
+            shp = list(com.data.shape)
+            insert = 0
+            if com.temporal:
+                insert = 1
+            shp.insert(insert, 1)
+            com.data = com.data.reshape(shp)
         else:
-            msg  = "%s Datasets are not uniformly layered: " % logstring
-            msg += "reference = %s, comparison = %s" % (ref.layered,com.layered)
+            msg = "%s Datasets are not uniformly layered: " % logstring
+            msg += "reference = %s, comparison = %s" % (ref.layered, com.layered)
             logger.debug(msg)
             raise NotLayeredVariable()
 
     # If the reference represents observation sites, extract them from
     # the comparison
-    if ref.ndata is not None and com.spatial: com = com.extractDatasites(ref.lat,ref.lon)
+    if ref.ndata is not None and com.spatial:
+        com = com.extractDatasites(ref.lat, ref.lon)
 
     # If both variables represent observations sites, make sure you
     # have the same number of sites and that they represent the same
     # location. Note this is after the above extraction so at this
     # point the ndata field of both variables should be equal.
     if (prune_sites) and (ref.ndata is not None) and (com.ndata is not None):
         deps = 1.0
 
         # prune the reference
-        r    = np.sqrt((ref.lat[:,np.newaxis]-com.lat)**2+(ref.lon[:,np.newaxis]-com.lon)**2)
+        r = np.sqrt(
+            (ref.lat[:, np.newaxis] - com.lat) ** 2
+            + (ref.lon[:, np.newaxis] - com.lon) ** 2
+        )
         rind = r.argmin(axis=0)
-        rind = rind[np.where(r[rind,range(com.ndata)]<deps)]
-        ref.lat = ref.lat[rind]; ref.lon = ref.lon[rind]; ref.data = ref.data[...,rind]
-        msg  = "%s Pruned %d sites from the reference and " % (logstring,ref.ndata-ref.lat.size)
+        rind = rind[np.where(r[rind, range(com.ndata)] < deps)]
+        ref.lat = ref.lat[rind]
+        ref.lon = ref.lon[rind]
+        ref.data = ref.data[..., rind]
+        msg = "%s Pruned %d sites from the reference and " % (
+            logstring,
+            ref.ndata - ref.lat.size,
+        )
         ref.ndata = ref.lat.size
 
         # prune the comparison
-        r    = np.sqrt((com.lat[:,np.newaxis]-ref.lat)**2+(com.lon[:,np.newaxis]-ref.lon)**2)
+        r = np.sqrt(
+            (com.lat[:, np.newaxis] - ref.lat) ** 2
+            + (com.lon[:, np.newaxis] - ref.lon) ** 2
+        )
         rind = r.argmin(axis=0)
-        rind = rind[np.where(r[rind,range(ref.ndata)]<deps)]
-        com.lat = com.lat[rind]; com.lon = com.lon[rind]; com.data = com.data[...,rind]
-        msg += "%d sites from the comparison." % (com.ndata-com.lat.size)
+        rind = rind[np.where(r[rind, range(ref.ndata)] < deps)]
+        com.lat = com.lat[rind]
+        com.lon = com.lon[rind]
+        com.data = com.data[..., rind]
+        msg += "%d sites from the comparison." % (com.ndata - com.lat.size)
         com.ndata = com.lat.size
         logger.info(msg)
     else:
         if ref.ndata != com.ndata:
-            msg  = "%s One or both datasets are understood as site data but differ in number of sites: " % logstring
-            msg += "reference = %d, comparison = %d" % (ref.ndata,com.ndata)
+            msg = (
+                "%s One or both datasets are understood as site data but differ in number of sites: "
+                % logstring
+            )
+            msg += "reference = %d, comparison = %d" % (ref.ndata, com.ndata)
             logger.debug(msg)
             raise VarsNotComparable()
     if ref.ndata is not None:
-        if not (np.allclose(ref.lat,com.lat,atol=site_atol) or np.allclose(ref.lon,com.lon,atol=site_atol)):
-            msg  = "%s Datasets represent sites, but the locations are different: " % logstring
-            msg += "maximum difference lat = %.2f, lon = %.2f" % (np.abs((ref.lat-com.lat)).max(),
-                                                                  np.abs((ref.lon-com.lon)).max())
+        if not (
+            np.allclose(ref.lat, com.lat, atol=site_atol)
+            or np.allclose(ref.lon, com.lon, atol=site_atol)
+        ):
+            msg = (
+                "%s Datasets represent sites, but the locations are different: "
+                % logstring
+            )
+            msg += "maximum difference lat = %.2f, lon = %.2f" % (
+                np.abs((ref.lat - com.lat)).max(),
+                np.abs((ref.lon - com.lon)).max(),
+            )
             logger.debug(msg)
             raise VarsNotComparable()
 
     # If the datasets are both spatial, ensure that both represent the
     # same spatial area and trim the datasets if not.
     if ref.spatial and com.spatial:
-
-        lat_bnds = (max(ref.lat_bnds[ 0,0],com.lat_bnds[ 0,0],extents[0,0]),
-                    min(ref.lat_bnds[-1,1],com.lat_bnds[-1,1],extents[0,1]))
-        lon_bnds = (max(ref.lon_bnds[ 0,0],com.lon_bnds[ 0,0],extents[1,0]),
-                    min(ref.lon_bnds[-1,1],com.lon_bnds[-1,1],extents[1,1]))
+        lat_bnds = (
+            max(ref.lat_bnds[0, 0], com.lat_bnds[0, 0], extents[0, 0]),
+            min(ref.lat_bnds[-1, 1], com.lat_bnds[-1, 1], extents[0, 1]),
+        )
+        lon_bnds = (
+            max(ref.lon_bnds[0, 0], com.lon_bnds[0, 0], extents[1, 0]),
+            min(ref.lon_bnds[-1, 1], com.lon_bnds[-1, 1], extents[1, 1]),
+        )
 
         # Clip reference
-        diff     = np.abs([ref.lat_bnds[[0,-1],[0,1]]-lat_bnds,
-                           ref.lon_bnds[[0,-1],[0,1]]-lon_bnds])
-        if diff.max() >= 5.:
-            shp0 = np.asarray(np.copy(ref.data.shape),dtype=int)
-            ref.trim(lat=[lat_bnds[0] if diff[0,0] >= 5. else -90.,
-                          lat_bnds[1] if diff[0,1] >= 5. else +90],
-                     lon=[lon_bnds[0] if diff[1,0] >= 5. else -180.,
-                          lon_bnds[1] if diff[1,1] >= 5. else +180])
-            shp  = np.asarray(np.copy(ref.data.shape),dtype=int)
-            msg  = "%s Spatial data was clipped from the reference: " % logstring
+        diff = np.abs(
+            [
+                ref.lat_bnds[[0, -1], [0, 1]] - lat_bnds,
+                ref.lon_bnds[[0, -1], [0, 1]] - lon_bnds,
+            ]
+        )
+        if diff.max() >= 5.0:
+            shp0 = np.asarray(np.copy(ref.data.shape), dtype=int)
+            ref.trim(
+                lat=[
+                    lat_bnds[0] if diff[0, 0] >= 5.0 else -90.0,
+                    lat_bnds[1] if diff[0, 1] >= 5.0 else +90,
+                ],
+                lon=[
+                    lon_bnds[0] if diff[1, 0] >= 5.0 else -180.0,
+                    lon_bnds[1] if diff[1, 1] >= 5.0 else +180,
+                ],
+            )
+            shp = np.asarray(np.copy(ref.data.shape), dtype=int)
+            msg = "%s Spatial data was clipped from the reference: " % logstring
             msg += " before: %s" % (shp0)
-            msg +=  " after: %s" % (shp )
+            msg += " after: %s" % (shp)
             logger.info(msg)
 
         # Clip comparison
-        diff     = np.abs([com.lat_bnds[[0,-1],[0,1]]-lat_bnds,
-                           com.lon_bnds[[0,-1],[0,1]]-lon_bnds])
-        if diff.max() >= 5.:
-            shp0 = np.asarray(np.copy(com.data.shape),dtype=int)
-            com.trim(lat=[lat_bnds[0] if diff[0,0] >= 5. else -90.,
-                          lat_bnds[1] if diff[0,1] >= 5. else +90],
-                     lon=[lon_bnds[0] if diff[1,0] >= 5. else -180.,
-                          lon_bnds[1] if diff[1,1] >= 5. else +180])
-            shp  = np.asarray(np.copy(com.data.shape),dtype=int)
-            msg  = "%s Spatial data was clipped from the comparison: " % logstring
+        diff = np.abs(
+            [
+                com.lat_bnds[[0, -1], [0, 1]] - lat_bnds,
+                com.lon_bnds[[0, -1], [0, 1]] - lon_bnds,
+            ]
+        )
+        if diff.max() >= 5.0:
+            shp0 = np.asarray(np.copy(com.data.shape), dtype=int)
+            com.trim(
+                lat=[
+                    lat_bnds[0] if diff[0, 0] >= 5.0 else -90.0,
+                    lat_bnds[1] if diff[0, 1] >= 5.0 else +90,
+                ],
+                lon=[
+                    lon_bnds[0] if diff[1, 0] >= 5.0 else -180.0,
+                    lon_bnds[1] if diff[1, 1] >= 5.0 else +180,
+                ],
+            )
+            shp = np.asarray(np.copy(com.data.shape), dtype=int)
+            msg = "%s Spatial data was clipped from the comparison: " % logstring
             msg += " before: %s" % (shp0)
-            msg +=  " after: %s" % (shp )
+            msg += " after: %s" % (shp)
             logger.info(msg)
 
     if ref.temporal:
-
         # If the reference time scale is significantly larger than the
         # comparison, coarsen the comparison
-        if np.log10(ref.dt/com.dt) > 0.5:
-            com = com.coarsenInTime(ref.time_bnds,window=window)
+        if np.log10(ref.dt / com.dt) > 0.5:
+            com = com.coarsenInTime(ref.time_bnds, window=window)
+        elif np.log10(com.dt / ref.dt) > 0.5:
+            msg = "%s Reference data was coarsened\n: " % logstring
+            msg += " dt before: %s" % (ref.dt)
+            ref = ref.coarsenInTime(com.time_bnds, window=window)
+            msg += " dt after: %s" % (ref.dt)
+            logger.info(msg)
 
         # Time bounds of the reference dataset
-        t0  = ref.time_bnds[ 0,0]
-        tf  = ref.time_bnds[-1,1]
+        t0 = ref.time_bnds[0, 0]
+        tf = ref.time_bnds[-1, 1]
 
         # Find the comparison time range which fully encompasses the reference
-        com = ClipTime(com,t0,tf)
+        com = ClipTime(com, t0, tf)
 
         if clip_ref:
-
             # We will clip the reference dataset too
-            t0  = max(t0,com.time_bnds[ 0,0])
-            tf  = min(tf,com.time_bnds[-1,1])
-            ref = ref.trim(t=[t0,tf])
-        
+            t0 = max(t0, com.time_bnds[0, 0])
+            tf = min(tf, com.time_bnds[-1, 1])
+            ref = ref.trim(t=[t0, tf])
+
         # Check that we now are on the same time intervals
         if not allow_diff_times:
             if ref.time.size != com.time.size:
-
                 # Special case - it frequently works out that we are 1
                 # time interval off for some reason. For now, we will
                 # detect this and push a fix.
-                if ref.time.size == (com.time.size+1):
-
-                    if(np.abs(com.time[0]-ref.time[1])/(ref.time_bnds[0,1]-ref.time_bnds[0,0])<1e-2):
+                if ref.time.size == (com.time.size + 1):
+                    if (
+                        np.abs(com.time[0] - ref.time[1])
+                        / (ref.time_bnds[0, 1] - ref.time_bnds[0, 0])
+                        < 1e-2
+                    ):
                         ref.time = ref.time[1:]
                         ref.time_bnds = ref.time_bnds[1:]
                         ref.data = ref.data[1:]
 
                 else:
-                    msg  = "%s Datasets have differing numbers of time intervals: " % logstring
-                    msg += "reference = %d, comparison = %d" % (ref.time.size,com.time.size)
+                    msg = (
+                        "%s Datasets have differing numbers of time intervals: "
+                        % logstring
+                    )
+                    msg += "reference = %d, comparison = %d" % (
+                        ref.time.size,
+                        com.time.size,
+                    )
                     logger.debug(msg)
                     raise VarsNotComparable()
 
-            if not np.allclose(ref.time_bnds,com.time_bnds,atol=0.75*ref.dt):
-                msg  = "%s Datasets are defined at different times" % logstring
+            if not np.allclose(ref.time_bnds, com.time_bnds, atol=0.75 * ref.dt):
+                msg = "%s Datasets are defined at different times" % logstring
                 logger.debug(msg)
                 raise VarsNotComparable()
 
     if ref.layered:
-
         # Try to resolve if the layers from the two quantities are
         # different
         if ref.depth.size == com.depth.size == 1:
-            ref = ref.integrateInDepth(mean = True)
-            com = com.integrateInDepth(mean = True)
+            ref = ref.integrateInDepth(mean=True)
+            com = com.integrateInDepth(mean=True)
         elif ref.depth.size != com.depth.size:
             # Compute the mean values from the comparison over the
             # layer breaks of the reference.
             if ref.depth.size == 1 and com.depth.size > 1:
-                com = com.integrateInDepth(z0=ref.depth_bnds[ 0,0],
-                                           zf=ref.depth_bnds[-1,1],
-                                           mean = True)
-                ref = ref.integrateInDepth(mean = True) # just removing the depth dimension
+                com = com.integrateInDepth(
+                    z0=ref.depth_bnds[0, 0], zf=ref.depth_bnds[-1, 1], mean=True
+                )
+                ref = ref.integrateInDepth(
+                    mean=True
+                )  # just removing the depth dimension
         else:
-            if not np.allclose(ref.depth,com.depth):
-                msg  = "%s Datasets have a different layering scheme" % logstring
+            if not np.allclose(ref.depth, com.depth):
+                msg = "%s Datasets have a different layering scheme" % logstring
                 logger.debug(msg)
                 raise VarsNotComparable()
 
     # Convert the comparison to the units of the reference
     com = com.convert(ref.unit)
 
-    return ref,com
+    return ref, com
 
 
 def CombineVariables(V):
     """Combines a list of variables into a single variable.
 
     This routine is intended to be used to merge variables when
     separate moments in time are scattered over several files.
@@ -1888,148 +2600,162 @@
         a list of variables to merge into a single variable
 
     Returns
     -------
     v : ILAMB.Variable.Variable
         the merged variable
     """
-    from .Variable import Variable
+    from ILAMB.Variable import Variable
 
     # checks on data
     assert type(V) == type([])
-    if len(V) == 1: return V[0]
-    for v in V: assert v.temporal
+    if len(V) == 1:
+        return V[0]
+    for v in V:
+        assert v.temporal
 
     # Put list in order by initial time
     V.sort(key=lambda v: v.time[0])
 
     # Check the beginning and ends times for monotonicity
-    nV  = len(V)
-    t0  = np.zeros(nV)
-    tf  = np.zeros(nV)
-    nt  = np.zeros(nV,dtype=int)
+    nV = len(V)
+    t0 = np.zeros(nV)
+    tf = np.zeros(nV)
+    nt = np.zeros(nV, dtype=int)
     ind = [0]
-    for i,v in enumerate(V):
-        t0[i] = v.time[ 0]
+    for i, v in enumerate(V):
+        t0[i] = v.time[0]
         tf[i] = v.time[-1]
         nt[i] = v.time.size
-        ind.append(nt[:(i+1)].sum())
+        ind.append(nt[: (i + 1)].sum())
 
     # Checks on monotonicity
-    if (((t0[1:]-t0[:-1]).min() < 0) or
-        ((tf[1:]-tf[:-1]).min() < 0) or
-        ((t0[1:]-tf[:-1]).min() < 0)):
+    if (
+        ((t0[1:] - t0[:-1]).min() < 0)
+        or ((tf[1:] - tf[:-1]).min() < 0)
+        or ((t0[1:] - tf[:-1]).min() < 0)
+    ):
         msg = "[MonotonicityError]"
         for i in range(nV):
             err = ""
-            if i > 0     :
-                err += "" if t0[i]   > tf[i-1] else "*"
-                err += "" if t0[i]   > t0[i-1] else "*"
-            if i < (nV-1):
-                err += "" if tf[i+1] > t0[i  ] else "*"
-                err += "" if tf[i+1] > tf[i  ] else "*"
-            msg  += "\n  %2d: t = [%.3f, %.3f] %2s %s" % (i,t0[i],tf[i],err,V[i].filename)
+            if i > 0:
+                err += "" if t0[i] > tf[i - 1] else "*"
+                err += "" if t0[i] > t0[i - 1] else "*"
+            if i < (nV - 1):
+                err += "" if tf[i + 1] > t0[i] else "*"
+                err += "" if tf[i + 1] > tf[i] else "*"
+            msg += "\n  %2d: t = [%.3f, %.3f] %2s %s" % (
+                i,
+                t0[i],
+                tf[i],
+                err,
+                V[i].filename,
+            )
         logger.debug(msg)
         raise MonotonicityError()
 
     # Assemble the data
-    shp       = (nt.sum(),)+V[0].data.shape[1:]
-    time      = np.zeros(shp[0])
-    time_bnds = np.zeros((shp[0],2))
-    data      = np.zeros(shp)
-    mask      = np.zeros(shp,dtype=bool)
-    for i,v in enumerate(V):
-        time     [ind[i]:ind[i+1]]     = v.time
-        time_bnds[ind[i]:ind[i+1],...] = v.time_bnds
-        data     [ind[i]:ind[i+1],...] = v.data
-        mask     [ind[i]:ind[i+1],...] = v.data.mask
+    shp = (nt.sum(),) + V[0].data.shape[1:]
+    time = np.zeros(shp[0])
+    time_bnds = np.zeros((shp[0], 2))
+    data = np.zeros(shp)
+    mask = np.zeros(shp, dtype=bool)
+    for i, v in enumerate(V):
+        time[ind[i] : ind[i + 1]] = v.time
+        time_bnds[ind[i] : ind[i + 1], ...] = v.time_bnds
+        data[ind[i] : ind[i + 1], ...] = v.data
+        mask[ind[i] : ind[i + 1], ...] = v.data.mask
 
     # If assembled from single slice files and no time bounds were
     # provided, they will not be reflective of true bounds here. If
     # any dt's are 0, make time_bounds none and recompute in the
     # constructor.
-    if np.any((time_bnds[:,1]-time_bnds[:,0])<1e-12): time_bnds = None
+    if np.any((time_bnds[:, 1] - time_bnds[:, 0]) < 1e-12):
+        time_bnds = None
 
     v = V[0]
-    v = Variable(data       = np.ma.masked_array(data,mask=mask),
-                 unit       = v.unit,
-                 name       = v.name,
-                 time       = time,
-                 time_bnds  = time_bnds,
-                 depth      = v.depth,
-                 depth_bnds = v.depth_bnds,
-                 lat        = v.lat,
-                 lat_bnds   = v.lat_bnds,
-                 lon        = v.lon,
-                 lon_bnds   = v.lon_bnds,
-                 area       = v.area,
-                 ndata      = v.ndata)
+    v = Variable(
+        data=np.ma.masked_array(data, mask=mask),
+        unit=v.unit,
+        name=v.name,
+        time=time,
+        time_bnds=time_bnds,
+        depth=v.depth,
+        depth_bnds=v.depth_bnds,
+        lat=v.lat,
+        lat_bnds=v.lat_bnds,
+        lon=v.lon,
+        lon_bnds=v.lon_bnds,
+        area=v.area,
+        ndata=v.ndata,
+    )
     v.attr = V[0].attr
     return v
 
+
 def ConvertBoundsTypes(x):
     y = None
     if x.ndim == 2:
-        y = np.zeros(x.shape[0]+1)
-        y[:-1] = x[ :, 0]
-        y[ -1] = x[-1,-1]
+        y = np.zeros(x.shape[0] + 1)
+        y[:-1] = x[:, 0]
+        y[-1] = x[-1, -1]
     if x.ndim == 1:
-        y = np.zeros((x.shape[0]-1,2))
-        y[:,0] = x[:-1]
-        y[:,1] = x[+1:]
+        y = np.zeros((x.shape[0] - 1, 2))
+        y[:, 0] = x[:-1]
+        y[:, 1] = x[+1:]
     return y
 
+
 def LandLinInterMissingValues(mdata):
-    land = np.any(mdata.mask,axis=0)==False
+    land = np.any(mdata.mask, axis=0) == False
     data = np.ma.masked_array(mdata)
-    data.data[data.mask] = 0.
-    data.fill_value      = 0.
+    data.data[data.mask] = 0.0
+    data.fill_value = 0.0
     data = data.data
     land = land.astype(int)
-    smooth               = data*land[np.newaxis,...]
-    suml                 = np.copy(land)
-    smooth[:,1:-1,1:-1] += data[:, :-2, :-2]*land[np.newaxis, :-2, :-2]
-    suml  [  1:-1,1:-1] +=                   land[            :-2, :-2]
-    smooth[:,1:-1,1:-1] += data[:, :-2,1:-1]*land[np.newaxis, :-2,1:-1]
-    suml  [  1:-1,1:-1] +=                   land[            :-2,1:-1]
-    smooth[:,1:-1,1:-1] += data[:, :-2, +2:]*land[np.newaxis, :-2, +2:]
-    suml  [  1:-1,1:-1] +=                   land[            :-2, +2:]
-    smooth[:,1:-1,1:-1] += data[:,1:-1, :-2]*land[np.newaxis,1:-1, :-2]
-    suml  [  1:-1,1:-1] +=                   land[           1:-1, :-2]
-    smooth[:,1:-1,1:-1] += data[:,1:-1, +2:]*land[np.newaxis,1:-1, +2:]
-    suml  [  1:-1,1:-1] +=                   land[           1:-1, +2:]
-    smooth[:,1:-1,1:-1] += data[:, +2:, :-2]*land[np.newaxis, +2:, :-2]
-    suml  [  1:-1,1:-1] +=                   land[            +2:, :-2]
-    smooth[:,1:-1,1:-1] += data[:, +2:,1:-1]*land[np.newaxis, +2:,1:-1]
-    suml  [  1:-1,1:-1] +=                   land[            +2:,1:-1]
-    smooth[:,1:-1,1:-1] += data[:, +2:, +2:]*land[np.newaxis, +2:, +2:]
-    suml  [  1:-1,1:-1] +=                   land[            +2:, +2:]
+    smooth = data * land[np.newaxis, ...]
+    suml = np.copy(land)
+    smooth[:, 1:-1, 1:-1] += data[:, :-2, :-2] * land[np.newaxis, :-2, :-2]
+    suml[1:-1, 1:-1] += land[:-2, :-2]
+    smooth[:, 1:-1, 1:-1] += data[:, :-2, 1:-1] * land[np.newaxis, :-2, 1:-1]
+    suml[1:-1, 1:-1] += land[:-2, 1:-1]
+    smooth[:, 1:-1, 1:-1] += data[:, :-2, +2:] * land[np.newaxis, :-2, +2:]
+    suml[1:-1, 1:-1] += land[:-2, +2:]
+    smooth[:, 1:-1, 1:-1] += data[:, 1:-1, :-2] * land[np.newaxis, 1:-1, :-2]
+    suml[1:-1, 1:-1] += land[1:-1, :-2]
+    smooth[:, 1:-1, 1:-1] += data[:, 1:-1, +2:] * land[np.newaxis, 1:-1, +2:]
+    suml[1:-1, 1:-1] += land[1:-1, +2:]
+    smooth[:, 1:-1, 1:-1] += data[:, +2:, :-2] * land[np.newaxis, +2:, :-2]
+    suml[1:-1, 1:-1] += land[+2:, :-2]
+    smooth[:, 1:-1, 1:-1] += data[:, +2:, 1:-1] * land[np.newaxis, +2:, 1:-1]
+    suml[1:-1, 1:-1] += land[+2:, 1:-1]
+    smooth[:, 1:-1, 1:-1] += data[:, +2:, +2:] * land[np.newaxis, +2:, +2:]
+    suml[1:-1, 1:-1] += land[+2:, +2:]
     smooth /= suml.clip(1)
-    smooth  = (mdata.mask==True)*smooth + (mdata.mask==False)*mdata.data
+    smooth = (mdata.mask == True) * smooth + (mdata.mask == False) * mdata.data
     return smooth
 
-class FileContextManager():
-
-    def __init__(self,master,mod_results,obs_results):
 
-        self.master       = master
-        self.mod_results  = mod_results
-        self.obs_results  = obs_results
-        self.mod_dset     = None
-        self.obs_dset     = None
+class FileContextManager:
+    def __init__(self, master, mod_results, obs_results):
+        self.master = master
+        self.mod_results = mod_results
+        self.obs_results = obs_results
+        self.mod_dset = None
+        self.obs_dset = None
 
     def __enter__(self):
-
         # Open the file on entering, both if you are the master
-        self.mod_dset                 = Dataset(self.mod_results,mode="w")
-        if self.master: self.obs_dset = Dataset(self.obs_results,mode="w")
+        self.mod_dset = Dataset(self.mod_results, mode="w")
+        if self.master:
+            self.obs_dset = Dataset(self.obs_results, mode="w")
         return self
 
     def __exit__(self, exc_type, exc_value, traceback):
-
         # Always close the file(s) on exit
         self.mod_dset.close()
-        if self.master: self.obs_dset.close()
+        if self.master:
+            self.obs_dset.close()
 
         # If an exception occurred, also remove the files
         if exc_type is not None:
             os.system("rm -f %s" % self.mod_results)
```

### Comparing `ILAMB-2.6/src/ILAMB/run.py` & `ILAMB-2.7/src/ILAMB/run.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,18 +1,31 @@
-from .ModelResult import ModelResult
+import logging
 import os
 import pickle
-from mpi4py import MPI
-import logging
-from . import ilamblib as il
 from traceback import format_exc
 
+from mpi4py import MPI
+
+from ILAMB import ilamblib as il
+from ILAMB.ModelResult import ModelResult
+
 logger = logging.getLogger("%i" % MPI.COMM_WORLD.rank)
 
-def InitializeModels(model_root,models=[],verbose=False,filter="",regex="",model_year=[],log=True,models_path="./",comm=MPI.COMM_WORLD):
+
+def InitializeModels(
+    model_root,
+    models=[],
+    verbose=False,
+    filter="",
+    regex="",
+    model_year=[],
+    log=True,
+    models_path="./",
+    comm=MPI.COMM_WORLD,
+):
     """Initializes a list of models
 
     Initializes a list of models where each model is the subdirectory
     beneath the given model root directory. The global list of models
     will exist on each processor.
 
     Parameters
@@ -29,64 +42,84 @@
     Returns
     -------
     M : list of ILAMB.ModelResults.ModelsResults
        a list of the model results, sorted alphabetically by name
 
     """
     rank = comm.rank
-    # initialize the models    
+    # initialize the models
     M = []
-    if len(model_year) != 2: model_year = None
+    if len(model_year) != 2:
+        model_year = None
     max_model_name_len = 0
-    if rank == 0 and verbose: print("\nSearching for model results in %s\n" % model_root)
+    if rank == 0 and verbose:
+        print("\nSearching for model results in %s\n" % model_root)
     for subdir, dirs, files in os.walk(model_root):
         for mname in dirs:
-            if len(models) > 0 and mname not in models: continue
-            pkl_file = os.path.join(models_path,"%s.pkl" % mname)
+            if len(models) > 0 and mname not in models:
+                continue
+            pkl_file = os.path.join(models_path, "%s.pkl" % mname)
             if os.path.isfile(pkl_file):
-                with open(pkl_file,'rb') as infile:
+                with open(pkl_file, "rb") as infile:
                     m = pickle.load(infile)
             else:
                 try:
-                    m = ModelResult(os.path.join(subdir,mname), modelname = mname, filter=filter, regex=regex, model_year = model_year)
+                    m = ModelResult(
+                        os.path.join(subdir, mname),
+                        modelname=mname,
+                        filter=filter,
+                        regex=regex,
+                        model_year=model_year,
+                    )
                 except Exception as ex:
-                    if log: logger.debug("[%s]" % mname,format_exc())
+                    if log:
+                        logger.debug("[%s]" % mname, format_exc())
                     continue
             M.append(m)
-            max_model_name_len = max(max_model_name_len,len(mname))
+            max_model_name_len = max(max_model_name_len, len(mname))
         break
-    M = sorted(M,key=lambda m: m.name.upper())
-    
+    M = sorted(M, key=lambda m: m.name.upper())
+
     # assign unique colors
     clrs = il.GenerateDistinctColors(len(M))
     for m in M:
-        clr     = clrs.pop(0)
+        clr = clrs.pop(0)
         m.color = clr
 
     # save model objects as pickle files
     comm.Barrier()
     if rank == 0:
         for m in M:
-            pkl_file = os.path.join(models_path,"%s.pkl" % m.name)
-            with open(pkl_file,'wb') as out:
-                pickle.dump(m,out,pickle.HIGHEST_PROTOCOL)
-        
+            pkl_file = os.path.join(models_path, "%s.pkl" % m.name)
+            with open(pkl_file, "wb") as out:
+                pickle.dump(m, out, pickle.HIGHEST_PROTOCOL)
+
     # optionally output models which were found
     if rank == 0 and verbose:
         for m in M:
             print(("    {0:>45}").format(m.name))
 
     if len(M) == 0:
-        if verbose and rank == 0: print("No model results found")
+        if verbose and rank == 0:
+            print("No model results found")
         comm.Barrier()
         comm.Abort(0)
 
     return M
 
-def ParseModelSetup(model_setup,models=[],verbose=False,filter="",regex="",models_path="./",comm=MPI.COMM_WORLD):
+
+def ParseModelSetup(
+    model_setup,
+    models=[],
+    verbose=False,
+    filter="",
+    regex="",
+    models_path="./",
+    comm=MPI.COMM_WORLD,
+):
     """Initializes a list of models
 
     Initializes a list of models where each model is the subdirectory
     beneath the given model root directory. The global list of models
     will exist on each processor.
 
     Parameters
@@ -104,62 +137,72 @@
        a list of the model results, sorted alphabetically by name
 
     """
     rank = comm.rank
     # initialize the models
     M = []
     max_model_name_len = 0
-    if rank == 0 and verbose: print("\nSetting up model results from %s\n" % model_setup)
+    if rank == 0 and verbose:
+        print("\nSetting up model results from %s\n" % model_setup)
     with open(model_setup) as f:
         for line in f.readlines():
-            if line.strip().startswith("#"): continue
-            line       = line.split(",")
-            mname      = None
-            mdir       = None
+            if line.strip().startswith("#"):
+                continue
+            line = line.split(",")
+            mname = None
+            mdir = None
             model_year = None
             if len(line) >= 2:
-                mname  = line[0].strip()
-                mdir   = line[1].strip()
+                mname = line[0].strip()
+                mdir = line[1].strip()
                 # if mdir not a directory, then maybe path is relative to ILAMB_ROOT
                 if not os.path.isdir(mdir):
-                    mdir = os.path.join(os.environ["ILAMB_ROOT"],mdir).strip()
+                    mdir = os.path.join(os.environ["ILAMB_ROOT"], mdir).strip()
             if len(line) == 4:
-                model_year = [float(line[2].strip()),float(line[3].strip())]
-            max_model_name_len = max(max_model_name_len,len(mname))
-            if (len(models) > 0 and mname not in models) or (mname is None): continue
-            pkl_file = os.path.join(models_path,"%s.pkl" % mname)
+                model_year = [float(line[2].strip()), float(line[3].strip())]
+            max_model_name_len = max(max_model_name_len, len(mname))
+            if (len(models) > 0 and mname not in models) or (mname is None):
+                continue
+            pkl_file = os.path.join(models_path, "%s.pkl" % mname)
             if os.path.isfile(pkl_file):
-                with open(pkl_file,'rb') as infile:
+                with open(pkl_file, "rb") as infile:
                     m = pickle.load(infile)
             else:
                 try:
-                    m = ModelResult(mdir, modelname = mname, filter=filter, regex=regex, model_year = model_year)
+                    m = ModelResult(
+                        mdir,
+                        modelname=mname,
+                        filter=filter,
+                        regex=regex,
+                        model_year=model_year,
+                    )
                 except Exception as ex:
-                    if log: logger.debug("[%s]" % mname,format_exc())
+                    logger.debug("[%s]" % mname, format_exc())
                     continue
             M.append(m)
 
     # assign unique colors
     clrs = il.GenerateDistinctColors(len(M))
     for m in M:
-        clr     = clrs.pop(0)
+        clr = clrs.pop(0)
         m.color = clr
 
     # save model objects as pickle files
     comm.Barrier()
     if rank == 0:
         for m in M:
-            pkl_file = os.path.join(models_path,"%s.pkl" % m.name)
-            with open(pkl_file,'wb') as out:
-                pickle.dump(m,out,pickle.HIGHEST_PROTOCOL)
-                
+            pkl_file = os.path.join(models_path, "%s.pkl" % m.name)
+            with open(pkl_file, "wb") as out:
+                pickle.dump(m, out, pickle.HIGHEST_PROTOCOL)
+
     # optionally output models which were found
     if rank == 0 and verbose:
         for m in M:
             print(("    {0:>45}").format(m.name))
 
     if len(M) == 0:
-        if verbose and rank == 0: print("No model results found")
+        if verbose and rank == 0:
+            print("No model results found")
         comm.Barrier()
         comm.Abort(0)
 
     return M
```

### Comparing `ILAMB-2.6/src/ILAMB.egg-info/SOURCES.txt` & `ILAMB-2.7/src/ILAMB.egg-info/SOURCES.txt`

 * *Files 8% similar despite different names*

```diff
@@ -1,47 +1,53 @@
-README.rst
+LICENSE.rst
+README.md
 setup.py
-bin/ilamb-doctor
 bin/ilamb-fetch
 bin/ilamb-mean
 bin/ilamb-run
 bin/ilamb-setup
-bin/ilamb-table
 src/ILAMB/ConfAlbedo.py
+src/ILAMB/ConfBasin.py
 src/ILAMB/ConfBurntArea.py
 src/ILAMB/ConfCO2.py
+src/ILAMB/ConfContentChange.py
+src/ILAMB/ConfDepthGradient.py
 src/ILAMB/ConfDiurnal.py
 src/ILAMB/ConfEvapFraction.py
-src/ILAMB/ConfIOMB.py
+src/ILAMB/ConfGSNF.py
 src/ILAMB/ConfNBP.py
 src/ILAMB/ConfPermafrost.py
 src/ILAMB/ConfRunoff.py
 src/ILAMB/ConfSWE.py
 src/ILAMB/ConfSoilCarbon.py
 src/ILAMB/ConfTWSA.py
+src/ILAMB/ConfUSGS.py
 src/ILAMB/ConfUncertainty.py
 src/ILAMB/Confrontation.py
 src/ILAMB/ModelResult.py
 src/ILAMB/Post.py
 src/ILAMB/Regions.py
 src/ILAMB/Relationship.py
 src/ILAMB/Scoreboard.py
 src/ILAMB/Variable.py
 src/ILAMB/__init__.py
 src/ILAMB/ccgfilt.py
 src/ILAMB/constants.py
+src/ILAMB/e3sm_result.py
 src/ILAMB/generated_version.py
 src/ILAMB/ilamblib.py
+src/ILAMB/point_result.py
 src/ILAMB/run.py
 src/ILAMB.egg-info/PKG-INFO
 src/ILAMB.egg-info/SOURCES.txt
 src/ILAMB.egg-info/dependency_links.txt
 src/ILAMB.egg-info/not-zip-safe
 src/ILAMB.egg-info/requires.txt
 src/ILAMB.egg-info/top_level.txt
 src/ILAMB/data/cmip.cfg
 src/ILAMB/data/diurnal.cfg
 src/ILAMB/data/ilamb.cfg
 src/ILAMB/data/iomb.cfg
+src/ILAMB/data/quantiles_Whittaker_cmip5v6.parquet
 src/ILAMB/data/sample.cfg
 test/test_Variable.py
 test/test_run_script.py
```

### Comparing `ILAMB-2.6/test/test_Variable.py` & `ILAMB-2.7/test/test_Variable.py`

 * *Files identical despite different names*

